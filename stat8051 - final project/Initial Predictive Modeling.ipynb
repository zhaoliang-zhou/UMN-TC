{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, StratifiedKFold, RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, auc, mean_squared_error as mse\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression, PoissonRegressor, GammaRegressor, TweedieRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Counts:\n",
      "0    16861\n",
      "1     1155\n",
      "2       70\n",
      "3        2\n",
      "Name: claim_count, dtype: int64\n",
      "Test Counts:\n",
      "0    4215\n",
      "1     288\n",
      "2      18\n",
      "3       1\n",
      "Name: claim_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Importing data, preprocessing, and splitting\n",
    "df = pd.read_csv('InsNova_train.csv').drop('id', axis=1)\n",
    "df = pd.get_dummies(df, columns=['veh_body', 'veh_age', 'gender', 'area', 'dr_age'])\n",
    "df['pure_premium'] = df['claim_cost'] / df['exposure']\n",
    "df['avg_cost'] = df['claim_cost'] / np.fmax(df['claim_count'], 1)\n",
    "df['frequency'] = df['claim_count'] / df['exposure']\n",
    "response_cols = ['exposure', 'claim_ind', 'claim_count', 'claim_cost', 'pure_premium', 'avg_cost', 'frequency']\n",
    "X, y = df.drop(response_cols, axis=1), df[response_cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y['claim_count'])\n",
    "claim_mask = (y_train['claim_cost'] > 0.0).values\n",
    "print('Train Counts:', y_train['claim_count'].value_counts(), 'Test Counts:', y_test['claim_count'].value_counts(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # sort rows on prediction column \n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:,0].argsort()][::-1,0]\n",
    "    pred_order = arr[arr[:,1].argsort()][::-1,0]\n",
    "    \n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order)\n",
    "    L_ones = np.linspace(1/n_samples, 1, n_samples)\n",
    "\n",
    "    # normalize to true Gini coefficient\n",
    "    return np.sum(L_ones - L_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_gini(y_true, y_pred):\n",
    "    return gini(y_true, y_pred) / gini(y_true, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our eval function\n",
    "model_scores = {}\n",
    "def eval_model(model):\n",
    "    train_preds = model.predict(X_train) * y_train['exposure']\n",
    "    test_preds = model.predict(X_test) * y_test['exposure']\n",
    "    return {'train': gini(y_train['claim_cost'], train_preds),\n",
    "            'test': gini(y_test['claim_cost'], test_preds),\n",
    "            'norm_train': normalized_gini(y_train['claim_cost'], train_preds),\n",
    "            'norm_test': normalized_gini(y_test['claim_cost'], test_preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a benchmark dummy model\n",
    "dummy = DummyRegressor().fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['dummy'] = eval_model(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing simple linear models\n",
    "linreg = LinearRegression().fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['linreg'] = eval_model(linreg)\n",
    "\n",
    "ridge = BayesSearchCV(make_pipeline(StandardScaler(),\n",
    "                                    Ridge()),\n",
    "                      {'ridge__alpha': Real(0.001, 10.0, 'uniform')},\n",
    "                      fit_params={'ridge__sample_weight': y_train['exposure']},\n",
    "                      n_iter=16,\n",
    "                      cv=5,\n",
    "                      scoring=make_scorer(normalized_gini),\n",
    "                      n_jobs=-1).fit(X_train, y_train['pure_premium']).best_estimator_\n",
    "model_scores['ridge'] = eval_model(ridge)\n",
    "\n",
    "tweedie = BayesSearchCV(make_pipeline(StandardScaler(),\n",
    "                                      TweedieRegressor()),\n",
    "                        {'tweedieregressor__alpha': Real(0.1, 5.0, 'uniform'),\n",
    "                         'tweedieregressor__power': Real(1.001, 1.99, 'uniform')},\n",
    "                        fit_params={'tweedieregressor__sample_weight': y_train['exposure']},\n",
    "                        n_iter=32,\n",
    "                        scoring=make_scorer(normalized_gini),\n",
    "                        cv=5,\n",
    "                        n_jobs=-1).fit(X_train, y_train['pure_premium']).best_estimator_\n",
    "model_scores['tweedie'] = eval_model(tweedie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing tree-based methods\n",
    "dt = DecisionTreeRegressor().fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['decision_tree'] = eval_model(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesRegressor(n_estimators=100, n_jobs=-1).fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['extra_trees'] = eval_model(et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1).fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['random_forest'] = eval_model(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = AdaBoostRegressor(n_estimators=100).fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['adaboost'] = eval_model(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators=100).fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['gbm'] = eval_model(gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbr = HistGradientBoostingRegressor(max_iter=100).fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['hist_gbm'] = eval_model(hgbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators=100, n_jobs=-1).fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['xgboost'] = eval_model(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMRegressor(n_estimators=100, n_jobs=-1).fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['lgbm'] = eval_model(lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbt = XGBRegressor(n_estimators=100, objective='reg:tweedie', n_jobs=-1).fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['tweedie-xgboost'] = eval_model(xgbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmt = LGBMRegressor(n_estimators=100, objective='tweedie', n_jobs=-1).fit(X_train, y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "model_scores['tweedie-lgbm'] = eval_model(lgbmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   11.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   12.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   15.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    9.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   15.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    9.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   14.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   10.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   15.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    7.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    7.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    9.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('colsample_bytree', 0.8980933014856265), ('gamma', 3.2516185888233133), ('learning_rate', 0.5696664381732708), ('max_depth', 5), ('n_estimators', 503), ('reg_lambda', 3.0339205452104507), ('subsample', 0.9505192100105055), ('tweedie_variance_power', 1.8777680756638517)])\n"
     ]
    }
   ],
   "source": [
    "# Trying a tuned XGBRegressor\n",
    "params = {'n_estimators': Integer(50, 750),\n",
    "          'max_depth': Integer(1, 6),\n",
    "          'learning_rate': Real(1e-7, 1.0, 'uniform'),\n",
    "          'gamma': Real(0.0, 20.0, 'uniform'),\n",
    "          'tweedie_variance_power': Real(1.01, 1.99, 'uniform'),\n",
    "          'subsample': Real(0.5, 1.0, 'uniform'),\n",
    "          'colsample_bytree': Real(0.5, 1.0, 'uniform'),\n",
    "          'reg_lambda': Real(0.0, 10.0, 'uniform')}\n",
    "\n",
    "xgb_opt = BayesSearchCV(\n",
    "    XGBRegressor(tree_method='hist', objective='reg:tweedie', n_jobs=-1),\n",
    "    params,\n",
    "    fit_params={'sample_weight': y_train['exposure']},\n",
    "    scoring=make_scorer(normalized_gini),\n",
    "    n_iter=32,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "xgb_opt.fit(X_train, y_train['pure_premium'])\n",
    "print(xgb_opt.best_params_)\n",
    "model_scores['xgb-tuned'] = eval_model(xgb_opt.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass n_splits=5, n_repeats=5 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('colsample_bytree', 0.8442454527010317), ('learning_rate', 0.8982509084659792), ('min_split_gain', 14.042854802280587), ('n_estimators', 466), ('num_leaves', 41), ('reg_lambda', 10.0), ('subsample', 0.890180570668657), ('subsample_freq', 1), ('tweedie_variance_power', 1.1669117671912612)])\n"
     ]
    }
   ],
   "source": [
    "# Trying a tuned LGBMRegressor\n",
    "params = {'n_estimators': Integer(50, 750),\n",
    "          'num_leaves': Integer(10, 62),\n",
    "          'learning_rate': Real(1e-7, 1.0, 'uniform'),\n",
    "          'min_split_gain': Real(0.0, 20.0, 'uniform'),\n",
    "          'tweedie_variance_power': Real(1.01, 1.99, 'uniform'),\n",
    "          'subsample': Real(0.5, 1.0, 'uniform'),\n",
    "          'subsample_freq': Integer(0, 5),\n",
    "          'colsample_bytree': Real(0.5, 1.0, 'uniform'),\n",
    "          'reg_lambda': Real(0.0, 10.0, 'uniform')}\n",
    "\n",
    "lgbm_opt = BayesSearchCV(\n",
    "    LGBMRegressor(objective='tweedie', n_jobs=-1),\n",
    "    params,\n",
    "    fit_params={'sample_weight': y_train['exposure']},\n",
    "    #scoring=make_scorer(normalized_gini),\n",
    "    n_iter=32,\n",
    "    cv=RepeatedKFold(5, 5),\n",
    "    n_jobs=1\n",
    ")\n",
    "lgbm_opt.fit(X_train, y_train['pure_premium'])\n",
    "print(lgbm_opt.best_params_)\n",
    "model_scores['lgbm-tuned'] = eval_model(lgbm_opt.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try adding our ensembles of frequency-severity models\n",
    "def eval_freq_sev_model(freq_model, sev_model, ind_model=None):\n",
    "    train_cost_pred = freq_model.predict(X_train) * sev_model.predict(X_train) * y_train['exposure']\n",
    "    test_cost_pred = freq_model.predict(X_test) * sev_model.predict(X_test) * y_test['exposure']\n",
    "    if ind_model is not None:\n",
    "        train_cost_pred *= ind_model.predict(X_train)\n",
    "        test_cost_pred *= ind_model.predict(X_test)\n",
    "    return {'train': gini(y_train['claim_cost'], train_cost_pred),\n",
    "            'test': gini(y_test['claim_cost'], test_cost_pred),\n",
    "            'norm_train': normalized_gini(y_train['claim_cost'], train_cost_pred),\n",
    "            'norm_test': normalized_gini(y_test['claim_cost'], test_cost_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('logisticregression__C', 2.4932123487012574), ('logisticregression__class_weight', None), ('logisticregression__l1_ratio', 1.0)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "# First with GLMs\n",
    "binom = BayesSearchCV(make_pipeline(StandardScaler(),\n",
    "                                    LogisticRegression(penalty='elasticnet', solver='saga', max_iter=500, n_jobs=1)),\n",
    "                      {'logisticregression__C': Real(0.0, 5.0, 'uniform'),\n",
    "                       'logisticregression__l1_ratio': Real(0.0, 1.0, 'uniform'),\n",
    "                       'logisticregression__class_weight': Categorical([None, 'balanced'])},\n",
    "                      n_iter=16,\n",
    "                      fit_params={'logisticregression__sample_weight': y_train['exposure']},\n",
    "                      cv=5,\n",
    "                      n_jobs=5).fit(X_train, y_train['claim_ind'])\n",
    "print(binom.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('poissonregressor__alpha', 0.04213590920992788)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:285: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    }
   ],
   "source": [
    "poisson = BayesSearchCV(make_pipeline(StandardScaler(),\n",
    "                                      PoissonRegressor()),\n",
    "                        {'poissonregressor__alpha': Real(1e-10, 1.0, 'uniform')},\n",
    "                        n_iter=16,\n",
    "                        fit_params={'poissonregressor__sample_weight': y_train['exposure']},\n",
    "                        cv=5,\n",
    "                        n_jobs=5).fit(X_train, y_train['frequency'])\n",
    "print(poisson.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('poissonregressor__alpha', 0.9719655830725147)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py:285: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    }
   ],
   "source": [
    "poisson2 = BayesSearchCV(make_pipeline(StandardScaler(),\n",
    "                                      PoissonRegressor()),\n",
    "                        {'poissonregressor__alpha': Real(1e-10, 50.0, 'uniform')},\n",
    "                        n_iter=16,\n",
    "                        fit_params={'poissonregressor__sample_weight': y_train['exposure'][claim_mask]},\n",
    "                        cv=5,\n",
    "                        n_jobs=5).fit(X_train[claim_mask], y_train['frequency'][claim_mask])\n",
    "print(poisson2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('gammaregressor__alpha', 0.0002999434191263241)])\n"
     ]
    }
   ],
   "source": [
    "gamma = BayesSearchCV(make_pipeline(StandardScaler(),\n",
    "                                    GammaRegressor()),\n",
    "                      {'gammaregressor__alpha': Real(1e-9, 10.0, 'uniform')},\n",
    "                      n_iter=16,\n",
    "                      fit_params={'gammaregressor__sample_weight': y_train['exposure'][claim_mask]},\n",
    "                      cv=5,\n",
    "                      n_jobs=5).fit(X_train[claim_mask], y_train['avg_cost'][claim_mask])\n",
    "print(gamma.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluating our freq-sev models\n",
    "binom = binom.best_estimator_\n",
    "poisson = poisson.best_estimator_\n",
    "poisson2 = poisson2.best_estimator_\n",
    "gamma = gamma.best_estimator_\n",
    "model_scores['freq-sev-glm'] = eval_freq_sev_model(poisson, gamma)\n",
    "model_scores['ind-freq-sev-glm'] = eval_freq_sev_model(poisson2, gamma, binom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's replace our GLMs with LGBM models\n",
    "params = {'n_estimators': Integer(50, 1000),\n",
    "          'num_leaves': Integer(10, 62),\n",
    "          'learning_rate': Real(1e-7, 1.0, 'uniform'),\n",
    "          'min_split_gain': Real(0.0, 20.0, 'uniform'),\n",
    "          'subsample': Real(0.5, 1.0, 'uniform'),\n",
    "          'colsample_bytree': Real(0.5, 1.0, 'uniform'),\n",
    "          'reg_lambda': Real(0.0, 20.0, 'uniform')}\n",
    "\n",
    "#lgbm_ind = BayesSearchCV(LGBMClassifier(subsample_freq=1, n_jobs=1),\n",
    "#                         params,\n",
    "#                         fit_params={'sample_weight': y_train['exposure']},\n",
    "#                         n_iter=32,\n",
    "#                         cv=5,\n",
    "#                         n_jobs=-1).fit(X_train, y_train['claim_ind'])\n",
    "lgbm_count = BayesSearchCV(LGBMRegressor(objective='poisson', subsample_freq=1, n_jobs=1),\n",
    "                         params,\n",
    "                         fit_params={'sample_weight': y_train['exposure']},\n",
    "                         n_iter=32,\n",
    "                         cv=5,\n",
    "                         n_jobs=-1).fit(X_train, y_train['frequency'])\n",
    "#lgbm_count2 = BayesSearchCV(LGBMRegressor(objective='poisson', subsample_freq=1, n_jobs=1),\n",
    "#                         params,\n",
    "#                         fit_params={'sample_weight': y_train['exposure'][claim_mask]},\n",
    "#                         n_iter=32,\n",
    "#                         cv=5,\n",
    "#                         n_jobs=-1).fit(X_train[claim_mask], y_train['frequency'][claim_mask])\n",
    "lgbm_sev = BayesSearchCV(LGBMRegressor(objective='gamma', subsample_freq=1, n_jobs=1),\n",
    "                         params,\n",
    "                         fit_params={'sample_weight': y_train['exposure'][claim_mask]},\n",
    "                         n_iter=32,\n",
    "                         cv=5,\n",
    "                         n_jobs=-1).fit(X_train[claim_mask], y_train['avg_cost'][claim_mask])\n",
    "\n",
    "#print(lgbm_count.best_params_, lgbm_count2.best_params_, lgbm_sev.best_params_, lgbm_ind.best_params_, sep='\\n')\n",
    "#lgbm_ind = lgbm_ind.best_estimator_\n",
    "lgbm_count = lgbm_count.best_estimator_\n",
    "#lgbm_count2 = lgbm_count2.best_estimator_\n",
    "lgbm_sev = lgbm_sev.best_estimator_\n",
    "model_scores['freq-sev-lgbm'] = eval_freq_sev_model(lgbm_count, lgbm_sev)\n",
    "#model_scores['ind-freq-sev-lgbm'] = eval_freq_sev_model(lgbm_count2, lgbm_sev, lgbm_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               model  train_gini_norm  test_gini_norm\n",
      "3            tweedie         0.235890        0.228799\n",
      "14        lgbm-tuned         0.904487        0.224042\n",
      "2              ridge         0.235182        0.221190\n",
      "1             linreg         0.235204        0.221186\n",
      "13      tweedie-lgbm         0.641960        0.200642\n",
      "7           adaboost         0.336690        0.199356\n",
      "12   tweedie-xgboost         0.299730        0.196280\n",
      "0              dummy         0.151139        0.181505\n",
      "10           xgboost         0.327906        0.181505\n",
      "18         xgb-tuned         0.929870        0.181505\n",
      "17     freq-sev-lgbm         0.763178        0.100128\n",
      "9           hist_gbm         0.216058        0.081064\n",
      "6      random_forest         0.973409        0.027838\n",
      "5        extra_trees         0.994418       -0.011462\n",
      "4      decision_tree         0.994418       -0.011746\n",
      "8                gbm        -0.071910       -0.013380\n",
      "11              lgbm         0.190758       -0.025777\n",
      "15      freq-sev-glm         0.478620       -0.037182\n",
      "16  ind-freq-sev-glm         0.234006       -0.076007\n"
     ]
    }
   ],
   "source": [
    "# Coercing scores to a dataframe\n",
    "df_scores = pd.DataFrame({'model': list(model_scores.keys()),\n",
    "                          'train_gini': [v['train'] for k, v in model_scores.items()],\n",
    "                          'test_gini': [v['test'] for k, v in model_scores.items()],\n",
    "                          'train_gini_norm': [v['norm_train'] for k, v in model_scores.items()],\n",
    "                          'test_gini_norm': [v['norm_test'] for k, v in model_scores.items()]\n",
    "                         })\n",
    "df_scores = df_scores.sort_values('test_gini_norm', ascending=False)\n",
    "print(df_scores[['model', 'train_gini_norm', 'test_gini_norm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "fig, ax = plt.subplots(figsize=(13, 9))\n",
    "ind = np.arange(df_scores.shape[0])\n",
    "width = 0.35\n",
    "ax.bar(ind, df_scores['train_gini_norm'], width, bottom=0, label='Train MSE')\n",
    "ax.bar(ind + width, df_scores['test_gini_norm'], width, bottom=0, label='Test MSE')\n",
    "\n",
    "ax.set_title('Train/Test Normalized Gini by Model')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(df_scores['model'], rotation=90.0)\n",
    "ax.legend()\n",
    "ax.autoscale_view()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should do CV rather than holding out to test model performance\n",
    "# Let's just use some hand-tuned lgbm and tweedie models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10804349444306859\n"
     ]
    }
   ],
   "source": [
    "tweedie_glm = make_pipeline(StandardScaler(), TweedieRegressor(alpha=0.1, power=1.9))\n",
    "tweedie_cv = cross_val_score(tweedie_glm,\n",
    "                             X,\n",
    "                             y['claim_cost'],\n",
    "                             scoring=make_scorer(normalized_gini),\n",
    "                             cv=RepeatedKFold(10, 5),\n",
    "                             n_jobs=-1,\n",
    "                             fit_params={'tweedieregressor__sample_weight': y['exposure']})\n",
    "print(np.mean(tweedie_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0034017891363120967\n"
     ]
    }
   ],
   "source": [
    "lgbm_base = LGBMRegressor(n_estimators=100,\n",
    "                          num_leaves=31,\n",
    "                          objective='tweedie',\n",
    "                          tweedie_variance_power=1.5,\n",
    "                         )\n",
    "lgbm_cv = cross_val_score(lgbm_base,\n",
    "                          X,\n",
    "                          y['claim_cost'],\n",
    "                          scoring=make_scorer(normalized_gini),\n",
    "                          cv=RepeatedKFold(10, 5),\n",
    "                          n_jobs=-1,\n",
    "                          fit_params={'sample_weight': y['exposure']})\n",
    "print(np.mean(lgbm_cv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
