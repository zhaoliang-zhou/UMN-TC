{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our packages\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_predict, cross_val_score, RandomizedSearchCV, cross_validate, KFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from sklearn.linear_model import TweedieRegressor, RidgeClassifierCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor, LGBMRanker, LGBMClassifier\n",
    "from catboost import CatBoostRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all our data\n",
    "df = pd.read_csv('InsNova_train.csv')\n",
    "df['pure_premium'] = df['claim_cost'] / df['exposure']\n",
    "df['pure_premium_rank'] = df['pure_premium'].rank(method='min').astype(np.int64)\n",
    "df['avg_cost'] = df['claim_cost'] / np.fmax(df['claim_count'], 1)\n",
    "df['frequency'] = df['claim_count'] / df['exposure']\n",
    "response_cols = ['exposure', 'claim_ind', 'claim_count', 'claim_cost', 'pure_premium','pure_premium_rank', 'avg_cost', 'frequency']\n",
    "X, y = df.drop(response_cols, axis=1), df[response_cols]\n",
    "X = X.drop('id', axis=1)\n",
    "\n",
    "# Adding a condensed veh_body column\n",
    "other_bodies = ['TRUCK', 'COUPE', 'MIBUS', 'PANVN', 'BUS', 'RDSTR', 'MCARA', 'CONVT']\n",
    "X['veh_body2'] = np.where(X['veh_body'].isin(other_bodies), 'OTHER', X['veh_body'])\n",
    "\n",
    "# Creating Categorical dataset for LightGBM and CatBoost\n",
    "for i in ['veh_body', 'veh_body2', 'gender', 'area']:\n",
    "    X[i] = X[i].astype('category')\n",
    "    \n",
    "# Defining column transformers for later steps          \n",
    "get_cats = make_column_selector(dtype_include=pd.CategoricalDtype)\n",
    "get_floats = make_column_selector(dtype_include=np.float64)\n",
    "one_hot_scale = ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats),\n",
    "                                    ('scaler', StandardScaler(), get_floats)],\n",
    "                                  remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # sort rows on prediction column \n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:,0].argsort()][::-1,0]\n",
    "    pred_order = arr[arr[:,1].argsort()][::-1,0]\n",
    "    \n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) / np.fmax(1.0, np.sum(true_order))\n",
    "    L_pred = np.cumsum(pred_order) / np.fmax(1.0, np.sum(pred_order))\n",
    "    L_ones = np.linspace(1/n_samples, 1, n_samples)\n",
    "    \n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "    \n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred/G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our cross validation function\n",
    "def eval_model(model, n_iters=5, cv=10, fit_params=None):\n",
    "    start_time = time()\n",
    "    if fit_params is None:\n",
    "        if hasattr(model, 'named_steps'):\n",
    "            fit_params={list(model.named_steps.keys())[-1] + '__' + 'sample_weight': y['exposure']}\n",
    "        else:\n",
    "            fit_params={'sample_weight': y['exposure']}\n",
    "        if isinstance(model, CatBoostRegressor):\n",
    "            fit_params['cat_features'] = ['veh_body', 'area', 'gender']\n",
    "        if isinstance(model, LGBMRanker):\n",
    "            fit_params['group'] = [X.shape[0]]\n",
    "    ginis = cross_val_score(model,\n",
    "                            X,\n",
    "                            y['pure_premium'],\n",
    "                            cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "                            scoring=make_scorer(gini),\n",
    "                            fit_params=fit_params,\n",
    "                            n_jobs=1)\n",
    "    return np.array(ginis), time() - start_time\n",
    "\n",
    "def print_eval(result, model_name):\n",
    "    msg = '{} MRCV Gini: {:.4f} | 90% [{:.4f}, {:.4f}] | {:.1f} secs'\n",
    "    means = np.random.choice(result[0],\n",
    "                             (result[0].shape[0], 1000),\n",
    "                             True)\n",
    "    means = np.mean(means, axis=0)\n",
    "    msg = msg.format(model_name,\n",
    "                     np.mean(means),\n",
    "                     np.quantile(means, 0.05),\n",
    "                     np.quantile(means, 0.95),\n",
    "                     result[1])\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model MRCV Gini: -0.1555 | 90% [-0.1908, -0.1204] | 0.1 secs\n"
     ]
    }
   ],
   "source": [
    "# Dummy Regressor baseline\n",
    "dummy = DummyRegressor()\n",
    "dummy_cv = eval_model(dummy)\n",
    "print_eval(dummy_cv, 'Baseline Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweedie GLM 1.01 MRCV Gini: 0.1649 | 90% [0.1032, 0.2255] | 69.9 secs\n",
      "Tweedie GLM 1.08 MRCV Gini: 0.1908 | 90% [0.1142, 0.2613] | 71.3 secs\n",
      "Tweedie GLM 1.15 MRCV Gini: 0.1947 | 90% [0.1409, 0.2445] | 75.2 secs\n",
      "Tweedie GLM 1.22 MRCV Gini: 0.1808 | 90% [0.1135, 0.2429] | 63.9 secs\n",
      "Tweedie GLM 1.29 MRCV Gini: 0.1530 | 90% [0.0888, 0.2076] | 60.6 secs\n",
      "Tweedie GLM 1.36 MRCV Gini: 0.1868 | 90% [0.1237, 0.2502] | 50.9 secs\n",
      "Tweedie GLM 1.43 MRCV Gini: 0.1472 | 90% [0.0911, 0.2042] | 41.9 secs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-0fc79de5efe9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     tweedie = make_pipeline(one_hot_scale,\n\u001b[0;32m      4\u001b[0m                             TweedieRegressor(power=i, alpha=0.001, max_iter=5000, warm_start=True))\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtweedie_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweedie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweedie_cv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Tweedie GLM {:.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-174-00c72a208ac4>\u001b[0m in \u001b[0;36meval_model\u001b[1;34m(model, n_iters, cv, fit_params)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLGBMRanker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'group'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     ginis = cross_val_score(model,\n\u001b[0m\u001b[0;32m     14\u001b[0m                             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                             \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pure_premium'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[0;32m    402\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    240\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[0;32m    241\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[1;32m--> 242\u001b[1;33m     scores = parallel(\n\u001b[0m\u001b[0;32m    243\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    244\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             opt_res = scipy.optimize.minimize(\n\u001b[0m\u001b[0;32m    277\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                 options={\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    615\u001b[0m                                   **options)\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[0;32m    618\u001b[0m                                 callback=callback, **options)\n\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py\u001b[0m in \u001b[0;36mfunc\u001b[1;34m(coef, X, y, weights, alpha, family, link)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                 y_pred, devp = _y_pred_deviance_derivative(\n\u001b[0m\u001b[0;32m    263\u001b[0m                     \u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\linear_model\\_glm\\glm.py\u001b[0m in \u001b[0;36m_y_pred_deviance_derivative\u001b[1;34m(coef, X, y, weights, family, link)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlin_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0md1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlin_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeviance_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mdevp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py\u001b[0m in \u001b[0;36mdeviance_derivative\u001b[1;34m(self, y, y_pred, weights)\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mWeights\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mexposure\u001b[0m \u001b[0mto\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mvariance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0minverse\u001b[0m \u001b[0mproportional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \"\"\"\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_deviance_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py\u001b[0m in \u001b[0;36munit_deviance_derivative\u001b[1;34m(self, y, y_pred)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_variance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeviance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\_loss\\glm_distribution.py\u001b[0m in \u001b[0;36munit_variance\u001b[1;34m(self, y_pred)\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0munit_deviance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Baseline Tweedie model\n",
    "for i in np.linspace(1.01, 1.99, 15):\n",
    "    tweedie = make_pipeline(one_hot_scale,\n",
    "                            TweedieRegressor(power=i, alpha=0.001, max_iter=5000, warm_start=True))\n",
    "    tweedie_cv = eval_model(tweedie)\n",
    "    print_eval(tweedie_cv, 'Tweedie GLM {:.2f}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shallow Extra Trees MRCV Gini: 0.2486 | 90% [0.1791, 0.3143] | 11.4 secs\n"
     ]
    }
   ],
   "source": [
    "# Starting with Shallow Extra Trees\n",
    "et = LGBMRegressor(n_estimators=1000,\n",
    "                   max_depth=1,\n",
    "                   learning_rate=1,\n",
    "                   subsample=0.9,\n",
    "                   subsample_freq=1,\n",
    "                   boosting_type='rf',\n",
    "                   extra_trees=True,\n",
    "                   objective='tweedie',\n",
    "                   tweedie_variance_power=1.15,\n",
    "                   boost_from_average=False,\n",
    "                   n_jobs=-1)\n",
    "et_cv = eval_model(et)\n",
    "print_eval(et_cv, 'Shallow Extra Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shallow Random Forest MRCV Gini: 0.2445 | 90% [0.1796, 0.3154] | 11.0 secs\n"
     ]
    }
   ],
   "source": [
    "# Starting with Shallow Random Forest\n",
    "rf = LGBMRegressor(n_estimators=1000,\n",
    "                   max_depth=1,\n",
    "                   learning_rate=1,\n",
    "                   subsample=0.67,\n",
    "                   subsample_freq=1,\n",
    "                   boosting_type='rf',\n",
    "                   objective='tweedie',\n",
    "                   tweedie_variance_power=1.15,\n",
    "                   boost_from_average=False,\n",
    "                   n_jobs=-1)\n",
    "rf_cv = eval_model(rf)\n",
    "print_eval(rf_cv, 'Shallow Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shallow LGBM MRCV Gini: 0.2371 | 90% [0.1828, 0.2940] | 5.3 secs\n"
     ]
    }
   ],
   "source": [
    "# Trying LightGBM with Tweedie Objective\n",
    "lgbm = LGBMRegressor(n_estimators=400,\n",
    "                     learning_rate=0.001,\n",
    "                     max_depth=1,\n",
    "                     extra_trees=True,\n",
    "                     objective='tweedie',\n",
    "                     tweedie_variance_power=1.15,\n",
    "                     boost_from_average=False,\n",
    "                     n_jobs=-1)\n",
    "lgbm_cv = eval_model(lgbm)\n",
    "print_eval(lgbm_cv, 'Shallow LGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-883206ce1345>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                      \u001b[0mboost_from_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                      n_jobs=-1)\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mlgbm_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgbm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgbm_cv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Shallow LGBM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-00c72a208ac4>\u001b[0m in \u001b[0;36meval_model\u001b[1;34m(model, n_iters, cv, fit_params)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLGBMRanker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'group'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     ginis = cross_val_score(model,\n\u001b[0m\u001b[0;32m     14\u001b[0m                             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                             \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pure_premium'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[0;32m    402\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    240\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[0;32m    241\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[1;32m--> 242\u001b[1;33m     scores = parallel(\n\u001b[0m\u001b[0;32m    243\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    244\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    749\u001b[0m             callbacks=None, init_model=None):\n\u001b[0;32m    750\u001b[0m         \u001b[1;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m         super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    752\u001b[0m                                        \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m                                        \u001b[0meval_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    593\u001b[0m             \u001b[0minit_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbooster_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         self._Booster = train(params, train_set,\n\u001b[0m\u001b[0;32m    596\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    250\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   2368\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2369\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot update due to null objective function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2370\u001b[1;33m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[0;32m   2371\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2372\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Trying LightGBM with Tweedie Objective\n",
    "lgbm = LGBMRegressor(n_estimators=500,\n",
    "                     learning_rate=0.001,\n",
    "                     num_leaves=6,\n",
    "                     min_child_samples=1,\n",
    "                     min_child_weight=0.0,\n",
    "                     subsample=0.67, \n",
    "                     subsample_freq=1,\n",
    "                     objective='tweedie',\n",
    "                     tweedie_variance_power=1.15,\n",
    "                     boost_from_average=False,\n",
    "                     n_jobs=-1)\n",
    "lgbm_cv = eval_model(lgbm)\n",
    "print_eval(lgbm_cv, 'Shallow LGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Variable  Feat. Imp.\n",
      "1   veh_body      0.3118\n",
      "4       area      0.2892\n",
      "3     gender      0.1638\n",
      "0  veh_value      0.1156\n",
      "2    veh_age      0.0688\n",
      "5     dr_age      0.0412\n",
      "6  veh_body2      0.0096\n",
      "Shallow LGBM MRCV Gini: 0.1393 | 90% [0.1030, 0.1758] | 16.7 secs\n"
     ]
    }
   ],
   "source": [
    "# Let's look at feature importances\n",
    "lgbm = LGBMRegressor(n_estimators=500,\n",
    "                     #learning_rate=0.001,\n",
    "                     num_leaves=11,\n",
    "                     #min_child_samples=100,\n",
    "                     #min_child_weight=0.0,\n",
    "                     #max_bin=32,\n",
    "                     #colsample_bytree=3.0 / 7.0,\n",
    "                     subsample=0.67, \n",
    "                     subsample_freq=1,\n",
    "                     #boosting_type='rf',\n",
    "                     objective='tweedie',\n",
    "                     mc=[1,0,1,0,0,-1, 0],\n",
    "                     mc_method='advanced',\n",
    "                     #path_smooth=100.0,\n",
    "                     #cat_smooth=100.0,\n",
    "                     #min_data_per_group=25,\n",
    "                     tweedie_variance_power=1.8,\n",
    "                     boost_from_average=False,\n",
    "                     n_jobs=-1)\n",
    "lgbm.fit(X, y['pure_premium'], sample_weight=y['exposure'])\n",
    "feats = lgbm.feature_importances_ / lgbm.feature_importances_.sum()\n",
    "print(pd.DataFrame({'Variable': X.columns, 'Feat. Imp.': feats}).sort_values('Feat. Imp.', ascending=False))\n",
    "lgbm_cv = eval_model(lgbm)\n",
    "print_eval(lgbm_cv, 'Shallow LGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.954764772038274e-06 16023.653424521231\n"
     ]
    }
   ],
   "source": [
    "print(lgbm.predict(X).min(), lgbm.predict(X).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for _ in range(15):\n",
    "    preds.append(cross_val_predict(lgbm,\n",
    "                                   X,\n",
    "                                   y['pure_premium'],\n",
    "                                   fit_params={'sample_weight':y['exposure']},\n",
    "                                   cv=KFold(5, shuffle=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1848789435901359"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini(y['claim_cost'], y['exposure'] * np.vstack(preds).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we mask the bottom 50 % of observations?\n",
    "def gini_mask(y_true, y_pred):\n",
    "    y_pred = np.where(y_pred < np.quantile(y_pred, 0.5), 0.0, y_pred)\n",
    "\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # sort rows on prediction column \n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:,0].argsort()][::-1,0]\n",
    "    pred_order = arr[arr[:,1].argsort()][::-1,0]\n",
    "    \n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) / np.fmax(1.0, np.sum(true_order))\n",
    "    L_pred = np.cumsum(pred_order) / np.fmax(1.0, np.sum(pred_order))\n",
    "    L_ones = np.linspace(1/n_samples, 1, n_samples)\n",
    "    \n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "    \n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred/G_true\n",
    "\n",
    "# Defining our cross validation function\n",
    "def eval_model_mask(model, n_iters=5, cv=10, fit_params=None):\n",
    "    start_time = time()\n",
    "    if fit_params is None:\n",
    "        if hasattr(model, 'named_steps'):\n",
    "            fit_params={list(model.named_steps.keys())[-1] + '__' + 'sample_weight': y['exposure']}\n",
    "        else:\n",
    "            fit_params={'sample_weight': y['exposure']}\n",
    "        if isinstance(model, CatBoostRegressor):\n",
    "            fit_params['cat_features'] = ['veh_body', 'area', 'gender']\n",
    "        if isinstance(model, LGBMRanker):\n",
    "            fit_params['group'] = [X.shape[0]]\n",
    "    ginis = cross_val_score(model,\n",
    "                            X,\n",
    "                            y['pure_premium'],\n",
    "                            cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "                            scoring=make_scorer(gini_mask),\n",
    "                            fit_params=fit_params,\n",
    "                            n_jobs=1)\n",
    "    return np.array(ginis), time() - start_time\n",
    "\n",
    "def print_eval_mask(result, model_name):\n",
    "    msg = '{} MRCV Gini: {:.4f} | 90% [{:.4f}, {:.4f}] | {:.1f} secs'\n",
    "    means = np.random.choice(result[0],\n",
    "                             (result[0].shape[0], 1000),\n",
    "                             True)\n",
    "    means = np.mean(means, axis=0)\n",
    "    msg = msg.format(model_name,\n",
    "                     np.mean(means),\n",
    "                     np.quantile(means, 0.05),\n",
    "                     np.quantile(means, 0.95),\n",
    "                     result[1])\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shallow Extra Trees MRCV Gini: 0.2027 | 90% [0.1313, 0.2740] | 8.3 secs\n",
      "Shallow Extra Trees MRCV Gini: 0.2302 | 90% [0.1844, 0.2805] | 7.3 secs\n"
     ]
    }
   ],
   "source": [
    "# Starting with Shallow Extra Trees\n",
    "lgbm = LGBMRegressor(n_estimators=500,\n",
    "                     learning_rate=1.0,\n",
    "                     num_leaves=2,\n",
    "                     #min_child_samples=100,\n",
    "                     #min_child_weight=0.0,\n",
    "                     #max_bin=32,\n",
    "                     #colsample_bytree=3.0 / 7.0,\n",
    "                     subsample=0.67, \n",
    "                     subsample_freq=1,\n",
    "                     boosting_type='rf',\n",
    "                     objective='tweedie',\n",
    "                     mc=[1,0,1,0,0,-1, 0],\n",
    "                     mc_method='advanced',\n",
    "                     #path_smooth=100.0,\n",
    "                     #cat_smooth=100.0,\n",
    "                     #min_data_per_group=25,\n",
    "                     tweedie_variance_power=1.8,\n",
    "                     boost_from_average=False,\n",
    "                     n_jobs=-1)\n",
    "lgbm_cv = eval_model_mask(lgbm)\n",
    "print_eval_mask(lgbm_cv, 'Shallow Extra Trees')\n",
    "lgbm_cv = eval_model(lgbm)\n",
    "print_eval(lgbm_cv, 'Shallow Extra Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Variable  Feat. Imp.\n",
      "0        veh_value    0.136470\n",
      "2           dr_age    0.106078\n",
      "1          veh_age    0.099942\n",
      "12  veh_body_SEDAN    0.069842\n",
      "16        gender_F    0.054939\n",
      "13  veh_body_STNWG    0.052309\n",
      "18          area_A    0.052016\n",
      "6   veh_body_HBACK    0.043542\n",
      "7   veh_body_HDTOP    0.040035\n",
      "17        gender_M    0.040035\n",
      "20          area_C    0.038282\n",
      "23          area_F    0.036821\n",
      "15    veh_body_UTE    0.035944\n",
      "21          area_D    0.035359\n",
      "19          area_B    0.035359\n",
      "22          area_E    0.034775\n",
      "14  veh_body_TRUCK    0.031268\n",
      "5   veh_body_COUPE    0.030099\n",
      "9   veh_body_MIBUS    0.018118\n",
      "10  veh_body_PANVN    0.008767\n",
      "3     veh_body_BUS    0.000000\n",
      "11  veh_body_RDSTR    0.000000\n",
      "8   veh_body_MCARA    0.000000\n",
      "4   veh_body_CONVT    0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shallow LGBM MRCV Gini: nan | 90% [nan, nan] | 1.6 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 751, in fit\n",
      "    super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\sklearn.py\", line 595, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\engine.py\", line 231, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 1988, in __init__\n",
      "    _safe_call(_LIB.LGBM_BoosterCreate(\n",
      "  File \"C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\lightgbm\\basic.py\", line 55, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Check failed: (static_cast<size_t>(train_data_->num_total_features())) == (config->monotone_constraints.size()) at D:\\a\\1\\s\\python-package\\compile\\src\\boosting\\gbdt.cpp, line 47 .\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "X_cat = pd.get_dummies(X.copy(), columns=['veh_body', 'gender', 'area'])\n",
    "mon_dict = {'veh_value': 1, 'dr_age': -1}\n",
    "mon_constraints = [mon_dict[i] if i in list(mon_dict.keys()) else 0 for i in X_cat.columns]\n",
    "lgbm = LGBMRegressor(n_estimators=500,\n",
    "                     learning_rate=0.001,\n",
    "                     num_leaves=8,\n",
    "                     min_child_samples=100,\n",
    "                     min_child_weight=0.0,\n",
    "                     subsample=0.67, \n",
    "                     subsample_freq=1,\n",
    "                     colsample_bytree=0.25,\n",
    "                     boosting_type='rf',\n",
    "                     objective='tweedie',\n",
    "                     mc=mon_constraints,\n",
    "                     mc_method='advanced',\n",
    "                     min_data_per_group=25,\n",
    "                     tweedie_variance_power=1.15,\n",
    "                     boost_from_average=False,\n",
    "                     n_jobs=-1)\n",
    "lgbm.fit(X_cat, y['pure_premium'], sample_weight=y['exposure'])\n",
    "feats = lgbm.feature_importances_ / lgbm.feature_importances_.sum()\n",
    "print(pd.DataFrame({'Variable': X_cat.columns, 'Feat. Imp.': feats}).sort_values('Feat. Imp.', ascending=False))\n",
    "lgbm_cv = eval_model(lgbm)\n",
    "print_eval(lgbm_cv, 'Shallow LGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost MRCV Gini: 0.1971 | 90% [0.1421, 0.2523] | 78.8 secs\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "cb = CatBoostRegressor(n_estimators=500,\n",
    "                       learning_rate=0.0001,\n",
    "                       max_depth=3,\n",
    "                       subsample=0.8,\n",
    "                       objective='Tweedie:variance_power=1.15',\n",
    "                       boost_from_average=False,\n",
    "                       verbose=0,\n",
    "                       thread_count=-1)\n",
    "cb_cv = eval_model(cb)\n",
    "print_eval(cb_cv, 'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boost_from_average=False, boosting_type='rf', extra_trees=True,\n",
       "              learning_rate=1, max_depth=1, n_estimators=1000,\n",
       "              objective='tweedie', subsample=0.9, subsample_freq=1,\n",
       "              tweedie_variance_power=1.15)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepping a submission\n",
    "et = LGBMRegressor(n_estimators=1000,\n",
    "                   max_depth=1,\n",
    "                   learning_rate=1,\n",
    "                   subsample=0.9,\n",
    "                   subsample_freq=1,\n",
    "                   boosting_type='rf',\n",
    "                   extra_trees=True,\n",
    "                   objective='tweedie',\n",
    "                   tweedie_variance_power=1.15,\n",
    "                   boost_from_average=False,\n",
    "                   n_jobs=-1)\n",
    "et.fit(X, y['pure_premium'], sample_weight=y['exposure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our predicstions\n",
    "df_test = pd.read_csv('InsNova_test.csv')\n",
    "X_test = df_test.drop(['exposure', 'id'], axis=1)\n",
    "\n",
    "# Creating Categorical dataset for LightGBM and CatBoost\n",
    "for i in ['veh_body', 'gender', 'area']:\n",
    "    X_test[i] = X_test[i].astype('category')\n",
    "\n",
    "df_test['claim_cost'] = df_test['exposure'] * et.predict(X_test)\n",
    "df_test['id'] = np.arange(df_test.shape[0])\n",
    "df_test['id'] = df_test['id'].astype(int)\n",
    "df_test['id'] += 1\n",
    "df_test[['id', 'claim_cost']].to_csv('shallow_et_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cats(x):\n",
    "    return [i for i in np.arange(x.shape[1]) if not isinstance(x[0, i], float) and not isinstance(x[0, i], int)]\n",
    "\n",
    "def get_floats(x):\n",
    "    return [i for i in np.arange(x.shape[1]) if isinstance(x[0, i], float)]\n",
    "\n",
    "one_hot_scale = ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats),\n",
    "                                   ('scaler', StandardScaler(), get_floats)],\n",
    "                                    remainder='passthrough')\n",
    "\n",
    "class ScaledTweedieRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = make_pipeline(one_hot_scale, TweedieRegressor(**kwargs))\n",
    "        \n",
    "    def fit(self, X, y, sample_weight):\n",
    "        self.model.fit(X, y, tweedieregressor__sample_weight=sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return self.model.named_steps['tweedieregressor'].get_params(deep)\n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        self.model.named_steps['tweedieregressor'].set_params(**kwargs)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweedieLGBMRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = LGBMRegressor(**kwargs)\n",
    "        \n",
    "    def fit(self, X, y, sample_weight):\n",
    "        _X = pd.DataFrame(X)\n",
    "        for i in [1, 3, 4]:\n",
    "            _X.iloc[:, i] = _X.iloc[:, i].astype('category')\n",
    "        _X.iloc[:, 0] = _X.iloc[:, 0].astype(np.float64)\n",
    "        _X.iloc[:, 2] = _X.iloc[:, 2].astype(np.int64)\n",
    "        _X.iloc[:, 5] = _X.iloc[:, 5].astype(np.int64)\n",
    "        if _X.shape[1] > 6:\n",
    "            _X.iloc[:, 6] = _X.iloc[:, 6].astype(np.float64)\n",
    "            _X.iloc[:, 7] = _X.iloc[:, 7].astype(np.float64)\n",
    "            _X.iloc[:, 8] = _X.iloc[:, 8].astype(np.float64)\n",
    "        self.model.fit(_X, y, sample_weight=sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _X = pd.DataFrame(X)\n",
    "        for i in [1, 3, 4]:\n",
    "            _X.iloc[:, i] = _X.iloc[:, i].astype('category')\n",
    "        _X.iloc[:, 0] = _X.iloc[:, 0].astype(np.float64)\n",
    "        _X.iloc[:, 2] = _X.iloc[:, 2].astype(np.int64)\n",
    "        _X.iloc[:, 5] = _X.iloc[:, 5].astype(np.int64)\n",
    "        if _X.shape[1] > 6:\n",
    "            _X.iloc[:, 6] = _X.iloc[:, 6].astype(np.float64)\n",
    "            _X.iloc[:, 7] = _X.iloc[:, 7].astype(np.float64)\n",
    "            _X.iloc[:, 8] = _X.iloc[:, 8].astype(np.float64)\n",
    "        return self.model.predict(_X)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return self.model.get_params(deep)\n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        self.model.set_params(**kwargs)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingCVRegressor(cv=15,\n",
       "                    meta_regressor=TweedieLGBMRegressor(boost_from_average=False,\n",
       "                                                        boosting_type='rf',\n",
       "                                                        class_weight=None,\n",
       "                                                        colsample_bytree=1.0,\n",
       "                                                        importance_type='split',\n",
       "                                                        learning_rate=1,\n",
       "                                                        max_depth=1,\n",
       "                                                        min_child_samples=2,\n",
       "                                                        min_child_weight=0.001,\n",
       "                                                        min_split_gain=0.0,\n",
       "                                                        n_estimators=1000,\n",
       "                                                        n_jobs=-1,\n",
       "                                                        num_leaves=31,\n",
       "                                                        objective='tweedie',\n",
       "                                                        random_state=None,\n",
       "                                                        reg_alpha=0...\n",
       "                                                     min_child_samples=2,\n",
       "                                                     min_child_weight=0.001,\n",
       "                                                     min_split_gain=0.0,\n",
       "                                                     n_estimators=500,\n",
       "                                                     n_jobs=-1, num_leaves=31,\n",
       "                                                     objective='tweedie',\n",
       "                                                     random_state=None,\n",
       "                                                     reg_alpha=0.0,\n",
       "                                                     reg_lambda=0.0,\n",
       "                                                     silent=True,\n",
       "                                                     subsample=0.67,\n",
       "                                                     subsample_for_bin=200000,\n",
       "                                                     subsample_freq=1,\n",
       "                                                     tweedie_variance_power=1.15),\n",
       "                                <catboost.core.CatBoostRegressor object at 0x000002C12108F280>],\n",
       "                    use_features_in_secondary=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a stacking regressor\n",
    "base_estimators = [ScaledTweedieRegressor(power=1.15, alpha=0.1, max_iter=1000),\n",
    "                   TweedieLGBMRegressor(n_estimators=500,\n",
    "                                          learning_rate=0.001,\n",
    "                                          max_depth=1,\n",
    "                                          extra_trees=True,\n",
    "                                          objective='tweedie',\n",
    "                                          min_child_samples=2,\n",
    "                                          subsample=0.67,\n",
    "                                          subsample_freq=1,\n",
    "                                          tweedie_variance_power=1.15,\n",
    "                                          boost_from_average=False,\n",
    "                                          n_jobs=-1),\n",
    "                    CatBoostRegressor(n_estimators=300,\n",
    "                                      learning_rate=0.0001,\n",
    "                                      min_child_samples=1,\n",
    "                                      max_depth=1,\n",
    "                                      subsample=0.8,\n",
    "                                      objective='Tweedie:variance_power=1.15',\n",
    "                                      boost_from_average=False,\n",
    "                                      verbose=0,\n",
    "                                      cat_features=[1, 3, 4],\n",
    "                                      thread_count=-1)]\n",
    "meta_estimator = TweedieLGBMRegressor(n_estimators=1000,\n",
    "                                       max_depth=1,\n",
    "                                       learning_rate=1,\n",
    "                                       min_child_samples=2,\n",
    "                                       subsample=0.9,\n",
    "                                       subsample_freq=1,\n",
    "                                       boosting_type='rf',\n",
    "                                       objective='tweedie',\n",
    "                                       tweedie_variance_power=1.15,\n",
    "                                       boost_from_average=False,\n",
    "                                       n_jobs=-1)\n",
    "model = StackingCVRegressor(base_estimators,\n",
    "                            meta_estimator,\n",
    "                            cv=15,\n",
    "                            use_features_in_secondary=True,\n",
    "                            n_jobs=1)\n",
    "model.fit(X, y['pure_premium'], sample_weight=y['exposure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our predicstions\n",
    "df_test = pd.read_csv('InsNova_test.csv')\n",
    "X_test = df_test.drop(['exposure', 'id'], axis=1)\n",
    "\n",
    "# Creating Categorical dataset for LightGBM and CatBoost\n",
    "for i in ['veh_body', 'gender', 'area']:\n",
    "    X_test[i] = X_test[i].astype('category')\n",
    "\n",
    "df_test['claim_cost'] = df_test['exposure'] * model.predict(X_test)\n",
    "df_test['id'] = np.arange(df_test.shape[0])\n",
    "df_test['id'] = df_test['id'].astype(int)\n",
    "df_test['id'] += 1\n",
    "df_test[['id', 'claim_cost']].to_csv('stacked_predictions2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming data\n",
    "X = one_hot_scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our Tweedie loss\n",
    "@tf.function\n",
    "def tweedie_loss(y_true, y_pred):\n",
    "    p = 1.8\n",
    "    loss = 2.0 * (tf.pow(y_true, 2-p) / ((1-p) * (2-p)) - y_true * tf.pow(y_pred, 1-p) / (1-p) + tf.pow(y_pred, 2-p) / (2-p))\n",
    "    return tf.reduce_mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402.92645177075065\n",
      "tf.Tensor(402.92645177075065, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_tweedie_deviance\n",
    "y_preds = np.array([0.1, 0.1, 1.0, 10.0])\n",
    "y_true = np.array([0.0, 100.0, 20.0, 0.0])\n",
    "print(mean_tweedie_deviance(y_true, y_preds, power=1.8))\n",
    "print(tweedie_loss(y_true, y_preds))\n",
    "# Looks like our implementation is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAA485AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAABC8C160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAEC5B040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAABE38940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAB2F78430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAF0064C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAEB9CF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAA993AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAEC5B940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAA485160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAE8199D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAB2B74940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAB2FAF790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAABC8C670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAE649D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAD5C61F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAABE38B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAA485F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAF006700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAEB9C8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAB24AC8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAABDAA5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAE819040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAAA990DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CAB2ADC280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Tensorflow MRCV Gini: 0.2448 | 90% [0.1893, 0.3008] | 12.2 secs\n"
     ]
    }
   ],
   "source": [
    "# Trying simple GLM\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input((21,)),\n",
    "        tf.keras.layers.Dense(1,\n",
    "                              kernel_initializer='zeros',\n",
    "                              bias_initializer=tf.keras.initializers.constant(np.log(y['pure_premium'].mean())),\n",
    "                              kernel_regularizer=tf.keras.regularizers.L2(1.0),\n",
    "                              activation=tf.keras.activations.exponential)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.001), loss=tweedie_loss)\n",
    "    return model\n",
    "tf_model = tf.keras.wrappers.scikit_learn.KerasRegressor(create_model, epochs=20, batch_size=4096, verbose=0)\n",
    "tf_model_cv = eval_model(tf_model)\n",
    "print_eval(tf_model_cv, 'Tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweedie_model = TweedieRegressor(power=1.8, alpha=1.0, link='log', max_iter=10000)\n",
    "#tweedie_model_cv = eval_model(tweedie_model)\n",
    "#print_eval(tweedie_model_cv, 'Tweedie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_model = tf.keras.wrappers.scikit_learn.KerasRegressor(create_model, epochs=20, batch_size=2048, verbose=0).fit(X, y['pure_premium'])\n",
    "tweedie_model = TweedieRegressor(power=1.8, alpha=0.0, link='log', max_iter=10000).fit(X, y['pure_premium'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.17526994e+01  1.25038164e+00  1.17368538e+00  1.05126807e+00\n",
      " -4.13737090e-01  2.28885233e-01 -1.63109558e-01  5.93158004e-01\n",
      "  1.57360318e+00  7.62899124e-01  5.34459086e-01  5.39815702e-01\n",
      "  2.60458093e-01  1.29114805e-01  2.03070660e-02  1.08488873e+00\n",
      " -1.64894121e-01  9.17025052e-01 -2.18726717e-02  1.89307916e-01\n",
      " -3.64956350e-01] 5.483571571191364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-1.6895263e-03],\n",
       "        [ 7.5927797e-05],\n",
       "        [-3.4942046e-02],\n",
       "        [ 8.7109575e-04],\n",
       "        [-1.9080314e-03],\n",
       "        [-8.7680127e-03],\n",
       "        [-1.0869695e-02],\n",
       "        [-2.1996172e-04],\n",
       "        [ 2.0410961e-01],\n",
       "        [-4.8673566e-02],\n",
       "        [-6.7389361e-03],\n",
       "        [-2.0518079e-02],\n",
       "        [-1.2585604e-02],\n",
       "        [-1.6887544e-02],\n",
       "        [-6.1889980e-02],\n",
       "        [ 2.0125842e-01],\n",
       "        [-4.0973466e-02],\n",
       "        [ 4.4123136e-02],\n",
       "        [-8.3352149e-02],\n",
       "        [ 1.8733530e-01],\n",
       "        [-2.7435362e-01]], dtype=float32),\n",
       " array([6.55971], dtype=float32)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweedie_model.coef_, tweedie_model.intercept_)\n",
    "tf_model.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: 0.932\n",
      "test_balanced_accuracy: 0.500\n",
      "test_neg_brier_score: -0.063\n",
      "test_average_precision: 0.068\n",
      "test_precision: 0.000\n",
      "test_recall: 0.000\n",
      "test_f1: 0.000\n",
      "test_roc_auc: 0.500\n"
     ]
    }
   ],
   "source": [
    "# Masking bench mark\n",
    "dummy_mask_cv = cross_validate(DummyClassifier(strategy='prior'),\n",
    "                               X_transform,\n",
    "                               y['claim_ind'],\n",
    "                               cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "                               verbose=1,\n",
    "                               scoring=['accuracy',\n",
    "                                        'balanced_accuracy',\n",
    "                                        'neg_brier_score',\n",
    "                                        'average_precision',\n",
    "                                        'precision',\n",
    "                                        'recall',\n",
    "                                        'f1',\n",
    "                                        'roc_auc']\n",
    "                             )\n",
    "for k, v in dummy_mask_cv.items():\n",
    "    if k in ['fit_time', 'score_time']:\n",
    "        continue\n",
    "    print('{}: {:.3f}'.format(k, v.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: 0.932\n",
      "test_balanced_accuracy: 0.500\n",
      "test_neg_brier_score: -0.064\n",
      "test_average_precision: 0.078\n",
      "test_precision: 0.000\n",
      "test_recall: 0.000\n",
      "test_f1: 0.000\n",
      "test_roc_auc: 0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   29.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Testing masking with LogisticRegression\n",
    "X_transform = one_hot_scale.fit_transform(X)\n",
    "logit_mask = LogisticRegressionCV(Cs=[0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "                                  cv=10,\n",
    "                                  scoring='f1',\n",
    "                                  max_iter=1000,\n",
    "                                  n_jobs=-1)\n",
    "logit_mask_cv = cross_validate(logit_mask,\n",
    "                               X_transform,\n",
    "                               y['claim_ind'],\n",
    "                               fit_params={'sample_weight': y['exposure']},\n",
    "                               cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "                               verbose=1,\n",
    "                               scoring=['accuracy',\n",
    "                                        'balanced_accuracy',\n",
    "                                        'neg_brier_score',\n",
    "                                        'average_precision',\n",
    "                                        'precision',\n",
    "                                        'recall',\n",
    "                                        'f1',\n",
    "                                        'roc_auc']\n",
    "                             )\n",
    "for k, v in logit_mask_cv.items():\n",
    "    if k in ['fit_time', 'score_time']:\n",
    "        continue\n",
    "    print('{}: {:.3f}'.format(k, v.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cross_val_predict(logit_mask, X_transform, y['claim_ind'], fit_params={'sample_weight': y['exposure']}, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: 0.932\n",
      "test_balanced_accuracy: 0.500\n",
      "test_neg_brier_score: -0.072\n",
      "test_average_precision: 0.080\n",
      "test_precision: 0.000\n",
      "test_recall: 0.000\n",
      "test_f1: 0.000\n",
      "test_roc_auc: 0.540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Adding masking to our predictions if we don't think there will be a claim\n",
    "lgbm_mask = LGBMClassifier(n_estimators=1000,\n",
    "                           learning_rate=1.0,\n",
    "                           num_leaves=2,\n",
    "                           boosting_type='rf',\n",
    "                           subsample=0.5,\n",
    "                           subsample_freq=1,\n",
    "                           extra_trees=True,\n",
    "                           boost_from_average=False,\n",
    "                           #is_unbalance=True,\n",
    "                           n_jobs=-1)\n",
    "lgbm_mask_cv = cross_validate(lgbm_mask,\n",
    "                               X,\n",
    "                               y['claim_ind'],\n",
    "                               fit_params={'sample_weight': y['exposure']},\n",
    "                               cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "                               scoring=['accuracy',\n",
    "                                        'balanced_accuracy',\n",
    "                                        'neg_brier_score',\n",
    "                                        'average_precision',\n",
    "                                        'precision',\n",
    "                                        'recall',\n",
    "                                        'f1',\n",
    "                                        'roc_auc']\n",
    "                             )\n",
    "for k, v in lgbm_mask_cv.items():\n",
    "    if k in ['fit_time', 'score_time']:\n",
    "        continue\n",
    "    print('{}: {:.3f}'.format(k, v.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: 0.862\n",
      "test_balanced_accuracy: 0.501\n",
      "test_neg_brier_score: -0.136\n",
      "test_average_precision: 0.070\n",
      "test_precision: 0.072\n",
      "test_recall: 0.083\n",
      "test_f1: 0.073\n",
      "test_roc_auc: 0.509\n"
     ]
    }
   ],
   "source": [
    "# Adding masking to our predictions if we don't think there will be a claim\n",
    "# We want to set high confidence negatives to 0\n",
    "lgbm_mask = LGBMClassifier(n_estimators=100,\n",
    "                           learning_rate=1.0,\n",
    "                           num_leaves=31,\n",
    "                           subsample=0.67,\n",
    "                           subsample_freq=1,\n",
    "                           max_bin=64,\n",
    "                           min_child_samples=1,\n",
    "                           n_jobs=-1)\n",
    "lgbm_mask_cv = cross_validate(lgbm_mask,\n",
    "                               X,\n",
    "                               y['claim_ind'],\n",
    "                               fit_params={'sample_weight': y['exposure']},\n",
    "                               cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "                               scoring=['accuracy',\n",
    "                                        'balanced_accuracy',\n",
    "                                        'neg_brier_score',\n",
    "                                        'average_precision',\n",
    "                                        'precision',\n",
    "                                        'recall',\n",
    "                                        'f1',\n",
    "                                        'roc_auc']\n",
    "                             )\n",
    "for k, v in lgbm_mask_cv.items():\n",
    "    if k in ['fit_time', 'score_time']:\n",
    "        continue\n",
    "    print('{}: {:.3f}'.format(k, v.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06784608580274215\n",
      "0.083812472357364\n"
     ]
    }
   ],
   "source": [
    "preds = cross_val_predict(lgbm_mask, X, y['claim_ind'], fit_params={'sample_weight': y['exposure']}, cv=10)\n",
    "print(y['claim_ind'].mean())\n",
    "print(preds.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    4.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    9.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    9.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=10, estimator=LGBMClassifier(class_weight='balanced'),\n",
       "              fit_params={'sample_weight': 0        0.241898\n",
       "1        0.856523\n",
       "2        0.417517\n",
       "3        0.626975\n",
       "4        0.089770\n",
       "           ...   \n",
       "22605    0.909445\n",
       "22606    0.999321\n",
       "22607    0.783724\n",
       "22608    0.841333\n",
       "22609    0.808868\n",
       "Name: exposure, Length: 22610, dtype: float64},\n",
       "              n_iter=32, scoring='f1',\n",
       "              search_spaces={'learning_rate': Real(low=1e-05, high=1.0, prior='uniform', transform='identity'),\n",
       "                             'n_estimators': Integer(low=100, high=1000, prior='uniform', transform='identity'),\n",
       "                             'num_leaves': Integer(low=2, high=11, prior='uniform', transform='identity'),\n",
       "                             'subsample': Real(low=0.5, high=1.0, prior='uniform', transform='identity')},\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What params tend to work for F1?\n",
    "params = {'n_estimators': Integer(100, 1000),\n",
    "          'learning_rate': Real(1e-5, 1.0, 'uniform'),\n",
    "          'subsample': Real(0.5, 1.0, 'uniform'),\n",
    "          'num_leaves': Integer(2, 11)}\n",
    "lgbm_mask = LGBMClassifier(class_weight='balanced', n_jobs=-1)\n",
    "bayes_mask = BayesSearchCV(lgbm_mask,\n",
    "                           params,\n",
    "                           n_iter=32,\n",
    "                           verbose=1,\n",
    "                           fit_params={'sample_weight': y['exposure']},\n",
    "                           cv=10,\n",
    "                           scoring='f1')\n",
    "bayes_mask.fit(X, y['claim_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('learning_rate', 1.0), ('n_estimators', 100), ('num_leaves', 2), ('subsample', 0.5)])\n"
     ]
    }
   ],
   "source": [
    "print(bayes_mask.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedStackingRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, stacker, masker):\n",
    "        self.stacker = stacker\n",
    "        self.masker = masker\n",
    "        \n",
    "    def fit(self, X, y, sample_weights):\n",
    "        self.stacker.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam tuning models\n",
    "params = {'tweedie_variance_power': np.linspace(1.01, 1.99, 10)}\n",
    "model = GridSearchCV(LGBMRegressor(boosting='rf',\n",
    "                                   n_estimators=250,\n",
    "                                   subsample=0.67,\n",
    "                                   max_depth=1,\n",
    "                                   subsample_freq=1,\n",
    "                                   objective='tweedie',\n",
    "                                   n_jobs=-1),\n",
    "                        params,\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=RepeatedKFold(n_splits=5, n_repeats=3))\n",
    "model_cv = cross_val_score(model,\n",
    "                           X,\n",
    "                           y['pure_premium'],\n",
    "                           scoring=make_scorer(gini),\n",
    "                           cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "                           verbose=1,\n",
    "                           n_jobs=1,\n",
    "                           fit_params={'sample_weight': y['exposure']})\n",
    "print(np.array(model_cv).round(4))\n",
    "print(np.mean(model_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping model submission\n",
    "params = {'tweedie_variance_power': np.linspace(1.01, 1.99, 10)}\n",
    "model = GridSearchCV(LGBMRegressor(boosting='rf',\n",
    "                                   n_estimators=2000,\n",
    "                                   subsample=0.67,\n",
    "                                   max_depth=1,\n",
    "                                   subsample_freq=1,\n",
    "                                   objective='tweedie',\n",
    "                                   n_jobs=-1),\n",
    "                        params,\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=RepeatedKFold(n_splits=5, n_repeats=3))\n",
    "model.fit(X, y['pure_premium'], sample_weight=y['exposure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating predictions\n",
    "# Getting our predicstions\n",
    "df_test = pd.read_csv('InsNova_test.csv')\n",
    "#df_test = pd.get_dummies(df_test, columns=['veh_body', 'veh_age', 'gender', 'area', 'dr_age'])\n",
    "X_test = df_test.drop(['exposure', 'id'], axis=1)\n",
    "for i in ['veh_body', 'gender', 'area']:\n",
    "    X_test[i] = X_test[i].astype('category')\n",
    "X_test['veh_value_2'] = X_test['veh_value'] ** 2\n",
    "df_test['claim_cost'] = df_test['exposure'] * model.best_estimator_.predict(X_test)\n",
    "df_test['id'] = np.arange(df_test.shape[0])\n",
    "df_test['id'] = df_test['id'].astype(int)\n",
    "df_test['id'] += 1\n",
    "df_test[['id', 'claim_cost']].to_csv('shallow_rf_predictions.csv', index=False)\n",
    "# Creating Categorical dataset for LightGBM and CatBoost\n",
    "for i in ['veh_body', 'gender', 'area']:\n",
    "    X[i] = X[i].astype('category')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
