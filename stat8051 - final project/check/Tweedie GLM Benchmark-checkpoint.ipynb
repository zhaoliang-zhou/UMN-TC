{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import TweedieRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data, preprocessing, and splitting\n",
    "df = pd.read_csv('InsNova_train.csv')\n",
    "df = pd.get_dummies(df, columns=['veh_body', 'veh_age', 'gender', 'area', 'dr_age'])\n",
    "df['pure_premium'] = df['claim_cost'] / df['exposure']\n",
    "df['avg_cost'] = df['claim_cost'] / np.fmax(df['claim_count'], 1)\n",
    "df['frequency'] = df['claim_count'] / df['exposure']\n",
    "response_cols = ['exposure', 'claim_ind', 'claim_count', 'claim_cost', 'pure_premium', 'avg_cost', 'frequency']\n",
    "X, y = df.drop(response_cols, axis=1), df[response_cols]\n",
    "X = X.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # sort rows on prediction column \n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:,0].argsort()][::-1,0]\n",
    "    pred_order = arr[arr[:,1].argsort()][::-1,0]\n",
    "    \n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) / np.fmax(1.0, np.sum(true_order))\n",
    "    L_pred = np.cumsum(pred_order) / np.fmax(1.0, np.sum(pred_order))\n",
    "    L_ones = np.linspace(1/n_samples, 1, n_samples)\n",
    "    \n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "    \n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred/G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, X, y, f_param=''):\n",
    "    scores = []\n",
    "    for _ in range(5):\n",
    "        preds = cross_val_predict(model, X, y['pure_premium'], cv=5, n_jobs=-1, fit_params={f_param +'sample_weight': y['exposure']})\n",
    "        preds *= y['exposure']\n",
    "        scores.append(gini(y['claim_cost'], preds))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1299108663532243\n"
     ]
    }
   ],
   "source": [
    "# Training a base Tweedie model\n",
    "base_model = TweedieRegressor(power=1.5)\n",
    "base_cv = cross_val_score(base_model,\n",
    "                           X,\n",
    "                           y['pure_premium'],\n",
    "                           scoring=make_scorer(gini),\n",
    "                           cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "                           n_jobs=1,\n",
    "                           fit_params={'sample_weight': y['exposure']})\n",
    "#base_cv = eval_model(base_model, X, y)\n",
    "print(np.mean(base_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15052526219937515\n"
     ]
    }
   ],
   "source": [
    "# Training a tuned model\n",
    "tuned_model= GridSearchCV(make_pipeline(StandardScaler(),\n",
    "                                         TweedieRegressor()),\n",
    "                        {'tweedieregressor__alpha': np.linspace(0.1, 1.0, 5),\n",
    "                         'tweedieregressor__power':  np.linspace(1.001, 1.99, 5)},\n",
    "                        scoring=make_scorer(gini),\n",
    "                        n_jobs=-1,\n",
    "                        cv=5)\n",
    "#tuned_cv = eval_model(tuned_model, X, y, 'tweedieregressor__')\n",
    "tuned_cv = cross_val_score(tuned_model,\n",
    "                           X,\n",
    "                          y['pure_premium'],\n",
    "                          scoring=make_scorer(gini),\n",
    "                          cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "                          n_jobs=1,\n",
    "                          fit_params={'tweedieregressor__sample_weight': y['exposure']})\n",
    "print(np.mean(tuned_cv))\n",
    "# Wow thats a pretty lame increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16830254256802135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                                       ('tweedieregressor',\n",
       "                                        TweedieRegressor())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'tweedieregressor__alpha': array([0.1  , 0.325, 0.55 , 0.775, 1.   ]),\n",
       "                         'tweedieregressor__power': array([1.001  , 1.24825, 1.4955 , 1.74275, 1.99   ])},\n",
       "             scoring=make_scorer(gini))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's just submit with Tweedie\n",
    "model = GridSearchCV(make_pipeline(StandardScaler(),\n",
    "                                   TweedieRegressor()),\n",
    "                        {'tweedieregressor__alpha': np.linspace(0.1, 1.0, 5),\n",
    "                         'tweedieregressor__power':  np.linspace(1.001, 1.99, 5)},\n",
    "                        scoring=make_scorer(gini),\n",
    "                        n_jobs=-1,\n",
    "                        cv=10)\n",
    "model_cv = cross_val_score(model,\n",
    "                           X,\n",
    "                           y['pure_premium'],\n",
    "                           scoring=make_scorer(gini),\n",
    "                           cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "                           n_jobs=1,\n",
    "                           fit_params={'tweedieregressor__sample_weight': y['exposure']})\n",
    "\n",
    "print(np.mean(model_cv))\n",
    "model.fit(X, y['pure_premium'], tweedieregressor__sample_weight=y['exposure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our predicstions\n",
    "df_test = pd.read_csv('InsNova_test.csv')\n",
    "df_test = pd.get_dummies(df_test, columns=['veh_body', 'veh_age', 'gender', 'area', 'dr_age'])\n",
    "X_test = df_test.drop(['exposure', 'id'], axis=1)\n",
    "df_test['claim_cost'] = df_test['exposure'] * model.best_estimator_.predict(X_test)\n",
    "df_test['id'] = np.arange(df_test.shape[0])\n",
    "df_test['id'] = df_test['id'].astype(int)\n",
    "df_test['id'] += 1\n",
    "df_test[['id', 'claim_cost']].to_csv('baseline_predictions.csv', index=False)\n",
    "tweedie_preds = df_test['claim_cost']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
