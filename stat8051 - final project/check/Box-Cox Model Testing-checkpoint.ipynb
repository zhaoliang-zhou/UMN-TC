{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, cross_val_score, GroupKFold\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.metrics import make_scorer, mean_poisson_deviance, mean_gamma_deviance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer, OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegressionCV, RidgeCV, PoissonRegressor, GammaRegressor, TweedieRegressor\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, model_type, **kwargs):\n",
    "        self.model_type = model_type\n",
    "        self.model_params = kwargs\n",
    "        self.model = self._resolve_estimator(model_type).set_params(**kwargs)\n",
    "        \n",
    "    def _resolve_estimator(self, func_name):\n",
    "        funcs = {'linear': LinearRegression(),\n",
    "                 'ridge': RidgeCV(),\n",
    "                 'gamma': GammaRegressor(),\n",
    "                 'poisson': PoissonRegressor(),\n",
    "                 'lgbm': LGBMRegressor()}\n",
    "\n",
    "        return funcs[func_name]\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _X, _y= X[y > 0.0].copy(), y[y > 0.0].copy()\n",
    "        if sample_weight is not None:\n",
    "            _sample_weight = sample_weight[y > 0.0].copy()\n",
    "        else:\n",
    "            _sample_weight = None\n",
    "        self.model.fit(_X, _y, _sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'model_type': self.model_type, **self.model.get_params(deep)}\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.model.set_params(params)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all our data\n",
    "df = pd.read_csv('InsNova_train.csv')\n",
    "df = df.sample(frac=1.0)\n",
    "df.loc[:, 'pure_premium'] = df['claim_cost'] / df['exposure']\n",
    "df.loc[:, 'severity'] = df['claim_cost'] / np.fmax(df['claim_count'], 1)\n",
    "df.loc[:, 'frequency'] = df['claim_count'] / df['exposure']\n",
    "\n",
    "# Getting CV inds\n",
    "cv = StratifiedKFold(15, shuffle=True, random_state=123)\n",
    "df.loc[:, 'fold'] = 0\n",
    "for fold, (_, test_inds) in enumerate(cv.split(df, df['claim_ind'])):\n",
    "    df.loc[test_inds, 'fold'] = fold\n",
    "    \n",
    "# Prepping box-cox transformers\n",
    "box_cox = {}\n",
    "# Frequency\n",
    "df.loc[:,'bc_frequency'] = df['frequency'].copy()\n",
    "box_cox['frequency'] = PowerTransformer(method='box-cox', standardize=False)\n",
    "df.loc[df['frequency'] > 0.0, 'bc_frequency'] = box_cox['frequency'].fit_transform(df.loc[df['frequency'] > 0.0, 'frequency'].values.reshape(-1, 1)).flatten()\n",
    "# Severity\n",
    "df.loc[:,'bc_severity'] = df['severity'].copy()\n",
    "box_cox['severity'] = PowerTransformer(method='box-cox', standardize=False)\n",
    "df.loc[df['severity'] > 0.0, 'bc_severity'] = box_cox['severity'].fit_transform(df.loc[df['severity'] > 0.0, 'severity'].values.reshape(-1, 1)).flatten()\n",
    "# Pure premium\n",
    "df.loc[:,'bc_pure_premium'] = df['pure_premium'].copy()\n",
    "box_cox['pure_premium'] = PowerTransformer(method='box-cox', standardize=False)\n",
    "df.loc[df['pure_premium'] > 0.0, 'bc_pure_premium'] = box_cox['pure_premium'].fit_transform(df.loc[df['pure_premium'] > 0.0, 'pure_premium'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Splitting into pred/response\n",
    "sev_mask = df['claim_ind'] == 1\n",
    "response_cols = ['fold',\n",
    "                 'exposure',\n",
    "                 'claim_ind',\n",
    "                 'claim_count',\n",
    "                 'claim_cost',\n",
    "                 'pure_premium',\n",
    "                 'severity',\n",
    "                 'frequency',\n",
    "                 'bc_pure_premium',\n",
    "                 'bc_severity',\n",
    "                 'bc_frequency']\n",
    "X, y = df.drop(response_cols, axis=1), df[response_cols]\n",
    "X = X.drop('id', axis=1)\n",
    "X.loc[:, 'exposure'] = y['exposure'].copy()\n",
    "\n",
    "# Adding a condensed veh_body column\n",
    "other_bodies = ['TRUCK', 'COUPE', 'MIBUS', 'PANVN', 'BUS', 'RDSTR', 'MCARA', 'CONVT']\n",
    "X.loc[:,'veh_body2'] = np.where(X['veh_body'].isin(other_bodies), 'OTHER', X['veh_body'])\n",
    "X.loc[:,'log_veh_value'] = np.log(X['veh_value'] + 0.1)\n",
    "X.loc[:,'log_veh_value^2'] = X['log_veh_value'] ** 2\n",
    "X.loc[:, 'intercept'] = 1.0\n",
    "\n",
    "# Creating Categorical dataset for LightGBM and CatBoost\n",
    "for i in ['veh_body', 'veh_body2', 'gender', 'area']:\n",
    "    X[i] = X[i].astype('category')\n",
    "X_sev = X[y['claim_cost'] > 0.0]\n",
    "y_sev = y[y['claim_cost'] > 0.0]\n",
    "lin_cols = ['veh_body2', 'veh_age', 'gender', 'area', 'dr_age', 'log_veh_value', 'exposure']\n",
    "boost_cols = ['veh_value', 'veh_body', 'veh_age', 'gender', 'area', 'dr_age']\n",
    "    \n",
    "# Defining column transformers for later steps          \n",
    "get_cats = make_column_selector(dtype_include=pd.CategoricalDtype)\n",
    "one_hot = lambda: ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough')\n",
    "\n",
    "# Initializing cross validated preds\n",
    "cv_preds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency Skew: 89.3497\n",
      "frequency Box-Cox Skew: 5.0385\n",
      "frequency Nonzero Skew: 26.0107\n",
      "frequency Nonzero Box-Cox Skew: 0.2592\n",
      "severity Skew: 19.8233\n",
      "severity Box-Cox Skew: 3.4541\n",
      "severity Nonzero Skew: 5.7075\n",
      "severity Nonzero Box-Cox Skew: 0.1227\n",
      "pure_premium Skew: 94.9824\n",
      "pure_premium Box-Cox Skew: 3.4612\n",
      "pure_premium Nonzero Skew: 24.9029\n",
      "pure_premium Nonzero Box-Cox Skew: 0.0773\n"
     ]
    }
   ],
   "source": [
    "# Checking skew of responses\n",
    "# Frequency\n",
    "for i in ['frequency', 'severity', 'pure_premium']:\n",
    "    print('{} Skew: {:.4f}'.format(i, y[i].skew()))\n",
    "    print('{} Box-Cox Skew: {:.4f}'.format(i, y['bc_' + i].skew()))\n",
    "    print('{} Nonzero Skew: {:.4f}'.format(i, y[i][y[i] > 0.0].skew()))\n",
    "    print('{} Nonzero Box-Cox Skew: {:.4f}'.format(i, y['bc_'+i][y[i] > 0.0].skew()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>exposure</th>\n",
       "      <th>claim_ind</th>\n",
       "      <th>claim_count</th>\n",
       "      <th>claim_cost</th>\n",
       "      <th>pure_premium</th>\n",
       "      <th>severity</th>\n",
       "      <th>frequency</th>\n",
       "      <th>bc_pure_premium</th>\n",
       "      <th>bc_severity</th>\n",
       "      <th>bc_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5344</th>\n",
       "      <td>13</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3391.995204</td>\n",
       "      <td>3394.263944</td>\n",
       "      <td>3391.995204</td>\n",
       "      <td>1.000669</td>\n",
       "      <td>3.505409</td>\n",
       "      <td>3.082749</td>\n",
       "      <td>0.000668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9583</th>\n",
       "      <td>7</td>\n",
       "      <td>0.600041</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1521.730732</td>\n",
       "      <td>2536.043048</td>\n",
       "      <td>1521.730732</td>\n",
       "      <td>1.666552</td>\n",
       "      <td>3.464824</td>\n",
       "      <td>3.000478</td>\n",
       "      <td>0.399917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9696</th>\n",
       "      <td>3</td>\n",
       "      <td>0.176853</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8398.339190</td>\n",
       "      <td>47487.686425</td>\n",
       "      <td>8398.339190</td>\n",
       "      <td>5.654414</td>\n",
       "      <td>3.765742</td>\n",
       "      <td>3.155132</td>\n",
       "      <td>0.822919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>6</td>\n",
       "      <td>0.407013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>203.092111</td>\n",
       "      <td>498.981943</td>\n",
       "      <td>203.092111</td>\n",
       "      <td>2.456924</td>\n",
       "      <td>3.176117</td>\n",
       "      <td>2.683534</td>\n",
       "      <td>0.592887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21329</th>\n",
       "      <td>6</td>\n",
       "      <td>0.251475</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>401.092731</td>\n",
       "      <td>1594.958234</td>\n",
       "      <td>401.092731</td>\n",
       "      <td>3.976532</td>\n",
       "      <td>3.393917</td>\n",
       "      <td>2.812219</td>\n",
       "      <td>0.748347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5307</th>\n",
       "      <td>12</td>\n",
       "      <td>0.522270</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>389.846069</td>\n",
       "      <td>746.445360</td>\n",
       "      <td>389.846069</td>\n",
       "      <td>1.914718</td>\n",
       "      <td>3.258775</td>\n",
       "      <td>2.807342</td>\n",
       "      <td>0.477669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19360</th>\n",
       "      <td>10</td>\n",
       "      <td>0.698509</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>608.245532</td>\n",
       "      <td>870.776828</td>\n",
       "      <td>608.245532</td>\n",
       "      <td>1.431621</td>\n",
       "      <td>3.288283</td>\n",
       "      <td>2.879128</td>\n",
       "      <td>0.301468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16671</th>\n",
       "      <td>10</td>\n",
       "      <td>0.201743</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1912.247657</td>\n",
       "      <td>9478.612227</td>\n",
       "      <td>1912.247657</td>\n",
       "      <td>4.956791</td>\n",
       "      <td>3.627214</td>\n",
       "      <td>3.025937</td>\n",
       "      <td>0.798047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9810</th>\n",
       "      <td>13</td>\n",
       "      <td>0.748715</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7534.820000</td>\n",
       "      <td>10063.673791</td>\n",
       "      <td>7534.820000</td>\n",
       "      <td>1.335622</td>\n",
       "      <td>3.633410</td>\n",
       "      <td>3.147453</td>\n",
       "      <td>0.251270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16520</th>\n",
       "      <td>12</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>692.755017</td>\n",
       "      <td>797.112495</td>\n",
       "      <td>692.755017</td>\n",
       "      <td>1.150641</td>\n",
       "      <td>3.271491</td>\n",
       "      <td>2.898404</td>\n",
       "      <td>0.130915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1534 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fold  exposure  claim_ind  claim_count   claim_cost  pure_premium  \\\n",
       "5344     13  0.999332          1            1  3391.995204   3394.263944   \n",
       "9583      7  0.600041          1            1  1521.730732   2536.043048   \n",
       "9696      3  0.176853          1            1  8398.339190  47487.686425   \n",
       "5179      6  0.407013          1            1   203.092111    498.981943   \n",
       "21329     6  0.251475          1            1   401.092731   1594.958234   \n",
       "...     ...       ...        ...          ...          ...           ...   \n",
       "5307     12  0.522270          1            1   389.846069    746.445360   \n",
       "19360    10  0.698509          1            1   608.245532    870.776828   \n",
       "16671    10  0.201743          1            1  1912.247657   9478.612227   \n",
       "9810     13  0.748715          1            1  7534.820000  10063.673791   \n",
       "16520    12  0.869081          1            1   692.755017    797.112495   \n",
       "\n",
       "          severity  frequency  bc_pure_premium  bc_severity  bc_frequency  \n",
       "5344   3391.995204   1.000669         3.505409     3.082749      0.000668  \n",
       "9583   1521.730732   1.666552         3.464824     3.000478      0.399917  \n",
       "9696   8398.339190   5.654414         3.765742     3.155132      0.822919  \n",
       "5179    203.092111   2.456924         3.176117     2.683534      0.592887  \n",
       "21329   401.092731   3.976532         3.393917     2.812219      0.748347  \n",
       "...            ...        ...              ...          ...           ...  \n",
       "5307    389.846069   1.914718         3.258775     2.807342      0.477669  \n",
       "19360   608.245532   1.431621         3.288283     2.879128      0.301468  \n",
       "16671  1912.247657   4.956791         3.627214     3.025937      0.798047  \n",
       "9810   7534.820000   1.335622         3.633410     3.147453      0.251270  \n",
       "16520   692.755017   1.150641         3.271491     2.898404      0.130915  \n",
       "\n",
       "[1534 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[y['claim_ind'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our gini function\n",
    "def gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # sort rows on prediction column \n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:,0].argsort()][::-1,0]\n",
    "    pred_order = arr[arr[:,1].argsort()][::-1,0]\n",
    "    \n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) / np.fmax(1.0, np.sum(true_order))\n",
    "    L_pred = np.cumsum(pred_order) / np.fmax(1.0, np.sum(pred_order))\n",
    "    L_ones = np.linspace(1/n_samples, 1, n_samples)\n",
    "    \n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "    \n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred/G_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency-Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3261523853858875\n",
      "1.3396928601089042\n",
      "1.3430064229949428\n"
     ]
    }
   ],
   "source": [
    "# Poisson Frequency\n",
    "poisson = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                        PolynomialFeatures(interaction_only=True, include_bias = False),\n",
    "                        StandardScaler(),\n",
    "                        PoissonRegressor(max_iter=1000))\n",
    "cv_preds['poisson'] = cross_val_predict(poisson,\n",
    "                                        X[lin_cols],\n",
    "                                        y['frequency'],\n",
    "                                        groups=y['fold'],\n",
    "                                        cv=GroupKFold(),\n",
    "                                        fit_params={'poissonregressor__sample_weight': y['exposure']},\n",
    "                                        n_jobs=-1)\n",
    "poisson.fit(X[lin_cols], y['frequency'], poissonregressor__sample_weight=y['exposure'])\n",
    "print(gini(y['claim_count'], cv_preds['poisson'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], poisson.predict(X[lin_cols])))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['poisson']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32027064616397166\n",
      "2.345125284833618\n",
      "2.345150752168261\n"
     ]
    }
   ],
   "source": [
    "# Box-Cox Poisson Frequency\n",
    "bcpoisson = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                          PolynomialFeatures(interaction_only=True, include_bias = False),\n",
    "                          StandardScaler(),\n",
    "                          PoissonRegressor(max_iter=1000))\n",
    "cv_preds['bcpoisson'] = cross_val_predict(bcpoisson,\n",
    "                                        X[lin_cols],\n",
    "                                        y['bc_frequency'],\n",
    "                                        groups=y['fold'],\n",
    "                                        cv=GroupKFold(),\n",
    "                                        fit_params={'poissonregressor__sample_weight': y['exposure']},\n",
    "                                        n_jobs=-1)\n",
    "# Inverse box-cox transform\n",
    "cv_preds['bcpoisson'][cv_preds['bcpoisson'] > 0.0] = box_cox['frequency'].inverse_transform(cv_preds['bcpoisson'][cv_preds['bcpoisson'] > 0.0].reshape(-1, 1)).flatten()\n",
    "bcpoisson.fit(X[lin_cols], y['bc_frequency'], poissonregressor__sample_weight=y['exposure'])\n",
    "preds = bcpoisson.predict(X[lin_cols])\n",
    "preds[preds > 0.0] = box_cox['frequency'].inverse_transform(preds[preds > 0.0].reshape(-1, 1)).flatten()\n",
    "\n",
    "print(gini(y['claim_count'], cv_preds['bcpoisson'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['bcpoisson']))\n",
    "# This doesn't seem much better even with box-cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3183444180810792\n",
      "2.346349368593401\n",
      "2.3528987057350066\n"
     ]
    }
   ],
   "source": [
    "# Box-Cox RidgeCV Frequency\n",
    "bcridgefreq = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                            PolynomialFeatures(interaction_only=True, include_bias = False),\n",
    "                            StandardScaler(),\n",
    "                            RidgeCV(alphas=np.linspace(1e-6, 10, 100)))\n",
    "cv_preds['bcridgefreq'] = cross_val_predict(bcridgefreq,\n",
    "                                            X[lin_cols],\n",
    "                                            y['bc_frequency'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'ridgecv__sample_weight': y['exposure']},\n",
    "                                            n_jobs=-1)\n",
    "# Inverse box-cox transform\n",
    "cv_preds['bcridgefreq'][cv_preds['bcridgefreq'] > 0.0] = box_cox['frequency'].inverse_transform(cv_preds['bcridgefreq'][cv_preds['bcridgefreq'] > 0.0].reshape(-1, 1)).flatten()\n",
    "bcridgefreq.fit(X[lin_cols], y['bc_frequency'], ridgecv__sample_weight=y['exposure'])\n",
    "preds = bcridgefreq.predict(X[lin_cols])\n",
    "preds[preds > 0.0] = box_cox['frequency'].inverse_transform(preds[preds > 0.0].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Clipping\n",
    "cv_preds['bcridgefreq'] = np.clip(cv_preds['bcridgefreq'], 1e-5, None)\n",
    "preds = np.clip(preds, 1e-5, None)\n",
    "\n",
    "print(gini(y['claim_count'], cv_preds['bcridgefreq'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['bcridgefreq']))\n",
    "# About the same as poisson above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3247824219219236\n",
      "1.3315598383407867\n",
      "1.3512104163962257\n"
     ]
    }
   ],
   "source": [
    "# RF (MAE)\n",
    "rf_mae_freq = LGBMRegressor(n_estimators=500,\n",
    "                            learning_rate=1.0,\n",
    "                            num_leaves=16,\n",
    "                            colsample_bytree=5 / 7,\n",
    "                            subsample=0.67, \n",
    "                            subsample_freq=1,\n",
    "                            boosting_type='rf',\n",
    "                            n_jobs=-1)\n",
    "cv_preds['rf_mae_freq'] = cross_val_predict(rf_mae_freq,\n",
    "                                            X[boost_cols],\n",
    "                                            y['frequency'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'sample_weight': y['exposure']},\n",
    "                                            n_jobs=1)\n",
    "# Fitting model\n",
    "rf_mae_freq.fit(X[boost_cols], y['frequency'], sample_weight=y['exposure'])\n",
    "preds = rf_mae_freq.predict(X[boost_cols])\n",
    "\n",
    "# Clipping\n",
    "cv_preds['rf_mae_freq'] = np.clip(cv_preds['rf_mae_freq'], 1e-5, None)\n",
    "preds = np.clip(preds, 1e-5, None)\n",
    "\n",
    "print(gini(y['claim_count'], cv_preds['rf_mae_freq'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['rf_mae_freq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32408855714154416\n",
      "1.3380944675703452\n",
      "1.348818954971869\n"
     ]
    }
   ],
   "source": [
    "# RF (Poisson)\n",
    "rf_poisson_freq = LGBMRegressor(n_estimators=500,\n",
    "                                learning_rate=1.0,\n",
    "                                num_leaves=16,\n",
    "                                colsample_bytree=5 / 7,\n",
    "                                subsample=0.67, \n",
    "                                subsample_freq=1,\n",
    "                                boosting_type='rf',\n",
    "                                objective='poisson',\n",
    "                                n_jobs=-1)\n",
    "cv_preds['rf_poisson_freq'] = cross_val_predict(rf_poisson_freq,\n",
    "                                                X[boost_cols],\n",
    "                                                y['frequency'],\n",
    "                                                groups=y['fold'],\n",
    "                                                cv=GroupKFold(),\n",
    "                                                fit_params={'sample_weight': y['exposure']},\n",
    "                                                n_jobs=1)\n",
    "\n",
    "# Fitting model\n",
    "rf_poisson_freq.fit(X[boost_cols], y['frequency'], sample_weight=y['exposure'])\n",
    "preds = rf_poisson_freq.predict(X[boost_cols])\n",
    "\n",
    "print(gini(y['claim_count'], cv_preds['rf_poisson_freq'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['rf_poisson_freq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32073082290644406\n",
      "2.3443398948805982\n",
      "2.3447794590174538\n"
     ]
    }
   ],
   "source": [
    "# BC RF (MAE)\n",
    "bcrf_mae_freq = LGBMRegressor(n_estimators=500,\n",
    "                                learning_rate=1.0,\n",
    "                                num_leaves=16,\n",
    "                                colsample_bytree=5 / 7,\n",
    "                                subsample=0.67, \n",
    "                                subsample_freq=1,\n",
    "                                boosting_type='rf',\n",
    "                                n_jobs=-1)\n",
    "cv_preds['bcrf_mae_freq'] = cross_val_predict(bcrf_mae_freq,\n",
    "                                            X[boost_cols],\n",
    "                                            y['bc_frequency'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'sample_weight': y['exposure']},\n",
    "                                            n_jobs=1)\n",
    "# Inverse box-cox transform\n",
    "cv_preds['bcrf_mae_freq'][cv_preds['bcrf_mae_freq'] > 0.0] = box_cox['frequency'].inverse_transform(cv_preds['bcrf_mae_freq'][cv_preds['bcrf_mae_freq'] > 0.0].reshape(-1, 1)).flatten()\n",
    "bcrf_mae_freq.fit(X[boost_cols], y['bc_frequency'], sample_weight=y['exposure'])\n",
    "preds = bcrf_mae_freq.predict(X[boost_cols])\n",
    "preds[preds > 0.0] = box_cox['frequency'].inverse_transform(preds[preds > 0.0].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Clipping\n",
    "cv_preds['bcrf_mae_freq'] = np.clip(cv_preds['bcrf_mae_freq'], 1e-5, None)\n",
    "preds = np.clip(preds, 1e-5, None)\n",
    "\n",
    "print(gini(y['claim_count'], cv_preds['bcrf_mae_freq'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['bcrf_mae_freq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32045991895989145\n",
      "2.344635860426992\n",
      "2.3448889782145668\n"
     ]
    }
   ],
   "source": [
    "# BC RF (Poisson)\n",
    "bcrf_poisson_freq = LGBMRegressor(n_estimators=500,\n",
    "                                learning_rate=1.0,\n",
    "                                num_leaves=16,\n",
    "                                colsample_bytree=5 / 7,\n",
    "                                subsample=0.67, \n",
    "                                subsample_freq=1,\n",
    "                                boosting_type='rf',\n",
    "                                objective='poisson',\n",
    "                                n_jobs=-1)\n",
    "cv_preds['bcrf_poisson_freq'] = cross_val_predict(bcrf_poisson_freq,\n",
    "                                            X[boost_cols],\n",
    "                                            y['bc_frequency'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'sample_weight': y['exposure']},\n",
    "                                            n_jobs=1)\n",
    "# Inverse box-cox transform\n",
    "cv_preds['bcrf_poisson_freq'][cv_preds['bcrf_poisson_freq'] > 0.0] = box_cox['frequency'].inverse_transform(cv_preds['bcrf_poisson_freq'][cv_preds['bcrf_poisson_freq'] > 0.0].reshape(-1, 1)).flatten()\n",
    "bcrf_poisson_freq.fit(X[boost_cols], y['bc_frequency'], sample_weight=y['exposure'])\n",
    "preds = bcrf_poisson_freq.predict(X[boost_cols])\n",
    "preds[preds > 0.0] = box_cox['frequency'].inverse_transform(preds[preds > 0.0].reshape(-1, 1)).flatten()\n",
    "\n",
    "print(gini(y['claim_count'], cv_preds['bcrf_poisson_freq'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['bcrf_poisson_freq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04090302538794337\n",
      "0.17428131305226074\n",
      "951436.3114163682\n",
      "988020.4396753135\n"
     ]
    }
   ],
   "source": [
    "# Gamma\n",
    "gamma = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                      PolynomialFeatures(interaction_only=True, include_bias = False),\n",
    "                      StandardScaler(),\n",
    "                      MaskedRegressor(model_type='gamma'))\n",
    "\n",
    "cv_preds['gamma'] = cross_val_predict(gamma,\n",
    "                                      X[lin_cols],\n",
    "                                      y['severity'],\n",
    "                                      groups=y['fold'],\n",
    "                                      cv=GroupKFold(),\n",
    "                                      fit_params={'maskedregressor__sample_weight': y['exposure']},\n",
    "                                      n_jobs=-1)\n",
    "# Getting predictions\n",
    "gamma.fit(X[lin_cols], y['severity'], maskedregressor__sample_weight=y['exposure'])\n",
    "preds = gamma.predict(X[lin_cols])\n",
    "\n",
    "# Showing metrics\n",
    "print(gini(y['claim_cost'], cv_preds['gamma'] * cv_preds['bcridgefreq']))\n",
    "print(gini(y['claim_cost'], cv_preds['gamma'] * cv_preds['bcridgefreq'] * y['exposure']))\n",
    "print(np.mean((y['claim_cost'] - preds * y['claim_count']) ** 2))\n",
    "print(np.mean((y['claim_cost'] - cv_preds['gamma'] * y['claim_count']) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0658542884415842\n",
      "0.17831083977340337\n",
      "1093093.7574347975\n",
      "1099825.4046863758\n"
     ]
    }
   ],
   "source": [
    "# Box-Cox Gamma\n",
    "bcgamma = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                        PolynomialFeatures(interaction_only=True, include_bias = False),\n",
    "                        StandardScaler(),\n",
    "                        MaskedRegressor(model_type='gamma'))\n",
    "\n",
    "cv_preds['bcgamma'] = cross_val_predict(bcgamma,\n",
    "                                        X[lin_cols],\n",
    "                                        y['bc_severity'],\n",
    "                                        groups=y['fold'],\n",
    "                                        cv=GroupKFold(),\n",
    "                                        fit_params={'maskedregressor__sample_weight': y['exposure']},\n",
    "                                        n_jobs=-1)\n",
    "# Inverse transform\n",
    "bcgamma.fit(X[lin_cols], y['bc_severity'], maskedregressor__sample_weight=y['exposure'])\n",
    "preds = box_cox['severity'].inverse_transform(bcgamma.predict(X[lin_cols]).reshape(-1, 1)).flatten()\n",
    "cv_preds['bcgamma'] = box_cox['severity'].inverse_transform(cv_preds['bcgamma'].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Showing metrics\n",
    "print(gini(y['claim_cost'], cv_preds['bcgamma'] * cv_preds['bcridgefreq']))\n",
    "print(gini(y['claim_cost'], cv_preds['bcgamma'] * cv_preds['bcridgefreq'] * y['exposure']))\n",
    "print(np.mean((y['claim_cost'] - preds * y['claim_count']) ** 2))\n",
    "print(np.mean((y['claim_cost'] - cv_preds['bcgamma'] * y['claim_count']) ** 2))\n",
    "# Looks like BC gamma is much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040522523821942893\n",
      "0.17368387999519333\n",
      "1070961.3343346626\n",
      "1096766.6800570958\n"
     ]
    }
   ],
   "source": [
    "# Box-Cox RidgeCV\n",
    "bcridge_sev = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                            PolynomialFeatures(interaction_only=True, include_bias = False),\n",
    "                            StandardScaler(),\n",
    "                            MaskedRegressor(model_type='ridge', alphas=np.linspace(1e-6, 10, 100)))\n",
    "cv_preds['bcridge_sev'] = cross_val_predict(bcridge_sev,\n",
    "                                            X[lin_cols],\n",
    "                                            y['bc_severity'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'maskedregressor__sample_weight': y['exposure']},\n",
    "                                            n_jobs=-1)\n",
    "# Inverse transform\n",
    "cv_preds['bcridge_sev'] = box_cox['severity'].inverse_transform(cv_preds['bcridge_sev'].reshape(-1, 1)).flatten()\n",
    "bcridge_sev.fit(X[lin_cols], y['bc_severity'], maskedregressor__sample_weight=y['exposure'])\n",
    "preds = box_cox['severity'].inverse_transform(bcridge_sev.predict(X[lin_cols]).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Clipping\n",
    "cv_preds['bcridge_sev'] = np.clip(cv_preds['bcridge_sev'], 1e-5, None)\n",
    "preds = np.clip(preds, 1e-5, None)\n",
    "\n",
    "# Showing metrics\n",
    "print(gini(y['claim_cost'], cv_preds['bcridge_sev'] * cv_preds['bcridgefreq']))\n",
    "print(gini(y['claim_cost'], cv_preds['bcridge_sev'] * cv_preds['bcridgefreq'] * y['exposure']))\n",
    "print(np.mean((y['claim_cost'] - preds * y['claim_count']) ** 2))\n",
    "print(np.mean((y['claim_cost'] - cv_preds['bcridge_sev'] * y['claim_count']) ** 2))\n",
    "# Looks like RidgeCV does the best for both frequency and severity after correcting for skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08599170710246025\n",
      "0.1741650059939889\n",
      "937766.4477245092\n",
      "992953.317297645\n"
     ]
    }
   ],
   "source": [
    "# RF (MAE)\n",
    "rf_mae_sev = MaskedRegressor(model_type='lgbm',\n",
    "                             n_estimators=1000,\n",
    "                             learning_rate=1.0,\n",
    "                             num_leaves=32,\n",
    "                             colsample_bytree=3 / 7,\n",
    "                             subsample=0.67, \n",
    "                             subsample_freq=1,\n",
    "                             boosting_type='rf',\n",
    "                             n_jobs=-1)\n",
    "cv_preds['rf_mae_sev'] = cross_val_predict(rf_mae_sev,\n",
    "                                            X[boost_cols],\n",
    "                                            y['severity'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'sample_weight': y['exposure']},\n",
    "                                            n_jobs=1)\n",
    "# Fitting model\n",
    "rf_mae_sev.fit(X[boost_cols], y['severity'], sample_weight=y['exposure'])\n",
    "preds = rf_mae_sev.predict(X[boost_cols])\n",
    "\n",
    "# Clipping\n",
    "cv_preds['rf_mae_sev'] = np.clip(cv_preds['rf_mae_sev'], 1e-5, None)\n",
    "preds = np.clip(preds, 1e-5, None)\n",
    "\n",
    "print(gini(y['claim_cost'], cv_preds['rf_mae_sev'] * cv_preds['bcridgefreq']))\n",
    "print(gini(y['claim_cost'], cv_preds['rf_mae_sev'] * cv_preds['bcridgefreq'] * y['exposure']))\n",
    "print(np.mean((y['claim_cost'] - preds * y['claim_count']) ** 2))\n",
    "print(np.mean((y['claim_cost'] - cv_preds['rf_mae_sev'] * y['claim_count']) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08823464289136064\n",
      "0.1794626576509829\n",
      "1056856.8272307266\n",
      "1096937.649146018\n"
     ]
    }
   ],
   "source": [
    "# RF (Gamma)\n",
    "rf_gamma_sev = MaskedRegressor(model_type='lgbm',\n",
    "                             n_estimators=1000,\n",
    "                                learning_rate=1.0,\n",
    "                                num_leaves=32,\n",
    "                                colsample_bytree=3 / 7,\n",
    "                                subsample=0.67, \n",
    "                             subsample_freq=1,\n",
    "                             boosting_type='rf',\n",
    "                             objective='gamma',\n",
    "                             n_jobs=-1)\n",
    "cv_preds['rf_gamma_sev'] = cross_val_predict(rf_gamma_sev,\n",
    "                                            X[boost_cols],\n",
    "                                            y['severity'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'sample_weight': y['exposure']},\n",
    "                                            n_jobs=1)\n",
    "# Fitting model\n",
    "rf_gamma_sev.fit(X[boost_cols], y['severity'], sample_weight=y['exposure'])\n",
    "preds = rf_gamma_sev.predict(X[boost_cols])\n",
    "\n",
    "# Clipping\n",
    "cv_preds['rf_gamma_sev'] = np.clip(cv_preds['rf_gamma_sev'], 1e-5, None)\n",
    "preds = np.clip(preds, 1e-5, None)\n",
    "\n",
    "print(gini(y['claim_cost'], cv_preds['rf_gamma_sev'] * cv_preds['bcridgefreq']))\n",
    "print(gini(y['claim_cost'], cv_preds['rf_gamma_sev'] * cv_preds['bcridgefreq'] * y['exposure']))\n",
    "print(np.mean((y['claim_cost'] - preds * y['claim_count']) ** 2))\n",
    "print(np.mean((y['claim_cost'] - cv_preds['rf_gamma_sev'] * y['claim_count']) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09403002838585293\n",
      "0.17863741829595653\n",
      "1089339.0673948512\n",
      "1099415.3823618968\n"
     ]
    }
   ],
   "source": [
    "# BC RF (MAE)\n",
    "bcrf_mae_sev = MaskedRegressor(model_type='lgbm',\n",
    "                               n_estimators=1000,\n",
    "                                learning_rate=1.0,\n",
    "                                num_leaves=32,\n",
    "                                colsample_bytree=3 / 7,\n",
    "                                subsample=0.67, \n",
    "                                subsample_freq=1,\n",
    "                                boosting_type='rf',\n",
    "                                n_jobs=-1)\n",
    "cv_preds['bcrf_mae_sev'] = cross_val_predict(bcrf_mae_sev,\n",
    "                                            X[boost_cols],\n",
    "                                            y['bc_severity'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'sample_weight': y['exposure']},\n",
    "                                            n_jobs=1)\n",
    "# Inverse box-cox transform\n",
    "cv_preds['bcrf_mae_sev'][cv_preds['bcrf_mae_sev'] > 0.0] = box_cox['severity'].inverse_transform(cv_preds['bcrf_mae_sev'][cv_preds['bcrf_mae_sev'] > 0.0].reshape(-1, 1)).flatten()\n",
    "bcrf_mae_sev.fit(X[boost_cols], y['bc_severity'], sample_weight=y['exposure'])\n",
    "preds = bcrf_mae_sev.predict(X[boost_cols])\n",
    "preds[preds > 0.0] = box_cox['severity'].inverse_transform(preds[preds > 0.0].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Clipping\n",
    "cv_preds['bcrf_mae_sev'] = np.clip(cv_preds['bcrf_mae_sev'], 1e-5, None)\n",
    "preds = np.clip(preds, 1e-5, None)\n",
    "\n",
    "print(gini(y['claim_cost'], cv_preds['bcrf_mae_sev'] * cv_preds['bcridgefreq']))\n",
    "print(gini(y['claim_cost'], cv_preds['bcrf_mae_sev'] * cv_preds['bcridgefreq'] * y['exposure']))\n",
    "print(np.mean((y['claim_cost'] - preds * y['claim_count']) ** 2))\n",
    "print(np.mean((y['claim_cost'] - cv_preds['bcrf_mae_sev'] * y['claim_count']) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04801440592859341\n",
      "0.17604117575322742\n",
      "1278865.4844212707\n",
      "1278871.918838265\n"
     ]
    }
   ],
   "source": [
    "# BC RF (Gamma)\n",
    "bcrf_gamma_sev = MaskedRegressor(model_type='lgbm',\n",
    "                                n_estimators=1000,\n",
    "                                learning_rate=1.0,\n",
    "                                num_leaves=32,\n",
    "                                colsample_bytree=3 / 7,\n",
    "                                subsample=0.67, \n",
    "                                subsample_freq=1,\n",
    "                                boosting_type='rf',\n",
    "                                objective='gamma',\n",
    "                                n_jobs=-1)\n",
    "cv_preds['bcrf_gamma_sev'] = cross_val_predict(bcrf_gamma_sev,\n",
    "                                            X[boost_cols],\n",
    "                                            y['bc_severity'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'sample_weight': y['exposure']},\n",
    "                                            n_jobs=1)\n",
    "# Inverse box-cox transform\n",
    "cv_preds['bcrf_gamma_sev'][cv_preds['bcrf_gamma_sev'] > 0.0] = box_cox['severity'].inverse_transform(cv_preds['bcrf_gamma_sev'][cv_preds['bcrf_gamma_sev'] > 0.0].reshape(-1, 1)).flatten()\n",
    "bcrf_gamma_sev.fit(X[boost_cols], y['bc_severity'], sample_weight=y['exposure'])\n",
    "preds = bcrf_gamma_sev.predict(X[boost_cols])\n",
    "preds[preds > 0.0] = box_cox['severity'].inverse_transform(preds[preds > 0.0].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Clipping\n",
    "cv_preds['bcrf_gamma_sev'] = np.clip(cv_preds['bcrf_gamma_sev'], 1e-5, None)\n",
    "preds = np.clip(preds, 1e-5, None)\n",
    "\n",
    "print(gini(y['claim_cost'], cv_preds['bcrf_gamma_sev'] * cv_preds['bcridgefreq']))\n",
    "print(gini(y['claim_cost'], cv_preds['bcrf_gamma_sev'] * cv_preds['bcridgefreq'] * y['exposure']))\n",
    "print(np.mean((y['claim_cost'] - preds * y['claim_count']) ** 2))\n",
    "print(np.mean((y['claim_cost'] - cv_preds['bcrf_gamma_sev'] * y['claim_count']) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2981066856434901\n",
      "0.373347453912453\n",
      "0.37969333502645164\n"
     ]
    }
   ],
   "source": [
    "# RF (Poisson) with exposure as predictor\n",
    "rfe_poisson_freq = LGBMRegressor(n_estimators=500,\n",
    "                                learning_rate=1.0,\n",
    "                                num_leaves=16,\n",
    "                                colsample_bytree=5 / 7,\n",
    "                                subsample=0.67, \n",
    "                                subsample_freq=1,\n",
    "                                boosting_type='rf',\n",
    "                                objective='poisson',\n",
    "                                n_jobs=-1)\n",
    "cv_preds['rfe_poisson_freq'] = cross_val_predict(rfe_poisson_freq,\n",
    "                                                X[boost_cols + ['exposure']],\n",
    "                                                y['claim_count'],\n",
    "                                                groups=y['fold'],\n",
    "                                                cv=GroupKFold(),\n",
    "                                                n_jobs=1)\n",
    "\n",
    "# Fitting model\n",
    "rfe_poisson_freq.fit(X[boost_cols + ['exposure']], y['claim_count'])\n",
    "preds = rfe_poisson_freq.predict(X[boost_cols + ['exposure']])\n",
    "\n",
    "print(gini(y['claim_count'], cv_preds['rfe_poisson_freq']))\n",
    "print(mean_poisson_deviance(y['claim_count'], preds))\n",
    "print(mean_poisson_deviance(y['claim_count'], cv_preds['rfe_poisson_freq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027895618175636645\n",
      "916035.4395668198\n",
      "995554.1307957585\n"
     ]
    }
   ],
   "source": [
    "# RF (Gamma) with exposure as predictor\n",
    "rfe_gamma_sev = MaskedRegressor(model_type='lgbm',\n",
    "                                n_estimators=1000,\n",
    "                                learning_rate=1.0,\n",
    "                                num_leaves=32,\n",
    "                                colsample_bytree=3 / 7,\n",
    "                                subsample=0.67, \n",
    "                                subsample_freq=1,\n",
    "                                boosting_type='rf',\n",
    "                                objective='gamma',\n",
    "                                n_jobs=-1)\n",
    "cv_preds['rfe_gamma_sev'] = cross_val_predict(rfe_gamma_sev,\n",
    "                                              X[boost_cols + ['exposure']],\n",
    "                                              y['severity'],\n",
    "                                              groups=y['fold'],\n",
    "                                              cv=GroupKFold(),\n",
    "                                              n_jobs=1)\n",
    "# Fitting model\n",
    "rfe_gamma_sev.fit(X[boost_cols + ['exposure']], y['severity'])\n",
    "preds = rfe_gamma_sev.predict(X[boost_cols + ['exposure']])\n",
    "\n",
    "# Clipping\n",
    "cv_preds['rfe_gamma_sev'] = np.clip(cv_preds['rfe_gamma_sev'], 1e-5, None)\n",
    "preds = np.clip(preds, 1e-5, None)\n",
    "\n",
    "print(gini(y['claim_cost'], cv_preds['rfe_gamma_sev'] * cv_preds['rfe_poisson_freq']))\n",
    "print(np.mean((y['claim_cost'] - preds * y['claim_count']) ** 2))\n",
    "print(np.mean((y['claim_cost'] - cv_preds['rfe_gamma_sev'] * y['claim_count']) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ind-Frequency-Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31783947774178045\n",
      "3.5238083762813064\n",
      "3.5347769984398574\n"
     ]
    }
   ],
   "source": [
    "# Masked Poisson Frequency\n",
    "mask_poisson = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                             PolynomialFeatures(interaction_only=True, include_bias = False),\n",
    "                             StandardScaler(),\n",
    "                             MaskedRegressor(model_type='poisson'))\n",
    "\n",
    "cv_preds['mask_poisson'] = cross_val_predict(mask_poisson,\n",
    "                                             X[lin_cols],\n",
    "                                             y['frequency'],\n",
    "                                             groups=y['fold'],\n",
    "                                             cv=GroupKFold(),\n",
    "                                             fit_params={'maskedregressor__sample_weight': y['exposure']},\n",
    "                                             n_jobs=-1)\n",
    "# Getting predictions\n",
    "mask_poisson.fit(X[lin_cols], y['frequency'], maskedregressor__sample_weight=y['exposure'])\n",
    "preds = mask_poisson.predict(X[lin_cols])\n",
    "\n",
    "# Showing metrics\n",
    "print(gini(y['claim_count'], cv_preds['mask_poisson'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], mask_poisson.predict(X[lin_cols])))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['mask_poisson']))\n",
    "# Wow that's actually pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3208188986216006\n",
      "3.003108258397578\n",
      "3.0053882576161937\n"
     ]
    }
   ],
   "source": [
    "# Masked BC Poisson Frequency\n",
    "mask_bcpoisson = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                               PolynomialFeatures(interaction_only=True, include_bias = False),\n",
    "                               StandardScaler(),\n",
    "                               MaskedRegressor(model_type='poisson'))\n",
    "\n",
    "cv_preds['mask_bcpoisson'] = cross_val_predict(mask_bcpoisson,\n",
    "                                               X[lin_cols],\n",
    "                                               y['bc_frequency'],\n",
    "                                               groups=y['fold'],\n",
    "                                               cv=GroupKFold(),\n",
    "                                               fit_params={'maskedregressor__sample_weight': y['exposure']},\n",
    "                                               n_jobs=-1)\n",
    "\n",
    "# Inverse transform\n",
    "mask_bcpoisson.fit(X[lin_cols], y['bc_frequency'], maskedregressor__sample_weight=y['exposure'])\n",
    "preds = box_cox['frequency'].inverse_transform(mask_bcpoisson.predict(X[lin_cols]).reshape(-1, 1)).flatten()\n",
    "cv_preds['mask_bcpoisson'] = box_cox['frequency'].inverse_transform(cv_preds['mask_bcpoisson'].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Showing metrics\n",
    "print(gini(y['claim_count'], cv_preds['mask_bcpoisson'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['mask_bcpoisson']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31923789018427223\n",
      "3.0396363014677092\n",
      "3.117283411882227\n"
     ]
    }
   ],
   "source": [
    "# Masked BC RidgeCV Frequency\n",
    "mask_ridge_freq = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                                PolynomialFeatures(interaction_only=True, include_bias = False),\n",
    "                                StandardScaler(),\n",
    "                                MaskedRegressor(model_type='ridge', alphas=np.linspace(1e-6, 10, 100)))\n",
    "cv_preds['mask_ridge_freq'] = cross_val_predict(mask_ridge_freq,\n",
    "                                                X[lin_cols],\n",
    "                                                y['bc_frequency'],\n",
    "                                                groups=y['fold'],\n",
    "                                                cv=GroupKFold(),\n",
    "                                                fit_params={'maskedregressor__sample_weight': y['exposure']},\n",
    "                                                n_jobs=-1)\n",
    "\n",
    "# Fitting and clipped train preds\n",
    "mask_ridge_freq.fit(X[lin_cols], y['bc_frequency'], maskedregressor__sample_weight=y['exposure'])\n",
    "preds = box_cox['frequency'].inverse_transform(mask_ridge_freq.predict(X[lin_cols]).reshape(-1, 1)).flatten()\n",
    "preds = np.clip(preds, 1e-5, None)\n",
    "\n",
    "# BC Inverse transform\n",
    "cv_preds['mask_ridge_freq'] = np.clip(cv_preds['mask_ridge_freq'], y['bc_frequency'].min(), y['bc_frequency'].max())\n",
    "cv_preds['mask_ridge_freq'] = box_cox['frequency'].inverse_transform(cv_preds['mask_ridge_freq'].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Showing metrics\n",
    "print(gini(y['claim_count'], cv_preds['mask_ridge_freq'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['mask_ridge_freq']))\n",
    "# Ridge and Poisson are pretty competitive here, Poisson with tuned alpha would probably work best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3184482858822757\n",
      "3.530521612601173\n",
      "3.53590152788737\n"
     ]
    }
   ],
   "source": [
    "# Masked RF Poisson\n",
    "# RF (Poisson)\n",
    "mask_rf_poisson_freq = MaskedRegressor(model_type='lgbm',\n",
    "                                  n_estimators=500,\n",
    "                                 learning_rate=1.0,\n",
    "                                 num_leaves=16,\n",
    "                                 colsample_bytree=5 / 7,\n",
    "                                 subsample=0.67, \n",
    "                                 subsample_freq=1,\n",
    "                                 boosting_type='rf',\n",
    "                                 objective='poisson',\n",
    "                                 n_jobs=-1)\n",
    "cv_preds['mask_rf_poisson_freq'] = cross_val_predict(mask_rf_poisson_freq,\n",
    "                                                X[boost_cols],\n",
    "                                                y['frequency'],\n",
    "                                                groups=y['fold'],\n",
    "                                                cv=GroupKFold(),\n",
    "                                                fit_params={'sample_weight': y['exposure']},\n",
    "                                                n_jobs=1)\n",
    "\n",
    "# Fitting model\n",
    "mask_rf_poisson_freq.fit(X[boost_cols], y['frequency'], sample_weight=y['exposure'])\n",
    "preds = mask_rf_poisson_freq.predict(X[boost_cols])\n",
    "\n",
    "print(gini(y['claim_count'], cv_preds['mask_rf_poisson_freq'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['mask_rf_poisson_freq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3202567700295469\n",
      "3.008660628926847\n",
      "3.0105849477563305\n"
     ]
    }
   ],
   "source": [
    "# Masked RF BC Poisson\n",
    "mask_bcrf_poisson_freq = MaskedRegressor(model_type='lgbm',\n",
    "                                         n_estimators=500,\n",
    "                                        learning_rate=1.0,\n",
    "                                        num_leaves=16,\n",
    "                                        colsample_bytree=5 / 7,\n",
    "                                        subsample=0.67, \n",
    "                                        subsample_freq=1,\n",
    "                                        boosting_type='rf',\n",
    "                                        objective='poisson',\n",
    "                                        n_jobs=-1)\n",
    "cv_preds['mask_bcrf_poisson_freq'] = cross_val_predict(mask_bcrf_poisson_freq,\n",
    "                                            X[boost_cols],\n",
    "                                            y['bc_frequency'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'sample_weight': y['exposure']},\n",
    "                                            n_jobs=1)\n",
    "# Inverse box-cox transform\n",
    "cv_preds['mask_bcrf_poisson_freq'][cv_preds['mask_bcrf_poisson_freq'] > 0.0] = box_cox['frequency'].inverse_transform(cv_preds['mask_bcrf_poisson_freq'][cv_preds['mask_bcrf_poisson_freq'] > 0.0].reshape(-1, 1)).flatten()\n",
    "mask_bcrf_poisson_freq.fit(X[boost_cols], y['bc_frequency'], sample_weight=y['exposure'])\n",
    "preds = mask_bcrf_poisson_freq.predict(X[boost_cols])\n",
    "preds[preds > 0.0] = box_cox['frequency'].inverse_transform(preds[preds > 0.0].reshape(-1, 1)).flatten()\n",
    "\n",
    "print(gini(y['claim_count'], cv_preds['mask_bcrf_poisson_freq'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['mask_bcrf_poisson_freq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3200338809916524\n",
      "3.015544473387602\n",
      "3.019544697222136\n"
     ]
    }
   ],
   "source": [
    "# Masked RF BC MSE\n",
    "mask_bcrf_mse_freq = MaskedRegressor(model_type='lgbm',\n",
    "                                     n_estimators=500,\n",
    "                                        learning_rate=1.0,\n",
    "                                        num_leaves=16,\n",
    "                                        colsample_bytree=5 / 7,\n",
    "                                        subsample=0.67, \n",
    "                                        subsample_freq=1,\n",
    "                                        boosting_type='rf',\n",
    "                                        objective='mse',\n",
    "                                        n_jobs=-1)\n",
    "cv_preds['mask_bcrf_mse_freq'] = cross_val_predict(mask_bcrf_mse_freq,\n",
    "                                            X[boost_cols],\n",
    "                                            y['bc_frequency'],\n",
    "                                            groups=y['fold'],\n",
    "                                            cv=GroupKFold(),\n",
    "                                            fit_params={'sample_weight': y['exposure']},\n",
    "                                            n_jobs=1)\n",
    "# Inverse box-cox transform\n",
    "cv_preds['mask_bcrf_mse_freq'][cv_preds['mask_bcrf_mse_freq'] > 0.0] = box_cox['frequency'].inverse_transform(cv_preds['mask_bcrf_mse_freq'][cv_preds['mask_bcrf_mse_freq'] > 0.0].reshape(-1, 1)).flatten()\n",
    "mask_bcrf_mse_freq.fit(X[boost_cols], y['bc_frequency'], sample_weight=y['exposure'])\n",
    "preds = mask_bcrf_mse_freq.predict(X[boost_cols])\n",
    "preds[preds > 0.0] = box_cox['frequency'].inverse_transform(preds[preds > 0.0].reshape(-1, 1)).flatten()\n",
    "\n",
    "print(gini(y['claim_count'], cv_preds['mask_bcrf_mse_freq'] * y['exposure']))\n",
    "print(mean_poisson_deviance(y['frequency'], preds))\n",
    "print(mean_poisson_deviance(y['frequency'], cv_preds['mask_bcrf_mse_freq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegressionCV\n",
    "# Getting class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.array([0, 1]), y=y['claim_ind'])\n",
    "ind_weights = np.where(y['claim_ind'] == 1,\n",
    "                       class_weights[1] * y['claim_count'] * y['exposure'],\n",
    "                       class_weights[0] * y['exposure']\n",
    "                      )\n",
    "\n",
    "logit = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                      StandardScaler(),\n",
    "                      LogisticRegressionCV(Cs=10,\n",
    "                                            l1_ratios=[0.3],\n",
    "                                            penalty='elasticnet',\n",
    "                                            solver='saga',\n",
    "                                            n_jobs=-1,\n",
    "                                            verbose=1,\n",
    "                                            max_iter=10000))\n",
    "cv_preds['logit'] = cross_val_predict(logit,\n",
    "                                      X[lin_cols],\n",
    "                                      y['claim_ind'],\n",
    "                                      groups=y['fold'],\n",
    "                                      cv=GroupKFold(),\n",
    "                                      fit_params={'logisticregressioncv__sample_weight': np.array(ind_weights)},\n",
    "                                      verbose=1,\n",
    "                                      method='predict_proba')[:, 1]\n",
    "# Getting predictions\n",
    "logit.fit(X[lin_cols], y['claim_ind'], logisticregressioncv__sample_weight=np.array(ind_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegressionCV with exposure as predictor\n",
    "# Getting class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.array([0, 1]), y=y['claim_ind'])\n",
    "ind_weights = np.where(y['claim_ind'] == 1,\n",
    "                       class_weights[1] * y['claim_count'],\n",
    "                       class_weights[0],\n",
    "                      )\n",
    "\n",
    "logite = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                       StandardScaler(),\n",
    "                       LogisticRegressionCV(Cs=10,\n",
    "                                            l1_ratios=[0.3],\n",
    "                                            penalty='elasticnet',\n",
    "                                            solver='saga',\n",
    "                                            n_jobs=-1,\n",
    "                                            verbose=1,\n",
    "                                            max_iter=10000))\n",
    "cv_preds['logite'] = cross_val_predict(logite,\n",
    "                                      X[lin_cols + ['exposure']],\n",
    "                                      y['claim_ind'],\n",
    "                                      groups=y['fold'],\n",
    "                                      cv=GroupKFold(),\n",
    "                                      fit_params={'logisticregressioncv__sample_weight': np.array(ind_weights)},\n",
    "                                      verbose=1,\n",
    "                                      method='predict_proba')[:, 1]\n",
    "# Getting predictions\n",
    "logite.fit(X[lin_cols + ['exposure']], y['claim_ind'], logisticregressioncv__sample_weight=np.array(ind_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_preds['logit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_preds['logite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='rf', colsample_bytree=0.42857142857142855,\n",
       "               learning_rate=1.0, n_estimators=1000, num_leaves=32,\n",
       "               objective='binary', subsample=0.67, subsample_freq=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF Ind\n",
    "# Getting class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.array([0, 1]), y=y['claim_ind'])\n",
    "ind_weights = np.where(y['claim_ind'] == 1,\n",
    "                       class_weights[1] * y['claim_count'] * y['exposure'],\n",
    "                       class_weights[0] * y['exposure']\n",
    "                      )\n",
    "rf_ind = LGBMClassifier(n_estimators=1000,\n",
    "                        learning_rate=1.0,\n",
    "                        num_leaves=32,\n",
    "                        colsample_bytree=3 / 7,\n",
    "                        subsample=0.67, \n",
    "                        subsample_freq=1,\n",
    "                        boosting_type='rf',\n",
    "                        objective='binary',\n",
    "                        n_jobs=-1)\n",
    "cv_preds['rf_ind'] = cross_val_predict(rf_ind,\n",
    "                                       X[boost_cols],\n",
    "                                       y['claim_ind'],\n",
    "                                       groups=y['fold'],\n",
    "                                       cv=GroupKFold(),\n",
    "                                       fit_params={'sample_weight': np.array(ind_weights)},\n",
    "                                       n_jobs=1,\n",
    "                                       method='predict_proba')[:, 1]\n",
    "# Fitting model\n",
    "rf_ind.fit(X[boost_cols], y['claim_ind'], sample_weight=np.array(ind_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='rf', colsample_bytree=0.42857142857142855,\n",
       "               learning_rate=1.0, n_estimators=1000, num_leaves=32,\n",
       "               objective='binary', subsample=0.67, subsample_freq=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF Ind with exposure as a predictor\n",
    "# Getting class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.array([0, 1]), y=y['claim_ind'])\n",
    "ind_weights = np.where(y['claim_ind'] == 1,\n",
    "                       class_weights[1] * y['claim_count'],\n",
    "                       class_weights[0]\n",
    "                      )\n",
    "rfe_ind = LGBMClassifier(n_estimators=10000,\n",
    "                        learning_rate=1.0,\n",
    "                        num_leaves=5,\n",
    "                        #colsample_bytree=5 / 7,\n",
    "                        subsample=0.67, \n",
    "                        subsample_freq=1,\n",
    "                        boosting_type='rf',\n",
    "                        objective='binary',\n",
    "                        extra_trees=True,\n",
    "                        n_jobs=-1)\n",
    "cv_preds['rfe_ind'] = cross_val_predict(rfe_ind,\n",
    "                                       X[boost_cols + ['exposure']],\n",
    "                                       y['claim_ind'],\n",
    "                                       groups=y['fold'],\n",
    "                                       cv=GroupKFold(),\n",
    "                                       fit_params={'sample_weight': np.array(ind_weights)},\n",
    "                                       n_jobs=1,\n",
    "                                       method='predict_proba')[:, 1]\n",
    "# Fitting model\n",
    "rf_ind.fit(X[boost_cols + ['exposure']], y['claim_ind'], sample_weight=np.array(ind_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2613259472504123\n",
      "0.6979458691396245\n"
     ]
    }
   ],
   "source": [
    "print(cv_preds['rf_ind'].min())\n",
    "print(cv_preds['rf_ind'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28049215522125853\n",
      "0.6573565972098736\n"
     ]
    }
   ],
   "source": [
    "print(cv_preds['rfe_ind'].min())\n",
    "print(cv_preds['rfe_ind'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517828412873705\n",
      "0.6540343038653431\n",
      "0.8145997184891994\n",
      "0.6775755465471276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss, classification_report\n",
    "print(roc_auc_score(y['claim_ind'], cv_preds['rf_ind']))\n",
    "print(roc_auc_score(y['claim_ind'], cv_preds['rfe_ind']))\n",
    "\n",
    "print(log_loss(y['claim_ind'], cv_preds['rf_ind']))\n",
    "print(log_loss(y['claim_ind'], cv_preds['rfe_ind']))\n",
    "# With exposure as variable seems better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'probability'}, xlabel='claim_ind'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({'claim_ind': y['claim_ind'], 'probability': cv_preds['rfe_ind']}).boxplot('probability', 'claim_ind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Tweedie Predictions!\n"
     ]
    }
   ],
   "source": [
    "# Now with a stacked tweedie input\n",
    "tweedie = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                        PolynomialFeatures(interaction_only=True, include_bias = False),  \n",
    "                        StandardScaler(),\n",
    "                        TweedieRegressor(alpha=0.0, power=1.8, max_iter=1000))\n",
    "cv_preds['tweedie'] = cross_val_predict(tweedie,\n",
    "                                        X[lin_cols],\n",
    "                                        y['pure_premium'],\n",
    "                                        groups=y['fold'],\n",
    "                                        cv=GroupKFold(),\n",
    "                                        fit_params={'tweedieregressor__sample_weight': y['exposure']},\n",
    "                                        n_jobs=-1)\n",
    "print('Got Tweedie Predictions!')\n",
    "rfs_ind = LGBMClassifier(n_estimators=1000,\n",
    "                         learning_rate=1.0,\n",
    "                         num_leaves=32,\n",
    "                         colsample_bytree=3 / 7,\n",
    "                         subsample=0.67, \n",
    "                         subsample_freq=1,\n",
    "                         boosting_type='rf',\n",
    "                         objective='binary',\n",
    "                         n_jobs=-1)\n",
    "X_stack_ind = X.copy()\n",
    "X_stack_ind['tweedie'] = cv_preds['tweedie']\n",
    "cv_preds['rfs_ind'] = cross_val_predict(rfs_ind,\n",
    "                                        X_stack_ind[boost_cols + ['tweedie']],\n",
    "                                        y['claim_ind'],\n",
    "                                        groups=y['fold'],\n",
    "                                        cv=GroupKFold(),\n",
    "                                        fit_params={'sample_weight': np.array(ind_weights)},\n",
    "                                        n_jobs=1,\n",
    "                                        method='predict_proba')[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency-Severity CV Ginis:\n",
      "            Frequency        Severity      Gini\n",
      "25        rf_mae_freq    rf_gamma_sev  0.188188\n",
      "4             poisson    rf_gamma_sev  0.186078\n",
      "23        rf_mae_freq     bcridge_sev  0.184651\n",
      "26        rf_mae_freq    bcrf_mae_sev  0.184298\n",
      "32    rf_poisson_freq    rf_gamma_sev  0.182806\n",
      "2             poisson     bcridge_sev  0.181894\n",
      "22        rf_mae_freq         bcgamma  0.181491\n",
      "5             poisson    bcrf_mae_sev  0.180993\n",
      "24        rf_mae_freq      rf_mae_sev  0.180655\n",
      "30    rf_poisson_freq     bcridge_sev  0.179584\n",
      "1             poisson         bcgamma  0.178869\n",
      "39      bcrf_mae_freq    rf_gamma_sev  0.178852\n",
      "27        rf_mae_freq  bcrf_gamma_sev  0.178805\n",
      "46  bcrf_poisson_freq    rf_gamma_sev  0.178600\n",
      "11          bcpoisson    rf_gamma_sev  0.178481\n",
      "33    rf_poisson_freq    bcrf_mae_sev  0.178343\n",
      "3             poisson      rf_mae_sev  0.177079\n",
      "37      bcrf_mae_freq     bcridge_sev  0.175785\n",
      "29    rf_poisson_freq         bcgamma  0.175752\n",
      "44  bcrf_poisson_freq     bcridge_sev  0.175526\n",
      "9           bcpoisson     bcridge_sev  0.175377\n",
      "6             poisson  bcrf_gamma_sev  0.174793\n",
      "31    rf_poisson_freq      rf_mae_sev  0.174300\n",
      "40      bcrf_mae_freq    bcrf_mae_sev  0.173587\n",
      "47  bcrf_poisson_freq    bcrf_mae_sev  0.173286\n",
      "34    rf_poisson_freq  bcrf_gamma_sev  0.173150\n",
      "12          bcpoisson    bcrf_mae_sev  0.173043\n",
      "18        bcridgefreq    rf_gamma_sev  0.172245\n",
      "36      bcrf_mae_freq         bcgamma  0.172030\n",
      "43  bcrf_poisson_freq         bcgamma  0.171762\n",
      "8           bcpoisson         bcgamma  0.171594\n",
      "38      bcrf_mae_freq      rf_mae_sev  0.169710\n",
      "45  bcrf_poisson_freq      rf_mae_sev  0.169466\n",
      "10          bcpoisson      rf_mae_sev  0.169279\n",
      "41      bcrf_mae_freq  bcrf_gamma_sev  0.168686\n",
      "48  bcrf_poisson_freq  bcrf_gamma_sev  0.168462\n",
      "13          bcpoisson  bcrf_gamma_sev  0.168280\n",
      "16        bcridgefreq     bcridge_sev  0.166899\n",
      "21        rf_mae_freq           gamma  0.166746\n",
      "19        bcridgefreq    bcrf_mae_sev  0.165412\n",
      "0             poisson           gamma  0.164718\n",
      "20        bcridgefreq  bcrf_gamma_sev  0.162932\n",
      "15        bcridgefreq         bcgamma  0.162864\n",
      "17        bcridgefreq      rf_mae_sev  0.162374\n",
      "28    rf_poisson_freq           gamma  0.161987\n",
      "35      bcrf_mae_freq           gamma  0.158825\n",
      "42  bcrf_poisson_freq           gamma  0.158593\n",
      "7           bcpoisson           gamma  0.158451\n",
      "14        bcridgefreq           gamma  0.150153\n"
     ]
    }
   ],
   "source": [
    "# Comparing different metrics\n",
    "# First freq-sev\n",
    "freqs, sevs, ginis = [], [], []\n",
    "print('Frequency-Severity CV Ginis:')\n",
    "for f in ['poisson', 'bcpoisson', 'bcridgefreq', 'rf_mae_freq', 'rf_poisson_freq', 'bcrf_mae_freq', 'bcrf_poisson_freq']:\n",
    "    for s in ['gamma', 'bcgamma', 'bcridge_sev', 'rf_mae_sev', 'rf_gamma_sev', 'bcrf_mae_sev', 'bcrf_gamma_sev']:\n",
    "        g = gini(y['claim_cost'], cv_preds[s] * cv_preds[f] * y['exposure'])\n",
    "        freqs.append(f)\n",
    "        sevs.append(s)\n",
    "        ginis.append(g)\n",
    "print(pd.DataFrame({'Frequency': freqs, 'Severity': sevs, 'Gini': ginis}).sort_values('Gini', ascending=False))\n",
    "# Looks like random forest is pretty favored here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ind-Frequency-Severity CV Ginis:\n",
      "         Ind               Frequency        Severity      Gini\n",
      "123  rfs_ind      mask_bcrf_mse_freq    rf_gamma_sev  0.182617\n",
      "39     logit      mask_bcrf_mse_freq    rf_gamma_sev  0.182513\n",
      "116  rfs_ind  mask_bcrf_poisson_freq    rf_gamma_sev  0.182112\n",
      "81    rf_ind      mask_bcrf_mse_freq    rf_gamma_sev  0.182078\n",
      "32     logit  mask_bcrf_poisson_freq    rf_gamma_sev  0.181974\n",
      "74    rf_ind  mask_bcrf_poisson_freq    rf_gamma_sev  0.181491\n",
      "95   rfs_ind          mask_bcpoisson    rf_gamma_sev  0.181360\n",
      "109  rfs_ind    mask_rf_poisson_freq    rf_gamma_sev  0.181091\n",
      "11     logit          mask_bcpoisson    rf_gamma_sev  0.181034\n",
      "53    rf_ind          mask_bcpoisson    rf_gamma_sev  0.180673\n",
      "25     logit    mask_rf_poisson_freq    rf_gamma_sev  0.180665\n",
      "67    rf_ind    mask_rf_poisson_freq    rf_gamma_sev  0.180399\n",
      "102  rfs_ind         mask_ridge_freq    rf_gamma_sev  0.180287\n",
      "18     logit         mask_ridge_freq    rf_gamma_sev  0.180132\n",
      "60    rf_ind         mask_ridge_freq    rf_gamma_sev  0.179512\n",
      "114  rfs_ind  mask_bcrf_poisson_freq     bcridge_sev  0.179102\n",
      "93   rfs_ind          mask_bcpoisson     bcridge_sev  0.179063\n",
      "121  rfs_ind      mask_bcrf_mse_freq     bcridge_sev  0.178683\n",
      "100  rfs_ind         mask_ridge_freq     bcridge_sev  0.178156\n",
      "9      logit          mask_bcpoisson     bcridge_sev  0.178046\n",
      "124  rfs_ind      mask_bcrf_mse_freq    bcrf_mae_sev  0.178012\n",
      "30     logit  mask_bcrf_poisson_freq     bcridge_sev  0.177963\n",
      "117  rfs_ind  mask_bcrf_poisson_freq    bcrf_mae_sev  0.177947\n",
      "72    rf_ind  mask_bcrf_poisson_freq     bcridge_sev  0.177927\n",
      "51    rf_ind          mask_bcpoisson     bcridge_sev  0.177813\n",
      "96   rfs_ind          mask_bcpoisson    bcrf_mae_sev  0.177693\n",
      "37     logit      mask_bcrf_mse_freq     bcridge_sev  0.177565\n",
      "79    rf_ind      mask_bcrf_mse_freq     bcridge_sev  0.177540\n",
      "16     logit         mask_ridge_freq     bcridge_sev  0.177441\n",
      "107  rfs_ind    mask_rf_poisson_freq     bcridge_sev  0.177429\n",
      "88   rfs_ind            mask_poisson    rf_gamma_sev  0.177224\n",
      "58    rf_ind         mask_ridge_freq     bcridge_sev  0.176961\n",
      "82    rf_ind      mask_bcrf_mse_freq    bcrf_mae_sev  0.176640\n",
      "4      logit            mask_poisson    rf_gamma_sev  0.176611\n",
      "46    rf_ind            mask_poisson    rf_gamma_sev  0.176608\n",
      "75    rf_ind  mask_bcrf_poisson_freq    bcrf_mae_sev  0.176575\n",
      "40     logit      mask_bcrf_mse_freq    bcrf_mae_sev  0.176549\n",
      "33     logit  mask_bcrf_poisson_freq    bcrf_mae_sev  0.176515\n",
      "103  rfs_ind         mask_ridge_freq    bcrf_mae_sev  0.176438\n",
      "54    rf_ind          mask_bcpoisson    bcrf_mae_sev  0.176400\n",
      "65    rf_ind    mask_rf_poisson_freq     bcridge_sev  0.176338\n",
      "110  rfs_ind    mask_rf_poisson_freq    bcrf_mae_sev  0.176174\n",
      "23     logit    mask_rf_poisson_freq     bcridge_sev  0.176061\n",
      "120  rfs_ind      mask_bcrf_mse_freq         bcgamma  0.176059\n",
      "113  rfs_ind  mask_bcrf_poisson_freq         bcgamma  0.175996\n",
      "12     logit          mask_bcpoisson    bcrf_mae_sev  0.175995\n",
      "92   rfs_ind          mask_bcpoisson         bcgamma  0.175627\n",
      "86   rfs_ind            mask_poisson     bcridge_sev  0.175408\n",
      "19     logit         mask_ridge_freq    bcrf_mae_sev  0.175383\n",
      "61    rf_ind         mask_ridge_freq    bcrf_mae_sev  0.175268\n",
      "36     logit      mask_bcrf_mse_freq         bcgamma  0.175060\n",
      "29     logit  mask_bcrf_poisson_freq         bcgamma  0.174905\n",
      "68    rf_ind    mask_rf_poisson_freq    bcrf_mae_sev  0.174814\n",
      "78    rf_ind      mask_bcrf_mse_freq         bcgamma  0.174776\n",
      "71    rf_ind  mask_bcrf_poisson_freq         bcgamma  0.174746\n",
      "99   rfs_ind         mask_ridge_freq         bcgamma  0.174724\n",
      "2      logit            mask_poisson     bcridge_sev  0.174499\n",
      "50    rf_ind          mask_bcpoisson         bcgamma  0.174408\n",
      "26     logit    mask_rf_poisson_freq    bcrf_mae_sev  0.174382\n",
      "15     logit         mask_ridge_freq         bcgamma  0.174356\n",
      "8      logit          mask_bcpoisson         bcgamma  0.174326\n",
      "44    rf_ind            mask_poisson     bcridge_sev  0.174283\n",
      "106  rfs_ind    mask_rf_poisson_freq         bcgamma  0.174069\n",
      "57    rf_ind         mask_ridge_freq         bcgamma  0.173575\n",
      "115  rfs_ind  mask_bcrf_poisson_freq      rf_mae_sev  0.173452\n",
      "122  rfs_ind      mask_bcrf_mse_freq      rf_mae_sev  0.173451\n",
      "89   rfs_ind            mask_poisson    bcrf_mae_sev  0.173115\n",
      "94   rfs_ind          mask_bcpoisson      rf_mae_sev  0.173114\n",
      "64    rf_ind    mask_rf_poisson_freq         bcgamma  0.173037\n",
      "22     logit    mask_rf_poisson_freq         bcgamma  0.172818\n",
      "125  rfs_ind      mask_bcrf_mse_freq  bcrf_gamma_sev  0.172807\n",
      "80    rf_ind      mask_bcrf_mse_freq      rf_mae_sev  0.172714\n",
      "73    rf_ind  mask_bcrf_poisson_freq      rf_mae_sev  0.172661\n",
      "38     logit      mask_bcrf_mse_freq      rf_mae_sev  0.172653\n",
      "41     logit      mask_bcrf_mse_freq  bcrf_gamma_sev  0.172541\n",
      "101  rfs_ind         mask_ridge_freq      rf_mae_sev  0.172534\n",
      "52    rf_ind          mask_bcpoisson      rf_mae_sev  0.172485\n",
      "31     logit  mask_bcrf_poisson_freq      rf_mae_sev  0.172460\n",
      "118  rfs_ind  mask_bcrf_poisson_freq  bcrf_gamma_sev  0.172020\n",
      "10     logit          mask_bcpoisson      rf_mae_sev  0.172018\n",
      "47    rf_ind            mask_poisson    bcrf_mae_sev  0.172018\n",
      "59    rf_ind         mask_ridge_freq      rf_mae_sev  0.171838\n",
      "17     logit         mask_ridge_freq      rf_mae_sev  0.171837\n",
      "34     logit  mask_bcrf_poisson_freq  bcrf_gamma_sev  0.171699\n",
      "111  rfs_ind    mask_rf_poisson_freq  bcrf_gamma_sev  0.171659\n",
      "108  rfs_ind    mask_rf_poisson_freq      rf_mae_sev  0.171622\n",
      "83    rf_ind      mask_bcrf_mse_freq  bcrf_gamma_sev  0.171503\n",
      "27     logit    mask_rf_poisson_freq  bcrf_gamma_sev  0.171347\n",
      "5      logit            mask_poisson    bcrf_mae_sev  0.171250\n",
      "85   rfs_ind            mask_poisson         bcgamma  0.171226\n",
      "104  rfs_ind         mask_ridge_freq  bcrf_gamma_sev  0.171194\n",
      "97   rfs_ind          mask_bcpoisson  bcrf_gamma_sev  0.171169\n",
      "66    rf_ind    mask_rf_poisson_freq      rf_mae_sev  0.170970\n",
      "13     logit          mask_bcpoisson  bcrf_gamma_sev  0.170728\n",
      "76    rf_ind  mask_bcrf_poisson_freq  bcrf_gamma_sev  0.170675\n",
      "20     logit         mask_ridge_freq  bcrf_gamma_sev  0.170618\n",
      "24     logit    mask_rf_poisson_freq      rf_mae_sev  0.170543\n",
      "69    rf_ind    mask_rf_poisson_freq  bcrf_gamma_sev  0.170366\n",
      "43    rf_ind            mask_poisson         bcgamma  0.170146\n",
      "62    rf_ind         mask_ridge_freq  bcrf_gamma_sev  0.170108\n",
      "55    rf_ind          mask_bcpoisson  bcrf_gamma_sev  0.169885\n",
      "1      logit            mask_poisson         bcgamma  0.169742\n",
      "87   rfs_ind            mask_poisson      rf_mae_sev  0.168648\n",
      "45    rf_ind            mask_poisson      rf_mae_sev  0.167933\n",
      "90   rfs_ind            mask_poisson  bcrf_gamma_sev  0.167623\n",
      "3      logit            mask_poisson      rf_mae_sev  0.167204\n",
      "6      logit            mask_poisson  bcrf_gamma_sev  0.167044\n",
      "48    rf_ind            mask_poisson  bcrf_gamma_sev  0.166443\n",
      "119  rfs_ind      mask_bcrf_mse_freq           gamma  0.161940\n",
      "112  rfs_ind  mask_bcrf_poisson_freq           gamma  0.161690\n",
      "35     logit      mask_bcrf_mse_freq           gamma  0.161346\n",
      "91   rfs_ind          mask_bcpoisson           gamma  0.161300\n",
      "28     logit  mask_bcrf_poisson_freq           gamma  0.161188\n",
      "98   rfs_ind         mask_ridge_freq           gamma  0.160660\n",
      "77    rf_ind      mask_bcrf_mse_freq           gamma  0.160590\n",
      "7      logit          mask_bcpoisson           gamma  0.160490\n",
      "70    rf_ind  mask_bcrf_poisson_freq           gamma  0.160355\n",
      "105  rfs_ind    mask_rf_poisson_freq           gamma  0.160261\n",
      "49    rf_ind          mask_bcpoisson           gamma  0.160135\n",
      "14     logit         mask_ridge_freq           gamma  0.160072\n",
      "56    rf_ind         mask_ridge_freq           gamma  0.159649\n",
      "21     logit    mask_rf_poisson_freq           gamma  0.159397\n",
      "63    rf_ind    mask_rf_poisson_freq           gamma  0.159025\n",
      "84   rfs_ind            mask_poisson           gamma  0.156898\n",
      "42    rf_ind            mask_poisson           gamma  0.155936\n",
      "0      logit            mask_poisson           gamma  0.155811\n"
     ]
    }
   ],
   "source": [
    "# Now with indicator model\n",
    "inds, freqs, sevs, ginis = [], [], [], []\n",
    "print('Ind-Frequency-Severity CV Ginis:')\n",
    "for i in ['logit', 'rf_ind', 'rfs_ind']:\n",
    "    for f in ['mask_poisson', 'mask_bcpoisson', 'mask_ridge_freq', 'mask_rf_poisson_freq', 'mask_bcrf_poisson_freq', 'mask_bcrf_mse_freq']:\n",
    "        for s in ['gamma', 'bcgamma', 'bcridge_sev', 'rf_mae_sev', 'rf_gamma_sev', 'bcrf_mae_sev', 'bcrf_gamma_sev']:\n",
    "            g = gini(y['claim_cost'], cv_preds[i] * cv_preds[s] * cv_preds[f] * y['exposure'])\n",
    "            inds.append(i)\n",
    "            freqs.append(f)\n",
    "            sevs.append(s)\n",
    "            ginis.append(g)\n",
    "print(pd.DataFrame({'Ind': inds, 'Frequency': freqs, 'Severity': sevs, 'Gini': ginis}).sort_values('Gini', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping FS submission\n",
    "# Getting our test predictions\n",
    "df_test = pd.read_csv('InsNova_test.csv')\n",
    "X_test = df_test.drop(['exposure', 'id'], axis=1)\n",
    "\n",
    "# Feature extraction\n",
    "other_bodies = ['TRUCK', 'COUPE', 'MIBUS', 'PANVN', 'BUS', 'RDSTR', 'MCARA', 'CONVT']\n",
    "X_test['veh_body2'] = np.where(X_test['veh_body'].isin(other_bodies), 'OTHER', X_test['veh_body'])\n",
    "X_test.loc[:,'log_veh_value'] = np.log(X_test['veh_value'] + 0.1)\n",
    "                                                                                      \n",
    "# Creating Categorical dataset for LightGBM and CatBoost\n",
    "for i in ['veh_body', 'veh_body2', 'gender', 'area']:\n",
    "    X_test[i] = X_test[i].astype('category')\n",
    "\n",
    "# Getting predictions\n",
    "freq = poisson.predict(X_test[lin_cols])\n",
    "#freq[freq > 0.0] = box_cox['frequency'].inverse_transform(freq[freq > 0.0].reshape(-1, 1)).flatten()\n",
    "sev = rf_gamma_sev.predict(X_test[boost_cols])\n",
    "preds = freq * sev * df_test['exposure']\n",
    "preds = np.clip(preds, 0.0, None)\n",
    "\n",
    "df_test['claim_cost'] = df_test['exposure'] * preds\n",
    "df_test['id'] = np.arange(df_test.shape[0])\n",
    "df_test['id'] = df_test['id'].astype(int)\n",
    "df_test['id'] += 1\n",
    "df_test[['id', 'claim_cost']].to_csv('box_cox_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01 0.19316696780978962\n",
      "1.0594736842105263 0.19323574825654477\n",
      "1.1089473684210527 0.1933219280963317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0caaa4666226>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                               \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGroupKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                               \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'tweedieregressor__sample_weight'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exposure'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                                               n_jobs=-1)\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtweedie\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlin_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pure_premium'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweedieregressor__sample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exposure'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgini\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'claim_cost'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_preds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweedie'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exposure'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m    771\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[0;32m    772\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[1;32m--> 773\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[1;31m# Concatenate the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in np.linspace(1.01, 1.95, 20):\n",
    "    tweedie = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                            StandardScaler(),\n",
    "                            TweedieRegressor(alpha=1.0, power=i, max_iter=1000))\n",
    "    cv_preds['tweedie'] = cross_val_predict(tweedie,\n",
    "                                              X[lin_cols],\n",
    "                                              y['pure_premium'],\n",
    "                                              groups=y['fold'],\n",
    "                                              cv=GroupKFold(),\n",
    "                                              fit_params={'tweedieregressor__sample_weight': y['exposure']},\n",
    "                                              n_jobs=-1)\n",
    "    tweedie.fit(X[lin_cols], y['pure_premium'], tweedieregressor__sample_weight=y['exposure'])\n",
    "    print(i, gini(y['claim_cost'], cv_preds['tweedie'] * y['exposure']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01 0.20637168890871402\n",
      "1.0594736842105263 0.20645421928282717\n",
      "1.1089473684210527 0.20652721346750894\n",
      "1.158421052631579 0.20662648682320686\n",
      "1.2078947368421054 0.2066889557098155\n",
      "1.2573684210526315 0.2066953896977399\n",
      "1.3068421052631578 0.20667667374310325\n",
      "1.3563157894736841 0.20654958800708256\n",
      "1.4057894736842105 0.20630887617324545\n",
      "1.4552631578947368 0.20595557436655607\n",
      "1.5047368421052632 0.20554135851948704\n",
      "1.5542105263157895 0.2048068118485625\n",
      "1.6036842105263158 0.20374889667021492\n",
      "1.6531578947368422 0.2023778488847026\n",
      "1.7026315789473685 0.2006184463648263\n",
      "1.7521052631578948 0.19848321097429497\n",
      "1.8015789473684212 0.1959518445428063\n",
      "1.8510526315789475 0.19311172010064614\n",
      "1.9005263157894738 0.18996922146843542\n",
      "1.95 0.18667635694536971\n"
     ]
    }
   ],
   "source": [
    "# Tweedie predictions\n",
    "for i in np.linspace(1.01, 1.95, 20):\n",
    "    lgbm_tweedie = LGBMRegressor(n_estimators=1000,\n",
    "                                 num_leaves=16,\n",
    "                                 learning_rate=1.0,\n",
    "                                 colsample_bytree=0.67,\n",
    "                                 min_gain_split=100.0,\n",
    "                                 subsample=0.67,\n",
    "                                 subsample_freq=1,\n",
    "                                 objective='tweedie',\n",
    "                                 tweedie_variance_power=1.85,\n",
    "                                 boosting_type='rf',\n",
    "                                 n_jobs=-1)\n",
    "    tweedie = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                            PolynomialFeatures(interaction_only=True, include_bias = False),  \n",
    "                            StandardScaler(),\n",
    "                            SelectFromModel(lgbm_tweedie, max_features=12),\n",
    "                            TweedieRegressor(alpha=1.0, power=i, max_iter=1000))\n",
    "    cv_preds['tweedie'] = cross_val_predict(tweedie,\n",
    "                                              X[lin_cols],\n",
    "                                              y['pure_premium'],\n",
    "                                              groups=y['fold'],\n",
    "                                              cv=GroupKFold(),\n",
    "                                              fit_params={'tweedieregressor__sample_weight': y['exposure']},\n",
    "                                              n_jobs=-1)\n",
    "    tweedie.fit(X[lin_cols], y['pure_premium'], tweedieregressor__sample_weight=y['exposure'])\n",
    "    print(i, gini(y['claim_cost'], cv_preds['tweedie'] * y['exposure']))\n",
    "    #print(np.mean((y['claim_cost'] - tweedie.predict(X[lin_cols]) * y['exposure']) ** 2))\n",
    "    #print(np.mean((y['claim_cost'] - cv_preds['tweedie'] * y['exposure']) ** 2))\n",
    "# It seems like lower tweedie powers do better here... (as close to 1 as possible)\n",
    "# When you add interactions optimal power looks like 1.9, also seems to fluctuate with alpha parameter\n",
    "# We might need to do a nested search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01 0.15844001379505068\n",
      "1.0594736842105263 0.16030396226946342\n",
      "1.1089473684210527 0.1606002164702775\n",
      "1.158421052631579 0.16121175179187763\n",
      "1.2078947368421054 0.1614914767668558\n",
      "1.2573684210526315 0.16216482733416082\n",
      "1.3068421052631578 0.16230779451486899\n",
      "1.3563157894736841 0.16271618223551915\n",
      "1.4057894736842105 0.16317220292954318\n",
      "1.4552631578947368 0.16352602821326417\n",
      "1.5047368421052632 0.16378885518030808\n",
      "1.5542105263157895 0.16429942200254807\n",
      "1.6036842105263158 0.1645310418290667\n",
      "1.6531578947368422 0.16509554362793985\n",
      "1.7026315789473685 0.16585564114612217\n",
      "1.7521052631578948 0.1666251607713954\n",
      "1.8015789473684212 0.16646677737989726\n",
      "1.8510526315789475 0.16661389101007154\n",
      "1.9005263157894738 0.1656360819630229\n",
      "1.95 0.1595627037499735\n"
     ]
    }
   ],
   "source": [
    "for i in np.linspace(1.01, 1.95, 20):\n",
    "    lgbm_tweedie = LGBMRegressor(n_estimators=500,\n",
    "                                 num_leaves=16,\n",
    "                                 learning_rate=0.001,\n",
    "                                 subsample=0.67,\n",
    "                                 subsample_freq=1,\n",
    "                                 objective='tweedie',\n",
    "                                 tweedie_variance_power=i,\n",
    "                                 #boosting_type='rf',\n",
    "                                 n_jobs=-1)\n",
    "    preds = cross_val_predict(lgbm_tweedie,\n",
    "                              X[boost_cols],\n",
    "                              y['pure_premium'],\n",
    "                              groups=y['fold'],\n",
    "                              cv=GroupKFold(),\n",
    "                              fit_params={'sample_weight': y['exposure']},\n",
    "                              n_jobs=-1)\n",
    "    print(i, gini(y['claim_cost'], preds * y['exposure']))\n",
    "# 1.85 is best for rf and lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17098828283014655\n",
      "1281220.9287971135\n",
      "1281221.792394848\n"
     ]
    }
   ],
   "source": [
    "# Box-Cox Tweedie predictions\n",
    "bctweedie = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                          StandardScaler(),\n",
    "                          TweedieRegressor(alpha=0.0, power=1.8, max_iter=1000))\n",
    "cv_preds['bctweedie'] = cross_val_predict(bctweedie,\n",
    "                                          X[lin_cols],\n",
    "                                          y['bc_pure_premium'],\n",
    "                                          groups=y['fold'],\n",
    "                                          cv=GroupKFold(),\n",
    "                                          fit_params={'tweedieregressor__sample_weight': y['exposure']},\n",
    "                                          n_jobs=-1)\n",
    "cv_preds['bctweedie'][cv_preds['bctweedie'] > 0.0] = box_cox['pure_premium'].inverse_transform(cv_preds['bctweedie'][cv_preds['bctweedie'] > 0.0].reshape(-1, 1)).flatten()\n",
    "bctweedie.fit(X[lin_cols], y['bc_pure_premium'], tweedieregressor__sample_weight=y['exposure'])\n",
    "preds = bctweedie.predict(X[lin_cols])\n",
    "preds[preds > 0.0] = box_cox['pure_premium'].inverse_transform(preds[preds > 0.0].reshape(-1, 1)).flatten()\n",
    "print(gini(y['claim_cost'], cv_preds['bctweedie'] * y['exposure']))\n",
    "print(np.mean((y['claim_cost'] - preds * y['exposure']) ** 2))\n",
    "print(np.mean((y['claim_cost'] - cv_preds['bctweedie'] * y['exposure']) ** 2))\n",
    "# Hmm, looks like the skew adjustment hurt our pure premium model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic net with stats models\n",
    "import statsmodels.api as sm\n",
    "class SMTweedieRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, power=1.5, alpha=0.0, l1_ratio=0.0, max_iter=1000):\n",
    "        self.power = power\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        model = sm.GLM(np.copy(y), np.copy(X), family=sm.families.Tweedie(var_power=self.power), freq_weights=sample_weight)\n",
    "        self.model = model.fit_regularized(maxiter=self.max_iter, alpha=self.alpha, L1_wt=self.l1_ratio)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.model.predict(np.copy(X))\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'power': self.power, 'alpha': self.alpha, 'l1_ratio': self.l1_ratio, 'max_iter': self.max_iter}\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        if 'power' in list(params.keys()):\n",
    "            self.power = params['power']\n",
    "        if 'alpha' in list(params.keys()):\n",
    "            self.alpha = params['alpha']\n",
    "        if 'l1_ratio' in list(params.keys()):\n",
    "            self.l1_ratio = params['l1_ratio']\n",
    "        if 'max_iter' in list(params.keys()):\n",
    "            self.max_iter = params['max_iter']\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.3s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.3s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.3s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.3s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.2s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.3s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.3s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.2s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 1 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.3s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b12cf54d40fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                             n_jobs=-1)\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtweedie_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlin_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pure_premium'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweedie_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\searchcv.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, callback)\u001b[0m\n\u001b[0;32m    692\u001b[0m                 optim_result = self._step(\n\u001b[0;32m    693\u001b[0m                     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m                     \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_points_adjusted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m                 )\n\u001b[0;32m    696\u001b[0m                 \u001b[0mn_iter\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mn_points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\searchcv.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, X, y, search_space, optimizer, groups, n_points)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;31m# get parameter values to evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;31m# convert parameters to python native types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py\u001b[0m in \u001b[0;36mask\u001b[1;34m(self, n_points, strategy)\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_lie\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_lie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_lie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m}\u001b[0m  \u001b[1;31m# cache_ the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py\u001b[0m in \u001b[0;36m_tell\u001b[1;34m(self, x, y, fit)\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m                 \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"next_xs_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macq_func\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"gp_hedge\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\learning\\gaussian_process\\gpr.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mnoise_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_level_bounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fixed\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             )\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGaussianProcessRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    232\u001b[0m             optima = [(self._constrained_optimization(obj_func,\n\u001b[0;32m    233\u001b[0m                                                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                                                       self.kernel_.bounds))]\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;31m# Additional runs are performed from log-uniform chosen initial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36m_constrained_optimization\u001b[1;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[0;32m    501\u001b[0m             opt_res = scipy.optimize.minimize(\n\u001b[0;32m    502\u001b[0m                 \u001b[0mobj_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_theta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m                 bounds=bounds)\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[0m_check_optimize_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lbfgs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mtheta_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 618\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    306\u001b[0m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001b[0;32m    307\u001b[0m                                   \u001b[0mbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_bounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                                   finite_diff_rel_step=finite_diff_rel_step)\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[0mfunc_and_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun_and_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m--> 262\u001b[1;33m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# Gradient evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36mobj_func\u001b[1;34m(theta, eval_gradient)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                     lml, grad = self.log_marginal_likelihood(\n\u001b[1;32m--> 225\u001b[1;33m                         theta, eval_gradient=True, clone_kernel=False)\n\u001b[0m\u001b[0;32m    226\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcho_solve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Line 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[1;31m# Compute log-likelihood (compare line 7)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\scipy\\linalg\\decomp_cholesky.py\u001b[0m in \u001b[0;36mcho_solve\u001b[1;34m(c_and_lower, b, overwrite_b, check_finite)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_and_lower\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray_chkfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray_chkfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\travelers\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AllFloat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         raise ValueError(\n\u001b[1;32m--> 486\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m    487\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "tweedie = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                        StandardScaler(),\n",
    "                        SMTweedieRegressor(max_iter=1000))\n",
    "params = {'smtweedieregressor__alpha': Real(0.01, 2.0, 'uniform'),\n",
    "          'smtweedieregressor__l1_ratio': Real(0.1, 0.9, 'uniform'),\n",
    "          'smtweedieregressor__power': Real(1.25, 1.75, 'uniform')}\n",
    "tweedie_opt = BayesSearchCV(tweedie,\n",
    "                            params,\n",
    "                            n_iter=64,\n",
    "                            scoring=make_scorer(gini),\n",
    "                            cv=12,\n",
    "                            error_score=0.0,\n",
    "                            fit_params={'smtweedieregressor__sample_weight': y['exposure']},\n",
    "                            verbose=10,\n",
    "                            n_jobs=-1)\n",
    "tweedie_opt.fit(X[lin_cols], y['pure_premium'])\n",
    "print(tweedie_opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "0.1575797222830057\n"
     ]
    }
   ],
   "source": [
    "tweedie = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                        StandardScaler(),\n",
    "                        SMTweedieRegressor(alpha=1.0, l1_ratio=0.2, power=1.5, max_iter=1000))\n",
    "preds = cross_val_predict(tweedie,\n",
    "                          X[lin_cols],\n",
    "                          y['pure_premium'],\n",
    "                          groups=y['fold'],\n",
    "                          cv=GroupKFold(),\n",
    "                          fit_params={'smtweedieregressor__sample_weight': y['exposure']},\n",
    "                          n_jobs=1)\n",
    "print(preds)\n",
    "print(gini(y['claim_cost'], preds * y['exposure']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: min_gain_split\n",
      "pure_premium 0.20176293814808957\n"
     ]
    }
   ],
   "source": [
    "# Claim cost with exposure as a variable\n",
    "lgbm_tweedie = LGBMRegressor(n_estimators=1000,\n",
    "                                 num_leaves=16,\n",
    "                                 learning_rate=1.0,\n",
    "                                 colsample_bytree=0.67,\n",
    "                                 min_gain_split=100.0,\n",
    "                                 subsample=0.67,\n",
    "                                 subsample_freq=1,\n",
    "                                 objective='tweedie',\n",
    "                                 tweedie_variance_power=1.85,\n",
    "                                 boosting_type='rf',\n",
    "                                 n_jobs=-1)\n",
    "tweedie = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                            PolynomialFeatures(interaction_only=True, include_bias = False),  \n",
    "                            StandardScaler(),\n",
    "                            SelectFromModel(lgbm_tweedie, max_features=12),\n",
    "                            TweedieRegressor(alpha=0.0, power=1.01, max_iter=1000))\n",
    "preds = cross_val_predict(tweedie,\n",
    "                          X[lin_cols + ['exposure']],\n",
    "                          y['claim_cost'],\n",
    "                          groups=y['fold'],\n",
    "                          cv=GroupKFold(),\n",
    "                          fit_params={'tweedieregressor__sample_weight': y['exposure']},\n",
    "                          n_jobs=-1)\n",
    "tweedie.fit(X[lin_cols + ['exposure']], y['claim_cost'])\n",
    "print(i, gini(y['claim_cost'], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    8.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   11.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   10.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    9.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   10.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   11.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    8.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   11.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   11.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    5.5s finished\n",
      "C:\\Users\\gursk\\anaconda3\\envs\\travelers\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    7.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    variable  importance\n",
      "0  veh_value           0\n",
      "1   veh_body           0\n",
      "2    veh_age           2\n",
      "3     gender           0\n",
      "4       area           0\n",
      "5     dr_age         252\n",
      "6   exposure         746\n",
      "0.14543728341717443\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': Integer(100, 1000),\n",
    "          'num_leaves': Integer(2, 11),\n",
    "          'learning_rate': Real(1e-4, 3.0, 'uniform'),\n",
    "          'min_split_gain': Real(0.0, 150.0, 'uniform'),\n",
    "          'subsample': Real(0.67, 0.9, 'uniform'),\n",
    "          'tweedie_variance_power': Real(1.01, 1.95, 'uniform'),\n",
    "          'boosting_type': Categorical(['rf', 'gbdt'])}\n",
    "lgbm_tweedie = LGBMRegressor(subsample_freq=1,\n",
    "                             objective='tweedie',\n",
    "                             n_jobs=-1)\n",
    "lgbm_opt = BayesSearchCV(lgbm_tweedie,\n",
    "                         params,\n",
    "                         n_iter=64,\n",
    "                         cv=15,\n",
    "                         scoring=make_scorer(gini),\n",
    "                         verbose=1,\n",
    "                         n_jobs=1)\n",
    "lgbm_opt.fit(X[boost_cols + ['exposure']], y['claim_cost'])\n",
    "print(pd.DataFrame({'variable': X[boost_cols + ['exposure']].columns, 'importance': lgbm_opt.best_estimator_.feature_importances_}))\n",
    "preds = cross_val_predict(lgbm_opt.best_estimator_,\n",
    "                          X[boost_cols + ['exposure']],\n",
    "                          y['claim_cost'],\n",
    "                          groups=y['fold'],\n",
    "                          cv=GroupKFold(),\n",
    "                          n_jobs=-1)\n",
    "print(gini(y['claim_cost'], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 67 features.\n",
      "Fitting estimator with 66 features.\n",
      "Fitting estimator with 65 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 62 features.\n",
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  5.8min remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "# Tweedie GLM with RFECV\n",
    "tweedie = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough'),\n",
    "                        PolynomialFeatures(interaction_only=True, include_bias = False),  \n",
    "                        StandardScaler(),\n",
    "                        RFECV(TweedieRegressor(alpha=1.0, power=1.2, max_iter=1000),\n",
    "                              step=0.01,\n",
    "                              cv=10,\n",
    "                              scoring=make_scorer(gini),\n",
    "                              verbose=5,\n",
    "                              n_jobs=10))\n",
    "preds = cross_val_predict(tweedie,\n",
    "                          X[lin_cols],\n",
    "                          y['claim_cost'],\n",
    "                          groups=y['fold'],\n",
    "                          cv=GroupKFold(),\n",
    "                          verbose=5,\n",
    "                          n_jobs=1)\n",
    "print(gini(y['claim_cost'], preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
