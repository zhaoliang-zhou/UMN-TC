{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_predict, cross_val_score, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.metrics import make_scorer, mean_poisson_deviance, mean_gamma_deviance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer, OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, PoissonRegressor, GammaRegressor, TweedieRegressor, LogisticRegressionCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "df = pd.read_csv('InsNova_train.csv')\n",
    "df = df.sample(frac=1.0)\n",
    "df.loc[:, 'pure_premium'] = df['claim_cost'] / df['exposure']\n",
    "df.loc[:, 'severity'] = df['claim_cost'] / np.fmax(df['claim_count'], 1)\n",
    "df.loc[:, 'frequency'] = df['claim_count'] / df['exposure']\n",
    "\n",
    "# Getting CV inds\n",
    "n_folds = 10\n",
    "cv = StratifiedKFold(n_folds, shuffle=True, random_state=123)\n",
    "df.loc[:, 'fold'] = 0\n",
    "for fold, (_, test_inds) in enumerate(cv.split(df, df['claim_ind'])):\n",
    "    df.loc[test_inds, 'fold'] = fold\n",
    "    \n",
    "# Feature engineering\n",
    "df['large_veh'] = np.where(df['veh_body'].isin(['MIBUS', 'MCARA', 'BUS']), 1, 0)\n",
    "df['expensive_area'] = np.where(df['area'].isin(['E','F']), 1, 0)\n",
    "df['expensive_age_risk'] = np.where(df['dr_age'].isin([1, 2]) & (df['veh_value'] > 5.0), 1, 0)\n",
    "df['expensive_veh'] = np.where(df['veh_value'] > 6.0, 1, 0)\n",
    "df['severe_veh'] = np.where(df['veh_body'].isin(['HDTOP', 'TRUCK', 'UTE']), 1, 0)\n",
    "df['young'] = np.where(df['dr_age'] == 1, 1, 0)\n",
    "\n",
    "# Creating Categorical dataset for LightGBM and CatBoost\n",
    "df['lm_gender'] = np.where(df['gender'] == 'M', 1, 0)\n",
    "for i in ['veh_body', 'area', 'gender', 'large_veh', 'expensive_area', 'expensive_age_risk', 'expensive_veh', 'severe_veh', 'young']:\n",
    "    df[i] = df[i].astype('category')\n",
    "    \n",
    "# Splitting into pred/response\n",
    "response_cols = ['fold',\n",
    "                 'exposure',\n",
    "                 'claim_ind',\n",
    "                 'claim_count',\n",
    "                 'claim_cost',\n",
    "                 'pure_premium',\n",
    "                 'severity',\n",
    "                 'frequency']\n",
    "X, y = df.drop(response_cols, axis=1), df[response_cols]\n",
    "X = X.drop('id', axis=1)\n",
    "X['exposure'] = y['exposure'].copy()\n",
    "lin_cols = ['veh_value', 'veh_body', 'veh_age', 'lm_gender', 'area', 'dr_age']\n",
    "boost_cols = ['veh_value', 'veh_body', 'veh_age', 'gender', 'area', 'dr_age', 'large_veh', 'expensive_area', 'expensive_age_risk', 'severe_veh', 'young']\n",
    "knn_cols = ['veh_value', 'veh_body', 'veh_age', 'lm_gender', 'area', 'dr_age', 'exposure']\n",
    "\n",
    "# Importing test set\n",
    "df_test = pd.read_csv('InsNova_test.csv')\n",
    "df_test['lm_gender'] = np.where(df_test['gender'] == 'M', 1, 0)\n",
    "df_test['large_veh'] = np.where(df_test['veh_body'].isin(['MIBUS', 'MCARA', 'BUS']), 1, 0)\n",
    "df_test['expensive_area'] = np.where(df_test['area'].isin(['E','F']), 1, 0)\n",
    "df_test['expensive_age_risk'] = np.where(df_test['dr_age'].isin([1, 2]) & (df_test['veh_value'] > 5.0), 1, 0)\n",
    "df_test['expensive_veh'] = np.where(df_test['veh_value'] > 6.0, 1, 0)\n",
    "df_test['severe_veh'] = np.where(df_test['veh_body'].isin(['HDTOP', 'TRUCK', 'UTE']), 1, 0)\n",
    "df_test['young'] = np.where(df_test['dr_age'] == 1, 1, 0)\n",
    "\n",
    "for i in ['veh_body', 'area', 'gender', 'large_veh', 'expensive_area', 'expensive_age_risk', 'expensive_veh', 'severe_veh', 'young']:\n",
    "    df_test[i] = df_test[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_curves(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # sort rows on prediction column \n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:,0].argsort()][::-1,0]\n",
    "    pred_order = arr[arr[:,1].argsort()][::-1,0]\n",
    "    \n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order)\n",
    "    L_ones = np.linspace(1/n_samples, 1, n_samples)\n",
    "    return L_ones, L_pred, L_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our gini function\n",
    "def gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # sort rows on prediction column \n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:,0].argsort()][::-1,0]\n",
    "    pred_order = arr[arr[:,1].argsort()][::-1,0]\n",
    "    \n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order)\n",
    "    L_ones = np.linspace(1/n_samples, 1, n_samples)\n",
    "    \n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred)\n",
    "    \n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred / G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini residual function\n",
    "def gini_residuals(y_true, y_pred):\n",
    "    g = gini(y_true, y_pred)\n",
    "    ginis = []\n",
    "    for i in range(len(y_true)):\n",
    "        t = np.delete(np.array(y_true), i)\n",
    "        p = np.delete(np.array(y_pred), i)\n",
    "        ginis.append(gini(t, p) - g)\n",
    "    return np.array(ginis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a target encoder\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, use_median=True, nonzero=True, min_samples=10):\n",
    "        self.nonzero = nonzero\n",
    "        self.use_median = use_median\n",
    "        self.min_samples = min_samples\n",
    "        self.categories_ = None\n",
    "        self.medians_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        _X, _y = np.array(X), np.array(y)\n",
    "        self.categories_ = {i: np.unique(_X[:, i]) for i in range(_X.shape[1])}\n",
    "        self.mu_ = {k:{i:[] for i in v} for k, v in self.categories_.items()}\n",
    "        mu_func = np.median if self.use_median else np.mean\n",
    "        all_mu = mu_func(_y[_y > 0.0]) if self.nonzero else mu_func(_y)\n",
    "        for k, v in self.categories_.items():\n",
    "            for i in v:\n",
    "                if self.nonzero:\n",
    "                    _x = _y[(_y > 0.0) & (_X[:, k] == i)]\n",
    "                    if _x.shape[0] == 0:\n",
    "                        self.mu_[k][i] = 0.0\n",
    "                    elif _x.shape[0] < self.min_samples:\n",
    "                        self.mu_[k][i] = all_mu\n",
    "                    else:\n",
    "                        self.mu_[k][i] = mu_func(_x)\n",
    "                else:\n",
    "                    _x = _y[_X[:, k] == i]\n",
    "                    if _x.shape[0] < self.min_samples:\n",
    "                        self.mu_[k][i] = all_mu\n",
    "                    else:\n",
    "                        self.mu_[k][i] = mu_func(_x)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        _X = np.array(X)\n",
    "        for i in range(_X.shape[1]):\n",
    "            _X[:, i] = [self.mu_[i][j] for j in _X[:, i]]\n",
    "        return np.array(_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonZeroRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, estimator, supports_sw=True):\n",
    "        self.estimator = estimator\n",
    "        self.supports_sw = supports_sw\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        non_zero = y > 0.0\n",
    "        if sample_weight is None or not self.supports_sw:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                _X, _y = X.loc[non_zero, :], y.loc[non_zero]\n",
    "            else:\n",
    "                _X, _y  = X[non_zero, :], y[non_zero]\n",
    "            self.estimator.fit(_X, _y)\n",
    "        else:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                _X, _y, _sample_weight  = X.loc[non_zero, :], y.loc[non_zero], sample_weight.loc[non_zero]\n",
    "            else:\n",
    "                _X, _y, _sample_weight  = X[non_zero, :], y[non_zero], sample_weight[non_zero]\n",
    "            self.estimator.fit(_X, _y, sample_weight=_sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonZeroClassifier(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, estimator, supports_sw=True, use_proba=True, stratify=False):\n",
    "        self.estimator = estimator\n",
    "        self.supports_sw = supports_sw\n",
    "        self.use_proba = use_proba\n",
    "        self.stratify = stratify\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "\n",
    "        if sample_weight is None or not self.supports_sw:\n",
    "            self.estimator.fit(X, y > 0.0)\n",
    "        else:\n",
    "            self.estimator.fit(X, y > 0.0, sample_weight=sample_weight.copy())\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        if self.use_proba:\n",
    "            return self.estimator.predict_proba(X)[:, 1]\n",
    "        return self.estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RangeRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, estimator, min_val=0.0, supports_sw=True):\n",
    "        self.estimator = estimator\n",
    "        self.min_val = min_val\n",
    "        self.supports_sw = supports_sw\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        above_min = y > self.min_val\n",
    "        if sample_weight is None or not self.supports_sw:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                _X, _y = X.loc[above_min, :], y.loc[above_min]\n",
    "            else:\n",
    "                _X, _y  = X[above_min, :], y[above_min]\n",
    "            self.estimator.fit(_X, _y)\n",
    "        else:\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                _X, _y, _sample_weight  = X.loc[above_min, :], y.loc[above_min], sample_weight.loc[above_min]\n",
    "            else:\n",
    "                _X, _y, _sample_weight  = X[above_min, :], y[above_min], sample_weight[above_min]\n",
    "            self.estimator.fit(_X, _y, sample_weight=_sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RangeClassifier(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, estimator, min_val=0.0, supports_sw=True, use_proba=True):\n",
    "        self.estimator = estimator\n",
    "        self.min_val = min_val\n",
    "        self.supports_sw = supports_sw\n",
    "        self.use_proba = use_proba\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if sample_weight is None or not self.supports_sw:\n",
    "            self.estimator.fit(X, y > self.min_val)\n",
    "        else:\n",
    "            self.estimator.fit(X, y > self.min_val, sample_weight=sample_weight.copy())\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        if self.use_proba:\n",
    "            return self.estimator.predict_proba(X)[:, 1]\n",
    "        return self.estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining column transformers for later steps          \n",
    "get_cats = make_column_selector(dtype_include=pd.CategoricalDtype)\n",
    "get_notcats = make_column_selector(dtype_exclude=pd.CategoricalDtype)\n",
    "get_notfloats = make_column_selector(dtype_exclude=np.float64)\n",
    "get_floats = make_column_selector(dtype_include=np.float64)\n",
    "get_ints = make_column_selector(dtype_include=[np.int32, np.int64])\n",
    "one_hot = lambda: ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats)], remainder='passthrough')\n",
    "\n",
    "# Initializing cross validated preds\n",
    "cv_preds = {}\n",
    "all_test_preds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tweedie Model Baseline\n",
    "tweedie = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats),\n",
    "                                           ('box-cox', PowerTransformer(standardize=False), get_notcats)\n",
    "                                           ], remainder='passthrough'),\n",
    "                           PolynomialFeatures(include_bias=False),\n",
    "                           StandardScaler(),\n",
    "                           PCA(),\n",
    "                           TweedieRegressor(max_iter=1000))\n",
    "\n",
    "# Hparam tuning\n",
    "tweedie_opt = RandomizedSearchCV(tweedie,\n",
    "                                 {'polynomialfeatures__degree': [1, 2, 3],\n",
    "                                  'polynomialfeatures__interaction_only': [True, False],\n",
    "                                  'pca__n_components': [3, 4, 5, 6, 7, 8],\n",
    "                                  'tweedieregressor__alpha': uniform(loc=0, scale=10),\n",
    "                                  'tweedieregressor__power': uniform(loc=1.01, scale=1.99)},\n",
    "                                 n_iter=64,\n",
    "                                 scoring=make_scorer(gini),\n",
    "                                 cv=20,\n",
    "                                 verbose=1,\n",
    "                                 n_jobs=-1)\n",
    "tweedie_opt.fit(X[lin_cols], y['pure_premium'], tweedieregressor__sample_weight=y['exposure'])\n",
    "print(tweedie_opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.198721466862644\n"
     ]
    }
   ],
   "source": [
    "# Manual model\n",
    "tweedie = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats),\n",
    "                                           ('box-cox', PowerTransformer(standardize=False), get_notcats)\n",
    "                                           ], remainder='passthrough'),\n",
    "                           PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    "                           StandardScaler(),\n",
    "                           PCA(5),\n",
    "                           TweedieRegressor(alpha=3.0, power=1.2, max_iter=1000))\n",
    "# Scoring model\n",
    "score = cross_val_score(tweedie,\n",
    "                        X[lin_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'tweedieregressor__sample_weight': y['exposure']},\n",
    "                        n_jobs=-1)\n",
    "print(np.mean(score))\n",
    "\n",
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    tweedie.fit(X_train[lin_cols], y_train['pure_premium'], tweedieregressor__sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(tweedie.predict(X_test[lin_cols]), index=X_test.index))\n",
    "    test_preds.append(tweedie.predict(df_test[lin_cols]))\n",
    "all_test_preds['tweedie'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['tweedie'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20043191  0.31661738  0.17737204  0.50835955  0.2905306   0.09399575\n",
      "  0.14565122  0.42903297 -0.52530833 -0.07458757  0.35850676  0.03527884\n",
      "  0.29211759  0.19573633  0.17902455]\n",
      "0.17485063949568116\n"
     ]
    }
   ],
   "source": [
    "# One-hot tweedie\n",
    "oh_tweedie = make_pipeline(ColumnTransformer([('one_hot', OneHotEncoder(drop='first', sparse=False), get_cats),\n",
    "                                              ('box-cox', PowerTransformer(standardize=False), get_notcats)\n",
    "                                              ], remainder='passthrough'),\n",
    "                           #PolynomialFeatures(degree=3, interaction_only=True, include_bias=False),\n",
    "                           StandardScaler(),\n",
    "                           #PCA(5),\n",
    "                           TweedieRegressor(alpha=5.0, power=1.4, max_iter=1000))\n",
    "score = cross_val_score(oh_tweedie,\n",
    "                        X[lin_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'tweedieregressor__sample_weight': y['exposure']},\n",
    "                        n_jobs=-1)\n",
    "print(score)\n",
    "print(np.mean(score))\n",
    "\n",
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    oh_tweedie.fit(X_train[lin_cols], y_train['pure_premium'], tweedieregressor__sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(oh_tweedie.predict(X_test[lin_cols]), index=X_test.index))\n",
    "    test_preds.append(oh_tweedie.predict(df_test[lin_cols]))\n",
    "all_test_preds['oh_tweedie'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['oh_tweedie'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14966578  0.2385659   0.21018667  0.48637794  0.27800741  0.13914778\n",
      "  0.00296292  0.44026499 -0.34447756 -0.0772104   0.37680352  0.07554164\n",
      "  0.31178615  0.11935788  0.11059778]\n",
      "0.16783855988265634\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = LGBMRegressor(n_estimators=1000,\n",
    "                   learning_rate=1.0,\n",
    "                   boosting_type='rf',\n",
    "                   objective='tweedie',\n",
    "                   tweedie_variance_power=1.55,\n",
    "                   num_leaves=4,\n",
    "                   subsample=0.67,\n",
    "                   subsample_freq=1,\n",
    "                   n_jobs=-1\n",
    "                   )\n",
    "score = cross_val_score(rf,\n",
    "                        X[boost_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    rf.fit(X_train[boost_cols], y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(rf.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(rf.predict(df_test[boost_cols]))\n",
    "all_test_preds['rf'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['rf'] = pd.concat(train_preds).loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    3.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    4.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('learning_rate', 1e-07), ('min_child_samples', 49), ('min_split_gain', 500.0), ('n_estimators', 473), ('num_leaves', 2), ('subsample', 0.9640331637540367), ('tweedie_variance_power', 1.5594489104484563)])\n",
      "0.24531778995147666\n",
      "[ 0.03388079  0.17022836  0.07888137  0.21598983 -0.40221429  0.48946783\n",
      " -0.03219759  0.33851482  0.24986033  0.57298795  0.59030364  0.07398932\n",
      "  0.19346411  0.10499096  0.37566635 -0.28321467  0.06708335  0.68482993\n",
      " -0.04886007  0.11779438  0.75558206 -0.00238375  0.0246525  -0.04546929\n",
      "  0.0518985 ]\n",
      "0.17502906881759991\n"
     ]
    }
   ],
   "source": [
    "# Lightgbm\n",
    "lgbm = LGBMRegressor(#n_estimators=50,\n",
    "                     #learning_rate=0.01,\n",
    "                     objective='tweedie',\n",
    "                     #tweedie_variance_power=1.001,\n",
    "                     #num_leaves=8,\n",
    "                     #subsample=1.0,\n",
    "                     subsample_freq=1,\n",
    "                     n_jobs=1)\n",
    "\n",
    "# Hparam tuning\n",
    "lgbm_opt = BayesSearchCV(lgbm,\n",
    "                                 {'n_estimators':Integer(100, 1000) ,\n",
    "                                  'learning_rate': Real(1e-7, 1.0, 'uniform'),\n",
    "                                  'tweedie_variance_power': Real(1.01, 1.99, 'uniform'),\n",
    "                                  'num_leaves': Integer(2, 7),\n",
    "                                  'subsample': Real(0.5, 0.99, 'uniform'),\n",
    "                                  'min_child_samples': Integer(5, 80),\n",
    "                                  'min_split_gain': Real(0.0, 500.0, 'uniform')},\n",
    "                                 n_iter=32,\n",
    "                                 scoring=make_scorer(gini),\n",
    "                                 fit_params={'sample_weight':y['exposure']},\n",
    "                                 cv=15,\n",
    "                                 verbose=1,\n",
    "                                 n_jobs=-1)\n",
    "lgbm_opt.fit(X[boost_cols], y['pure_premium'])\n",
    "print(lgbm_opt.best_params_)\n",
    "print(lgbm_opt.best_score_)\n",
    "\n",
    "score = cross_val_score(lgbm_opt.best_estimator_,\n",
    "                        X[boost_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['veh_value', 'veh_body', 'veh_age', 'gender', 'area', 'dr_age', 'large_veh', 'expensive_area', 'expensive_age_risk', 'severe_veh']\n"
     ]
    }
   ],
   "source": [
    "print(boost_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11821868 0.06141409 0.17106938 0.0185217  0.13696235 0.14380278\n",
      " 0.19396555 0.17336976 0.42665612 0.006318  ]\n",
      "0.14502984018117987\n",
      "2601     286.196471\n",
      "21245    284.162609\n",
      "1326     312.710981\n",
      "16606    318.054273\n",
      "15281    307.164900\n",
      "            ...    \n",
      "8037     281.909105\n",
      "1428     289.991871\n",
      "7870     233.287666\n",
      "13433    269.096916\n",
      "10531    314.126203\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Manual training\n",
    "boost_cols2 = ['veh_value', 'veh_body', 'veh_age', 'gender', 'area', 'dr_age']\n",
    "lgbm = LGBMRegressor(n_estimators=250,\n",
    "                     learning_rate=0.001,\n",
    "                     #colsample_bytree=0.8,\n",
    "                     objective='tweedie',\n",
    "                     tweedie_variance_power=1.999,\n",
    "                     num_leaves=8,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "score = cross_val_score(lgbm,\n",
    "                        X[boost_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))\n",
    "\n",
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    lgbm.fit(X_train[boost_cols], y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(lgbm.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(lgbm.predict(df_test[boost_cols]))\n",
    "all_test_preds['lgbm'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['lgbm'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['lgbm'])\n",
    "# Reduc cols 0.19363898258806375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(colsample_bylevel=0.3, extra_trees=True, learning_rate=0.001,\n",
       "              n_estimators=250, num_leaves=11, objective='tweedie',\n",
       "              tweedie_variance_power=1.999)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm = LGBMRegressor(n_estimators=250,\n",
    "                     learning_rate=0.001,\n",
    "                     extra_trees=True,\n",
    "                     colsample_bylevel=0.3,\n",
    "                     objective='tweedie',\n",
    "                     tweedie_variance_power=1.999,\n",
    "                     num_leaves=11,\n",
    "                     n_jobs=-1)\n",
    "lgbm.fit(X[boost_cols], y['pure_premium'], sample_weight=y['exposure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>splits</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dr_age</td>\n",
       "      <td>300</td>\n",
       "      <td>109212.484268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>veh_body</td>\n",
       "      <td>542</td>\n",
       "      <td>98773.462383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>area</td>\n",
       "      <td>482</td>\n",
       "      <td>72578.195187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gender</td>\n",
       "      <td>325</td>\n",
       "      <td>60935.484423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>veh_value</td>\n",
       "      <td>231</td>\n",
       "      <td>37263.797497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>expensive_area</td>\n",
       "      <td>149</td>\n",
       "      <td>32700.821354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>severe_veh</td>\n",
       "      <td>129</td>\n",
       "      <td>22461.367973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>expensive_age_risk</td>\n",
       "      <td>142</td>\n",
       "      <td>19869.461926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>young</td>\n",
       "      <td>102</td>\n",
       "      <td>19729.155472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>veh_age</td>\n",
       "      <td>84</td>\n",
       "      <td>13522.827534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>large_veh</td>\n",
       "      <td>14</td>\n",
       "      <td>1937.114613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature  splits           gain\n",
       "5               dr_age     300  109212.484268\n",
       "1             veh_body     542   98773.462383\n",
       "4                 area     482   72578.195187\n",
       "3               gender     325   60935.484423\n",
       "0            veh_value     231   37263.797497\n",
       "7       expensive_area     149   32700.821354\n",
       "9           severe_veh     129   22461.367973\n",
       "8   expensive_age_risk     142   19869.461926\n",
       "10               young     102   19729.155472\n",
       "2              veh_age      84   13522.827534\n",
       "6            large_veh      14    1937.114613"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'feature': boost_cols, 'splits': lgbm.feature_importances_, 'gain': lgbm.booster_.feature_importance(importance_type='gain')}).sort_values('gain', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08583931  0.34643854  0.18719417 -0.01450969  0.01894102  0.05158356\n",
      "  0.17119621  0.2134954   0.12373006  0.05443981]\n",
      "0.12383483822533861\n"
     ]
    }
   ],
   "source": [
    "# LGBM Quantile\n",
    "lgbmq = LGBMRegressor(n_estimators=500,\n",
    "                      learning_rate=0.001,\n",
    "                      subsample=0.67,\n",
    "                      colsample_bytree=0.5,\n",
    "                      objective='quantile',\n",
    "                      alpha=0.95,\n",
    "                      num_leaves=11,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "score = cross_val_score(lgbmq,\n",
    "                        X[boost_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18108581 -0.39093373  0.24278109  0.68519186  0.23574645  0.20568025\n",
      "  0.1087838   0.54383639 -0.02013459 -0.1327518   0.39290745  0.03954856\n",
      " -0.0142931   0.24299808  0.15343917]\n",
      "0.1649257112765702\n",
      "6885     110.344475\n",
      "12161    124.952278\n",
      "12540     69.103523\n",
      "4151     126.110100\n",
      "10352    138.443909\n",
      "            ...    \n",
      "14311    110.847580\n",
      "15735    136.210342\n",
      "14475    111.036064\n",
      "9240     109.553711\n",
      "18320    119.759125\n",
      "Length: 22610, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "xgb = make_pipeline(#ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats)], remainder='passthrough'),\n",
    "                    ColumnTransformer([('target_enc', OrdinalEncoder(dtype=np.int64), get_cats)], remainder='passthrough'),\n",
    "                    XGBRegressor(n_estimators=300,\n",
    "                                 learning_rate=0.02,\n",
    "                                 objective='reg:tweedie',\n",
    "                                 tree_method='hist',\n",
    "                                 tweedie_variance_power=1.999,\n",
    "                                 max_depth=3,\n",
    "                                 n_jobs=-1))\n",
    "\n",
    "score = cross_val_score(xgb,\n",
    "                        X[boost_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'xgbregressor__sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))\n",
    "\n",
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    xgb.fit(X_train[boost_cols], y_train['pure_premium'], xgbregressor__sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(xgb.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(xgb.predict(df_test[boost_cols]))\n",
    "all_test_preds['xgb'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['xgb'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['xgb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13958095 -0.17635848  0.22005491  0.62310353  0.24535255  0.15492344\n",
      "  0.19734019  0.57929146 -0.24932221 -0.0845525   0.47425562  0.02547992\n",
      "  0.35595483  0.20075316  0.26505159]\n",
      "0.1980605975636425\n"
     ]
    }
   ],
   "source": [
    "# Catboost\n",
    "cat = CatBoostRegressor(n_estimators=200,\n",
    "                        learning_rate=0.03,\n",
    "                        objective='Tweedie:variance_power=1.9',\n",
    "                        max_depth=3,\n",
    "                        cat_features=['veh_body', 'gender', 'area'],\n",
    "                        verbose=0,\n",
    "                        thread_count=-1)\n",
    "\n",
    "score = cross_val_score(cat,\n",
    "                        X[boost_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885     178.076602\n",
      "12161    198.260013\n",
      "12540    173.230984\n",
      "4151     208.643522\n",
      "10352    243.746311\n",
      "            ...    \n",
      "14311    178.278781\n",
      "15735    229.407184\n",
      "14475    183.772836\n",
      "9240     183.358397\n",
      "18320    217.989457\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    cat.fit(X_train[boost_cols], y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(cat.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(cat.predict(df_test[boost_cols]))\n",
    "all_test_preds['cat'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['cat'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05734663830954559\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "knn = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats)\n",
    "                                      ], remainder='passthrough'),\n",
    "                    StandardScaler(),\n",
    "                    BaggingRegressor(NonZeroRegressor(KNeighborsRegressor(n_neighbors=25, n_jobs=-1), False),\n",
    "                                     n_estimators=15,\n",
    "                                     max_features=5,\n",
    "                                     max_samples=0.8))\n",
    "# Scoring model\n",
    "score = cross_val_score(knn,\n",
    "                        X[knn_cols],\n",
    "                        y['claim_cost'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        n_jobs=1)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885     1610.269571\n",
      "12161    3629.205859\n",
      "12540    4854.713807\n",
      "4151     8989.101114\n",
      "10352    2177.862953\n",
      "            ...     \n",
      "14311    1922.386836\n",
      "15735    2124.286704\n",
      "14475    3485.412158\n",
      "9240     4373.170825\n",
      "18320    7895.825330\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    knn.fit(X_train[knn_cols], y_train['claim_cost'])\n",
    "    train_preds.append(pd.Series(knn.predict(X_test[knn_cols]), index=X_test.index) / y_test['exposure'])\n",
    "    test_preds.append(knn.predict(df_test[knn_cols]) / df_test['exposure'])\n",
    "all_test_preds['knn'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['knn'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['knn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19407822519484602\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "knn_clf = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats)\n",
    "                                      ], remainder='passthrough'),\n",
    "                    StandardScaler(),\n",
    "                    BaggingRegressor(NonZeroClassifier(KNeighborsClassifier(n_neighbors=170, n_jobs=-1), False),\n",
    "                                     n_estimators=25,\n",
    "                                     max_features=5,\n",
    "                                     max_samples=0.9))\n",
    "# Scoring model\n",
    "score = cross_val_score(knn_clf,\n",
    "                        X[knn_cols],\n",
    "                        y['claim_cost'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        n_jobs=1)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "6885     0.080706\n",
      "12161    0.079765\n",
      "12540    0.064471\n",
      "4151     0.058824\n",
      "10352    0.109882\n",
      "           ...   \n",
      "14311    0.096000\n",
      "15735    0.115294\n",
      "14475    0.046353\n",
      "9240     0.065176\n",
      "18320    0.054353\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    print(i)\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    knn_clf.fit(X_train[knn_cols], y_train['claim_cost'])\n",
    "    train_preds.append(pd.Series(knn_clf.predict(X_test[knn_cols]), index=X_test.index))\n",
    "    test_preds.append(knn_clf.predict(df_test[knn_cols]))\n",
    "all_test_preds['knn_clf'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['knn_clf'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['knn_clf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14511218787891336\n"
     ]
    }
   ],
   "source": [
    "# LogisticregressionCV\n",
    "logit = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats),\n",
    "                                         ('box-cox', PowerTransformer(standardize=False), get_notcats)\n",
    "                                        ], remainder='passthrough'),\n",
    "                    StandardScaler(),\n",
    "                    NonZeroClassifier(LogisticRegressionCV(class_weight='balanced', cv=10, n_jobs=-1)))\n",
    "# Scoring model\n",
    "score = cross_val_score(logit,\n",
    "                        X[lin_cols],\n",
    "                        y['claim_cost'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        n_jobs=1)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885     0.440645\n",
      "12161    0.481772\n",
      "12540    0.462702\n",
      "4151     0.540487\n",
      "10352    0.545346\n",
      "           ...   \n",
      "14311    0.489169\n",
      "15735    0.536102\n",
      "14475    0.442848\n",
      "9240     0.542170\n",
      "18320    0.521526\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    logit.fit(X_train[lin_cols], y_train['claim_cost'])\n",
    "    train_preds.append(pd.Series(logit.predict(X_test[lin_cols]), index=X_test.index))\n",
    "    test_preds.append(logit.predict(df_test[lin_cols]))\n",
    "all_test_preds['logit'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['logit'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['logit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04101349  0.19780271  0.18847764  0.77237682  0.37685804  0.07154275\n",
      "  0.15611665  0.54415544 -0.30560372 -0.17553633  0.30247395 -0.11098303\n",
      "  0.23886687  0.21872303  0.1982777 ]\n",
      "0.1809707997243479\n"
     ]
    }
   ],
   "source": [
    "# LGBM CLF\n",
    "lgbm_clf = NonZeroClassifier(LGBMClassifier(n_estimators=500,\n",
    "                                            learning_rate=0.0001,\n",
    "                                            subsample=0.9,\n",
    "                                            subsample_freq=1,\n",
    "                                            num_leaves=2,\n",
    "                                            unbalance=True,\n",
    "                                            n_jobs=-1))\n",
    "\n",
    "score = cross_val_score(lgbm_clf,\n",
    "                        X[boost_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885     0.087037\n",
      "12161    0.089342\n",
      "12540    0.086959\n",
      "4151     0.090218\n",
      "10352    0.089868\n",
      "           ...   \n",
      "14311    0.088090\n",
      "15735    0.089737\n",
      "14475    0.087201\n",
      "9240     0.088022\n",
      "18320    0.090218\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    lgbm_clf.fit(X_train[boost_cols], y_train['pure_premium'])\n",
    "    train_preds.append(pd.Series(lgbm_clf.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(lgbm_clf.predict(df_test[boost_cols]))\n",
    "all_test_preds['lgbm_clf'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['lgbm_clf'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['lgbm_clf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23263073 -0.01658864  0.08587136  0.33704254  0.14128359  0.12692438\n",
      "  0.20290174  0.41528218 -0.55007011  0.08634005  0.31696776  0.1763312\n",
      "  0.32501209 -0.0269242   0.28912824]\n",
      "0.14280886186854486\n"
     ]
    }
   ],
   "source": [
    "# Ridge severity\n",
    "ridge = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats)], remainder='passthrough'),\n",
    "                           StandardScaler(),\n",
    "                           NonZeroRegressor(RidgeCV(alphas=np.linspace(1e-4, 5.0, 10))))\n",
    "                               #TransformedTargetRegressor(RidgeCV(alphas=np.linspace(1e-4, 5.0, 10)),\n",
    "                               #                           transformer=PowerTransformer('box-cox'))))\n",
    "score = cross_val_score(ridge,\n",
    "                        X[lin_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'nonzeroregressor__sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885     2107.988633\n",
      "12161    3292.801386\n",
      "12540    2691.032597\n",
      "4151     2596.765066\n",
      "10352    4506.341937\n",
      "            ...     \n",
      "14311    2502.015634\n",
      "15735    3762.464394\n",
      "14475    2586.242514\n",
      "9240     3814.693576\n",
      "18320    4614.147051\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    ridge.fit(X_train[lin_cols], y_train['pure_premium'], nonzeroregressor__sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(ridge.predict(X_test[lin_cols]), index=X_test.index))\n",
    "    test_preds.append(ridge.predict(df_test[lin_cols]))\n",
    "all_test_preds['ridge'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['ridge'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['ridge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08000906798346649\n"
     ]
    }
   ],
   "source": [
    "# MLP Severity (We should switch to Keras)\n",
    "mlp = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats),\n",
    "                                       ('box-cox', PowerTransformer(standardize=False), get_notcats)\n",
    "                                        ], remainder='passthrough'),\n",
    "                           StandardScaler(),\n",
    "                           NonZeroRegressor(TransformedTargetRegressor(MLPRegressor(4,\n",
    "                                                                                    alpha=0.01,\n",
    "                                                                                    batch_size=256,\n",
    "                                                                                    early_stopping=True,\n",
    "                                                                                    max_iter=500),\n",
    "                                                                       transformer=StandardScaler())))\n",
    "# Scoring model\n",
    "score = cross_val_score(mlp,\n",
    "                        X[lin_cols],\n",
    "                        y['claim_cost'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        n_jobs=1)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885      8139.666016\n",
      "12161    41011.097531\n",
      "12540    -4840.198589\n",
      "4151      8343.487554\n",
      "10352    16763.637999\n",
      "             ...     \n",
      "14311    13364.997172\n",
      "15735    42961.045373\n",
      "14475    39303.169455\n",
      "9240     21495.940343\n",
      "18320    19684.601456\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    mlp.fit(X_train[lin_cols], y_train['pure_premium'])\n",
    "    train_preds.append(pd.Series(mlp.predict(X_test[lin_cols]), index=X_test.index) / y_test['exposure'])\n",
    "    test_preds.append(mlp.predict(df_test[lin_cols]) / df_test['exposure'])\n",
    "all_test_preds['mlp'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['mlp'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['mlp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25710048 -0.12800786  0.09876649  0.48126826  0.15781985  0.15297123\n",
      "  0.23047263  0.48861871 -0.48860714  0.17959459  0.45521955  0.14813572\n",
      "  0.32733374  0.07554325  0.37287572]\n",
      "0.18727368158689908\n"
     ]
    }
   ],
   "source": [
    "# Gamma severity\n",
    "gamma = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats),\n",
    "                                         ('box-cox', PowerTransformer(standardize=False), get_notcats)\n",
    "                                          ], remainder='passthrough'),\n",
    "                           PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    "                           StandardScaler(),\n",
    "                           PCA(5),\n",
    "                           NonZeroRegressor(GammaRegressor(alpha=1.0, max_iter=1000)))\n",
    "score = cross_val_score(gamma,\n",
    "                        X[lin_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'nonzeroregressor__sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885     2421.980303\n",
      "12161    3283.235437\n",
      "12540    2653.309812\n",
      "4151     2701.603559\n",
      "10352    4656.092809\n",
      "            ...     \n",
      "14311    2647.498291\n",
      "15735    3553.768134\n",
      "14475    2761.051201\n",
      "9240     3374.530629\n",
      "18320    4070.883790\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    gamma.fit(X_train[lin_cols], y_train['pure_premium'], nonzeroregressor__sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(gamma.predict(X_test[lin_cols]), index=X_test.index))\n",
    "    test_preds.append(gamma.predict(df_test[lin_cols]))\n",
    "all_test_preds['gamma'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['gamma'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23242023  0.53715691 -0.1894238   0.74628769 -0.29767913  0.017821\n",
      " -0.02532987 -0.10952595 -0.02414476  0.16639791  0.16127623  0.07742982\n",
      "  0.36201776  0.38122164 -0.04555531]\n",
      "0.13269135872231916\n"
     ]
    }
   ],
   "source": [
    "# LGBM Severity\n",
    "lgbm_sev = NonZeroRegressor(LGBMRegressor(n_estimators=500,\n",
    "                                          learning_rate=0.001,\n",
    "                                          num_leaves=6,\n",
    "                                          objective='rmse',\n",
    "                                          n_jobs=-1))\n",
    "\n",
    "score = cross_val_score(lgbm_sev,\n",
    "                        X[boost_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885     3176.065357\n",
      "12161    3587.230346\n",
      "12540    2836.040262\n",
      "4151     4962.546236\n",
      "10352    3720.319362\n",
      "            ...     \n",
      "14311    3131.620077\n",
      "15735    3313.608513\n",
      "14475    3188.819943\n",
      "9240     3241.498023\n",
      "18320    3360.783370\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    lgbm_sev.fit(X_train[boost_cols], y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(lgbm_sev.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(lgbm_sev.predict(df_test[boost_cols]))\n",
    "all_test_preds['lgbm_sev'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['lgbm_sev'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['lgbm_sev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08389231  0.48783364 -0.04200906  0.71876219 -0.02414942  0.13741699\n",
      "  0.16056212  0.09274029 -0.23005408 -0.08336625  0.29926543  0.11531065\n",
      "  0.19352046  0.07948427  0.14942526]\n",
      "0.14257565441292078\n"
     ]
    }
   ],
   "source": [
    "# Catboost severity\n",
    "cat_sev = NonZeroRegressor(CatBoostRegressor(n_estimators=250,\n",
    "                                             max_depth=3,\n",
    "                                             cat_features=['veh_body', 'gender', 'area'],\n",
    "                                             verbose=0,\n",
    "                                             thread_count=-1))\n",
    "\n",
    "score = cross_val_score(cat_sev,\n",
    "                        X[boost_cols],\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885     1971.065601\n",
      "12161    3345.712974\n",
      "12540    2063.043716\n",
      "4151     2044.236676\n",
      "10352    4043.863917\n",
      "            ...     \n",
      "14311    2117.640267\n",
      "15735    3144.232624\n",
      "14475    2710.175951\n",
      "9240     2824.465097\n",
      "18320    3627.217626\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    cat_sev.fit(X_train[boost_cols], y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(cat_sev.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(cat_sev.predict(df_test[boost_cols]))\n",
    "all_test_preds['cat_sev'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['cat_sev'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['cat_sev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2857.50010858  41694.55770595   9090.8808276  240346.66657709\n",
      "  11916.29112563   5521.11985886  31019.43787417  35139.11857241\n",
      " 160585.65084723  10985.12930893  27979.77766694  22445.1005335\n",
      "  13303.3138418   31296.9212742   49424.85367965]\n",
      "46240.4\n"
     ]
    }
   ],
   "source": [
    "# LGBM to try not underpredicting large losses\n",
    "lgbm_hl = RangeRegressor(LGBMRegressor(n_estimators=250,\n",
    "                        learning_rate=0.1,\n",
    "                        objective='mse',\n",
    "                        num_leaves=21,\n",
    "                        n_jobs=-1), 8000.0)\n",
    "\n",
    "score = cross_val_score(lgbm_hl,\n",
    "                           X[boost_cols],\n",
    "                           y['pure_premium'],\n",
    "                           groups=y['fold'],\n",
    "                           scoring=make_scorer(lambda x, y: np.sqrt(np.mean((x[x > 0.0] - y[x > 0.0]).clip(lower=0.0) ** 2))),\n",
    "                           cv=GroupKFold(n_folds),\n",
    "                           fit_params={'sample_weight': y['exposure']},\n",
    "                           n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score).round(1))\n",
    "# 46412.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885      8088.310287\n",
      "12161    15095.576208\n",
      "12540    47397.384903\n",
      "4151     40566.161496\n",
      "10352    37837.845711\n",
      "             ...     \n",
      "14311    10825.929682\n",
      "15735    33328.966561\n",
      "14475    15992.501842\n",
      "9240     17082.953747\n",
      "18320    22478.450091\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    lgbm_hl.fit(X_train[boost_cols], y_train['pure_premium'], sample_weight=y_train['exposure'])\n",
    "    train_preds.append(pd.Series(lgbm_hl.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(lgbm_hl.predict(df_test[boost_cols]))\n",
    "all_test_preds['lgbm_hl'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['lgbm_hl'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['lgbm_hl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 511.97545379 2361.29476446  642.4694825   464.78988347  777.32014265\n",
      "    0.          357.81076308 1507.91167409 1395.18005875 1402.41124566\n",
      "  908.38875698  339.63479361  355.00709796 4454.643188    731.39888864]\n",
      "1080.7\n"
     ]
    }
   ],
   "source": [
    "# Doing the same thing with claim cost\n",
    "lgbm_hlc = RangeRegressor(LGBMRegressor(n_estimators=250,\n",
    "                        learning_rate=0.1,\n",
    "                        objective='mse',\n",
    "                        num_leaves=21,\n",
    "                        n_jobs=-1), 8000.0)\n",
    "\n",
    "score = cross_val_score(lgbm_hlc,\n",
    "                           X[boost_cols],\n",
    "                           y['claim_cost'],\n",
    "                           groups=y['fold'],\n",
    "                           scoring=make_scorer(lambda x, y: np.sqrt(np.mean((x[x > 0.0] - y[x > 0.0]).clip(lower=0.0) ** 2))),\n",
    "                           cv=GroupKFold(n_folds),\n",
    "                           #fit_params={'sample_weight': y['exposure']},\n",
    "                           n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score).round(1))\n",
    "# 46412.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885      9348.723976\n",
      "12161    20199.958889\n",
      "12540    49289.933045\n",
      "4151     82014.327745\n",
      "10352    13562.409984\n",
      "             ...     \n",
      "14311    13751.915775\n",
      "15735    12742.418361\n",
      "14475    30705.494419\n",
      "9240     18083.689105\n",
      "18320    27165.862482\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    lgbm_hlc.fit(X_train[boost_cols], y_train['claim_cost'])\n",
    "    train_preds.append(pd.Series(lgbm_hlc.predict(X_test[boost_cols]), index=X_test.index) / y_test['exposure'])\n",
    "    test_preds.append(lgbm_hlc.predict(df_test[boost_cols]) / df_test['exposure'])\n",
    "all_test_preds['lgbm_hlc'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['lgbm_hlc'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['lgbm_hlc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04529938 -0.21876899  0.02375734  0.21888487  0.13312103  0.09576454\n",
      "  0.06774186  0.09710913  0.04223131  0.07903954  0.18931858  0.1114991\n",
      "  0.05828546 -0.06246209  0.19424674]\n",
      "0.065631268912514\n"
     ]
    }
   ],
   "source": [
    "# LGBM to classify if there will be a large loss\n",
    "lgbm_hlclf = RangeClassifier(LGBMClassifier(n_estimators=250,\n",
    "                                            learning_rate=0.001,\n",
    "                                            subsample=0.8,\n",
    "                                            subsample_freq=1,\n",
    "                                            num_leaves=8,\n",
    "                                            unbalance=True,\n",
    "                                            n_jobs=-1), 8000.0)\n",
    "\n",
    "score = cross_val_score(lgbm_hlclf,\n",
    "                        X[boost_cols],\n",
    "                        y['claim_cost'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885     0.070523\n",
      "12161    0.005746\n",
      "12540    0.083506\n",
      "4151     0.055777\n",
      "10352    0.115263\n",
      "           ...   \n",
      "14311    0.099994\n",
      "15735    0.146098\n",
      "14475    0.087981\n",
      "9240     0.102509\n",
      "18320    0.099170\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    lgbm_hlclf.fit(X_train[boost_cols], y_train['claim_cost'])\n",
    "    train_preds.append(pd.Series(lgbm_hlclf.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(lgbm_hlclf.predict(df_test[boost_cols]))\n",
    "all_test_preds['lgbm_hlclf'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['lgbm_hlclf'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['lgbm_hlclf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00061765  0.20052677  0.11025034 -0.06688248  0.18835548 -0.05247836\n",
      "  0.06579927  0.02094464 -0.00028767 -0.08363002  0.00056031 -0.02930136\n",
      "  0.10711024 -0.0467124   0.20418549]\n",
      "0.041270526978449434\n"
     ]
    }
   ],
   "source": [
    "# LGBM to classify medium losses\n",
    "lgbm_mlclf = RangeClassifier(LGBMClassifier(n_estimators=250,\n",
    "                                            learning_rate=0.001,\n",
    "                                            subsample=0.8,\n",
    "                                            subsample_freq=1,\n",
    "                                            num_leaves=8,\n",
    "                                            unbalance=True,\n",
    "                                            n_jobs=-1), 0.0)\n",
    "\n",
    "score = cross_val_score(lgbm_mlclf,\n",
    "                        X[boost_cols],\n",
    "                        np.where(y['claim_cost'] > 8000.0, 0.0, y['claim_cost']),\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6885     0.152392\n",
      "12161    0.160953\n",
      "12540    0.164904\n",
      "4151     0.154526\n",
      "10352    0.170449\n",
      "           ...   \n",
      "14311    0.158460\n",
      "15735    0.166783\n",
      "14475    0.153795\n",
      "9240     0.161751\n",
      "18320    0.168797\n",
      "Length: 22610, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions\n",
    "test_preds, train_preds = [], []\n",
    "for i in range(n_folds):\n",
    "    X_train, X_test, y_train, y_test = X.loc[y['fold'] != i, :], X.loc[y['fold'] == i, :], y.loc[y['fold'] != i, :], y.loc[y['fold'] == i, :]\n",
    "    lgbm_mlclf.fit(X_train[boost_cols], np.where(y_train['claim_cost'] > 8000.0, 0.0, y_train['claim_cost']))\n",
    "    train_preds.append(pd.Series(lgbm_mlclf.predict(X_test[boost_cols]), index=X_test.index))\n",
    "    test_preds.append(lgbm_mlclf.predict(df_test[boost_cols]))\n",
    "all_test_preds['lgbm_mlclf'] = np.mean(test_preds, axis=0)\n",
    "cv_preds['lgbm_mlclf'] = pd.concat(train_preds).loc[y.index]\n",
    "print(cv_preds['lgbm_mlclf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tweedie  oh_tweedie        rf      lgbm       xgb       cat  \\\n",
      "tweedie     1.000000    0.872917  0.757139  0.653568  0.534789  0.845641   \n",
      "oh_tweedie  0.872917    1.000000  0.726681  0.590390  0.477728  0.763803   \n",
      "rf          0.757139    0.726681  1.000000  0.844113  0.683448  0.899720   \n",
      "lgbm        0.653568    0.590390  0.844113  1.000000  0.577722  0.825789   \n",
      "xgb         0.534789    0.477728  0.683448  0.577722  1.000000  0.690394   \n",
      "cat         0.845641    0.763803  0.899720  0.825789  0.690394  1.000000   \n",
      "knn         0.071748    0.065957  0.071896  0.058162  0.047755  0.069418   \n",
      "logit       0.793547    0.747168  0.692063  0.623102  0.474387  0.705513   \n",
      "lgbm_clf    0.639051    0.582028  0.650155  0.669528  0.481720  0.696007   \n",
      "ridge       0.854532    0.793548  0.608692  0.480803  0.426035  0.716228   \n",
      "mlp         0.058432    0.058151  0.079228  0.072296  0.068484  0.068410   \n",
      "gamma       0.871265    0.776756  0.620527  0.508873  0.481006  0.756397   \n",
      "lgbm_sev    0.360817    0.354072  0.598780  0.389328  0.321165  0.445816   \n",
      "cat_sev     0.172631    0.168317  0.235977  0.135671  0.129327  0.175401   \n",
      "lgbm_hl     0.302827    0.271761  0.387268  0.382240  0.261088  0.409234   \n",
      "lgbm_hlclf  0.398910    0.346643  0.391855  0.272066  0.538038  0.423710   \n",
      "lgbm_hlc    0.032354    0.030889  0.040407  0.044947  0.006014  0.032065   \n",
      "lgbm_mlclf  0.520131    0.472239  0.452905  0.443076  0.485435  0.529807   \n",
      "knn_clf     0.209469    0.188725  0.173172  0.155043  0.152284  0.197609   \n",
      "\n",
      "                 knn     logit  lgbm_clf     ridge       mlp     gamma  \\\n",
      "tweedie     0.071748  0.793547  0.639051  0.854532  0.058432  0.871265   \n",
      "oh_tweedie  0.065957  0.747168  0.582028  0.793548  0.058151  0.776756   \n",
      "rf          0.071896  0.692063  0.650155  0.608692  0.079228  0.620527   \n",
      "lgbm        0.058162  0.623102  0.669528  0.480803  0.072296  0.508873   \n",
      "xgb         0.047755  0.474387  0.481720  0.426035  0.068484  0.481006   \n",
      "cat         0.069418  0.705513  0.696007  0.716228  0.068410  0.756397   \n",
      "knn         1.000000  0.049898  0.050802  0.065181  0.582454  0.066922   \n",
      "logit       0.049898  1.000000  0.686307  0.643902  0.062605  0.537635   \n",
      "lgbm_clf    0.050802  0.686307  1.000000  0.499621  0.056514  0.485825   \n",
      "ridge       0.065181  0.643902  0.499621  1.000000  0.060455  0.930053   \n",
      "mlp         0.582454  0.062605  0.056514  0.060455  1.000000  0.068202   \n",
      "gamma       0.066922  0.537635  0.485825  0.930053  0.068202  1.000000   \n",
      "lgbm_sev    0.058706  0.244501  0.214220  0.375568  0.042279  0.360377   \n",
      "cat_sev     0.034495  0.131124  0.098028  0.177453  0.014607  0.154275   \n",
      "lgbm_hl     0.037999  0.320738  0.276215  0.253205 -0.000020  0.198348   \n",
      "lgbm_hlclf  0.035794  0.366319  0.271741  0.308397  0.037151  0.317157   \n",
      "lgbm_hlc    0.947967  0.035059  0.026667  0.016231  0.559657  0.006292   \n",
      "lgbm_mlclf  0.021754  0.628817  0.642299  0.334470  0.019516  0.307670   \n",
      "knn_clf    -0.303825  0.231561  0.206917  0.150905 -0.201646  0.140687   \n",
      "\n",
      "            lgbm_sev   cat_sev   lgbm_hl  lgbm_hlclf  lgbm_hlc  lgbm_mlclf  \\\n",
      "tweedie     0.360817  0.172631  0.302827    0.398910  0.032354    0.520131   \n",
      "oh_tweedie  0.354072  0.168317  0.271761    0.346643  0.030889    0.472239   \n",
      "rf          0.598780  0.235977  0.387268    0.391855  0.040407    0.452905   \n",
      "lgbm        0.389328  0.135671  0.382240    0.272066  0.044947    0.443076   \n",
      "xgb         0.321165  0.129327  0.261088    0.538038  0.006014    0.485435   \n",
      "cat         0.445816  0.175401  0.409234    0.423710  0.032065    0.529807   \n",
      "knn         0.058706  0.034495  0.037999    0.035794  0.947967    0.021754   \n",
      "logit       0.244501  0.131124  0.320738    0.366319  0.035059    0.628817   \n",
      "lgbm_clf    0.214220  0.098028  0.276215    0.271741  0.026667    0.642299   \n",
      "ridge       0.375568  0.177453  0.253205    0.308397  0.016231    0.334470   \n",
      "mlp         0.042279  0.014607 -0.000020    0.037151  0.559657    0.019516   \n",
      "gamma       0.360377  0.154275  0.198348    0.317157  0.006292    0.307670   \n",
      "lgbm_sev    1.000000  0.354595  0.211206    0.202789  0.030827   -0.054875   \n",
      "cat_sev     0.354595  1.000000  0.110265    0.067388  0.027979   -0.003109   \n",
      "lgbm_hl     0.211206  0.110265  1.000000    0.251200  0.053191    0.219958   \n",
      "lgbm_hlclf  0.202789  0.067388  0.251200    1.000000 -0.004186    0.307201   \n",
      "lgbm_hlc    0.030827  0.027979  0.053191   -0.004186  1.000000    0.015995   \n",
      "lgbm_mlclf -0.054875 -0.003109  0.219958    0.307201  0.015995    1.000000   \n",
      "knn_clf     0.008444  0.010551  0.103053    0.113622 -0.313125    0.254626   \n",
      "\n",
      "             knn_clf  \n",
      "tweedie     0.209469  \n",
      "oh_tweedie  0.188725  \n",
      "rf          0.173172  \n",
      "lgbm        0.155043  \n",
      "xgb         0.152284  \n",
      "cat         0.197609  \n",
      "knn        -0.303825  \n",
      "logit       0.231561  \n",
      "lgbm_clf    0.206917  \n",
      "ridge       0.150905  \n",
      "mlp        -0.201646  \n",
      "gamma       0.140687  \n",
      "lgbm_sev    0.008444  \n",
      "cat_sev     0.010551  \n",
      "lgbm_hl     0.103053  \n",
      "lgbm_hlclf  0.113622  \n",
      "lgbm_hlc   -0.313125  \n",
      "lgbm_mlclf  0.254626  \n",
      "knn_clf     1.000000  \n",
      "             tweedie  oh_tweedie        rf      lgbm       xgb       cat  \\\n",
      "tweedie     1.000000    0.881896  0.776928  0.736024  0.590451  0.877583   \n",
      "oh_tweedie  0.881896    1.000000  0.747330  0.667728  0.533401  0.795093   \n",
      "rf          0.776928    0.747330  1.000000  0.926093  0.745672  0.926148   \n",
      "lgbm        0.736024    0.667728  0.926093  1.000000  0.679434  0.905887   \n",
      "xgb         0.590451    0.533401  0.745672  0.679434  1.000000  0.759770   \n",
      "cat         0.877583    0.795093  0.926148  0.905887  0.759770  1.000000   \n",
      "knn         0.162959    0.142575  0.169184  0.145427  0.121217  0.172832   \n",
      "logit       0.835715    0.783002  0.738315  0.745750  0.550867  0.761444   \n",
      "lgbm_clf    0.691222    0.632717  0.706259  0.750976  0.559478  0.755125   \n",
      "ridge       0.871322    0.813857  0.630243  0.545872  0.475520  0.757035   \n",
      "mlp         0.183185    0.188925  0.257723  0.247224  0.251857  0.251984   \n",
      "gamma       0.876841    0.790807  0.641756  0.562959  0.525624  0.789190   \n",
      "lgbm_sev    0.381945    0.374890  0.635415  0.430175  0.397786  0.476121   \n",
      "cat_sev     0.244075    0.254871  0.333423  0.214196  0.209438  0.250945   \n",
      "lgbm_hl     0.322951    0.284223  0.403821  0.442416  0.308623  0.439651   \n",
      "lgbm_hlclf  0.409739    0.355228  0.393450  0.296751  0.586240  0.434175   \n",
      "lgbm_hlc    0.063115    0.054067  0.088063  0.103465  0.008797  0.074202   \n",
      "lgbm_mlclf  0.556347    0.502380  0.486616  0.525141  0.516780  0.569302   \n",
      "knn_clf     0.218190    0.196781  0.171104  0.168275  0.161325  0.203291   \n",
      "\n",
      "                 knn     logit  lgbm_clf     ridge       mlp     gamma  \\\n",
      "tweedie     0.162959  0.835715  0.691222  0.871322  0.183185  0.876841   \n",
      "oh_tweedie  0.142575  0.783002  0.632717  0.813857  0.188925  0.790807   \n",
      "rf          0.169184  0.738315  0.706259  0.630243  0.257723  0.641756   \n",
      "lgbm        0.145427  0.745750  0.750976  0.545872  0.247224  0.562959   \n",
      "xgb         0.121217  0.550867  0.559478  0.475520  0.251857  0.525624   \n",
      "cat         0.172832  0.761444  0.755125  0.757035  0.251984  0.789190   \n",
      "knn         1.000000  0.110091  0.118167  0.160156  0.812055  0.166463   \n",
      "logit       0.110091  1.000000  0.773973  0.685606  0.171334  0.573679   \n",
      "lgbm_clf    0.118167  0.773973  1.000000  0.554986  0.209207  0.533097   \n",
      "ridge       0.160156  0.685606  0.554986  1.000000  0.201172  0.941959   \n",
      "mlp         0.812055  0.171334  0.209207  0.201172  1.000000  0.224306   \n",
      "gamma       0.166463  0.573679  0.533097  0.941959  0.224306  1.000000   \n",
      "lgbm_sev    0.149465  0.272717  0.264179  0.400340  0.149448  0.387816   \n",
      "cat_sev     0.088940  0.199140  0.147791  0.266668  0.051938  0.236083   \n",
      "lgbm_hl     0.102165  0.339512  0.302212  0.271135  0.022468  0.216936   \n",
      "lgbm_hlclf  0.069425  0.389238  0.290956  0.322797  0.122588  0.329356   \n",
      "lgbm_hlc    0.907973  0.074254  0.053512  0.030611  0.668851  0.008113   \n",
      "lgbm_mlclf  0.043390  0.687057  0.695150  0.368573  0.073453  0.332354   \n",
      "knn_clf    -0.610018  0.257316  0.232437  0.155770 -0.505637  0.138359   \n",
      "\n",
      "            lgbm_sev   cat_sev   lgbm_hl  lgbm_hlclf  lgbm_hlc  lgbm_mlclf  \\\n",
      "tweedie     0.381945  0.244075  0.322951    0.409739  0.063115    0.556347   \n",
      "oh_tweedie  0.374890  0.254871  0.284223    0.355228  0.054067    0.502380   \n",
      "rf          0.635415  0.333423  0.403821    0.393450  0.088063    0.486616   \n",
      "lgbm        0.430175  0.214196  0.442416    0.296751  0.103465    0.525141   \n",
      "xgb         0.397786  0.209438  0.308623    0.586240  0.008797    0.516780   \n",
      "cat         0.476121  0.250945  0.439651    0.434175  0.074202    0.569302   \n",
      "knn         0.149465  0.088940  0.102165    0.069425  0.907973    0.043390   \n",
      "logit       0.272717  0.199140  0.339512    0.389238  0.074254    0.687057   \n",
      "lgbm_clf    0.264179  0.147791  0.302212    0.290956  0.053512    0.695150   \n",
      "ridge       0.400340  0.266668  0.271135    0.322797  0.030611    0.368573   \n",
      "mlp         0.149448  0.051938  0.022468    0.122588  0.668851    0.073453   \n",
      "gamma       0.387816  0.236083  0.216936    0.329356  0.008113    0.332354   \n",
      "lgbm_sev    1.000000  0.528533  0.224143    0.212438  0.069221   -0.058680   \n",
      "cat_sev     0.528533  1.000000  0.183996    0.105591  0.061196    0.002392   \n",
      "lgbm_hl     0.224143  0.183996  1.000000    0.260347  0.151619    0.239458   \n",
      "lgbm_hlclf  0.212438  0.105591  0.260347    1.000000 -0.033712    0.322563   \n",
      "lgbm_hlc    0.069221  0.061196  0.151619   -0.033712  1.000000    0.036871   \n",
      "lgbm_mlclf -0.058680  0.002392  0.239458    0.322563  0.036871    1.000000   \n",
      "knn_clf    -0.005436  0.014674  0.092245    0.119801 -0.611079    0.291467   \n",
      "\n",
      "             knn_clf  \n",
      "tweedie     0.218190  \n",
      "oh_tweedie  0.196781  \n",
      "rf          0.171104  \n",
      "lgbm        0.168275  \n",
      "xgb         0.161325  \n",
      "cat         0.203291  \n",
      "knn        -0.610018  \n",
      "logit       0.257316  \n",
      "lgbm_clf    0.232437  \n",
      "ridge       0.155770  \n",
      "mlp        -0.505637  \n",
      "gamma       0.138359  \n",
      "lgbm_sev   -0.005436  \n",
      "cat_sev     0.014674  \n",
      "lgbm_hl     0.092245  \n",
      "lgbm_hlclf  0.119801  \n",
      "lgbm_hlc   -0.611079  \n",
      "lgbm_mlclf  0.291467  \n",
      "knn_clf     1.000000  \n",
      "tweedie       0.562131\n",
      "oh_tweedie    0.526304\n",
      "rf            0.567239\n",
      "lgbm          0.531252\n",
      "xgb           0.472752\n",
      "cat           0.589462\n",
      "knn           0.212235\n",
      "logit         0.523632\n",
      "lgbm_clf      0.487971\n",
      "ridge         0.497559\n",
      "mlp           0.241162\n",
      "gamma         0.488195\n",
      "lgbm_sev      0.331080\n",
      "cat_sev       0.231257\n",
      "lgbm_hl       0.295154\n",
      "lgbm_hlclf    0.315104\n",
      "lgbm_hlc      0.147849\n",
      "lgbm_mlclf    0.378453\n",
      "knn_clf       0.088888\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Checking correlation of base models\n",
    "print(pd.DataFrame(cv_preds).corr())\n",
    "print(pd.DataFrame(all_test_preds).corr())\n",
    "print(pd.DataFrame(all_test_preds).corr().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12665462 0.48709002 0.19450324 0.88256832 0.32371732 0.1456702\n",
      " 0.48036729 0.54561856 0.78212694 0.20179186 0.43594239 0.55849628\n",
      " 0.40473369 0.26862338 0.65577944]\n",
      "0.43291223655210564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.3, learning_rate=0.001, n_estimators=500,\n",
       "              num_leaves=4, objective='mse', subsample=0.9, subsample_freq=1)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LGBM Stacking\n",
    "X_cv = pd.concat([X[boost_cols].copy(), pd.DataFrame(cv_preds)], axis=1)\n",
    "\n",
    "stacker = LGBMRegressor(n_estimators=500,\n",
    "                        learning_rate=0.001,\n",
    "                        num_leaves=4,\n",
    "                        objective='mse',\n",
    "                        subsample=0.9,\n",
    "                        subsample_freq=1,\n",
    "                        colsample_bytree=0.3,\n",
    "                        #monotone_constraints=[0] * len(boost_cols) + [1] * (len(list(cv_preds.keys()))),\n",
    "                        #monotone_constraints_method='advanced',\n",
    "                        n_jobs=-1)\n",
    "\n",
    "score = cross_val_score(stacker,\n",
    "                        X_cv,\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))\n",
    "\n",
    "stack_cv_preds = cross_val_predict(stacker,\n",
    "                        X_cv,\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "stacker.fit(X_cv, y['pure_premium'], sample_weight=y['exposure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>knn</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lgbm_hlc</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lgbm_sev</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mlp</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tweedie</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>veh_value</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>knn_clf</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xgb</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lgbm_hl</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>veh_age</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cat_sev</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rf</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lgbm_mlclf</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ridge</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm_clf</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gamma</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oh_tweedie</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>lgbm_hlclf</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cat</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dr_age</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gender</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>area</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>veh_body</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature  splits\n",
       "12         knn     205\n",
       "22    lgbm_hlc     203\n",
       "18    lgbm_sev     181\n",
       "16         mlp     180\n",
       "6      tweedie     126\n",
       "0    veh_value     123\n",
       "24     knn_clf      89\n",
       "10         xgb      54\n",
       "20     lgbm_hl      49\n",
       "2      veh_age      44\n",
       "19     cat_sev      32\n",
       "8           rf      30\n",
       "23  lgbm_mlclf      28\n",
       "15       ridge      28\n",
       "14    lgbm_clf      23\n",
       "13       logit      21\n",
       "17       gamma      20\n",
       "7   oh_tweedie      18\n",
       "21  lgbm_hlclf      14\n",
       "11         cat      11\n",
       "5       dr_age       8\n",
       "3       gender       7\n",
       "9         lgbm       6\n",
       "4         area       0\n",
       "1     veh_body       0"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'feature': X_cv.columns, 'splits': stacker.feature_importances_}).sort_values('splits', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resids = pd.DataFrame({'y': y['pure_premium'], 'pred': stack_cv_preds, 'resid': np.abs(y['pure_premium'] - stack_cv_preds)}).sort_values('resid', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22610, 3)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='pred', ylabel='y'>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "df_resids.plot.scatter(x='pred', y='y')\n",
    "# Hmm, looks like we are not doing a good job of not predicting 0 for some claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31158264 0.51253784 0.28364598 0.90942939 0.31228853 0.29532555\n",
      " 0.61492537 0.61265553 0.75781165 0.28117246 0.58301401 0.6509604\n",
      " 0.36328947 0.53005347 0.64006639]\n",
      "0.5105839117634997\n"
     ]
    }
   ],
   "source": [
    "# Tweedie Stacking\n",
    "X_cv = pd.concat([X[lin_cols].copy(), pd.DataFrame(cv_preds)], axis=1)\n",
    "if 'knn_clf' in X_cv.columns:\n",
    "    X_cv = X_cv.drop(['knn_clf'], axis=1)\n",
    "\n",
    "tweedie_stacker = make_pipeline(ColumnTransformer([('target_enc', TargetEncoder(True, True, 20), get_cats)], remainder='passthrough'),\n",
    "                        StandardScaler(),\n",
    "                        TweedieRegressor(alpha=1.0, power=1.1, max_iter=1000)\n",
    "                       )\n",
    "\n",
    "score = cross_val_score(tweedie_stacker,\n",
    "                        X_cv,\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        scoring=make_scorer(gini),\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'tweedieregressor__sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "print(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('target_enc',\n",
       "                                                  TargetEncoder(min_samples=20),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000002BF503BA760>)])),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('tweedieregressor',\n",
       "                 TweedieRegressor(max_iter=1000, power=1.1))])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_cv_preds_tweedie = cross_val_predict(tweedie_stacker,\n",
    "                        X_cv,\n",
    "                        y['pure_premium'],\n",
    "                        groups=y['fold'],\n",
    "                        cv=GroupKFold(n_folds),\n",
    "                        fit_params={'tweedieregressor__sample_weight': y['exposure']},\n",
    "                        n_jobs=1)\n",
    "tweedie_stacker.fit(X_cv, y['pure_premium'], tweedieregressor__sample_weight=y['exposure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "      <th>resid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.162888e+09</td>\n",
       "      <td>5.162888e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18746</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.055908e+08</td>\n",
       "      <td>2.055908e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.061978e+07</td>\n",
       "      <td>2.061978e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.016582e+07</td>\n",
       "      <td>1.016582e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14642</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.226152e+06</td>\n",
       "      <td>6.226152e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19348</th>\n",
       "      <td>2.537745e+06</td>\n",
       "      <td>3.786993e+05</td>\n",
       "      <td>2.159045e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19333</th>\n",
       "      <td>1.678722e+06</td>\n",
       "      <td>4.351080e+03</td>\n",
       "      <td>1.674370e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22116</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.009596e+05</td>\n",
       "      <td>8.009596e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9748</th>\n",
       "      <td>5.204061e+05</td>\n",
       "      <td>8.064435e+02</td>\n",
       "      <td>5.195997e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.949122e+05</td>\n",
       "      <td>4.949122e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.720589e+05</td>\n",
       "      <td>4.720589e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22538</th>\n",
       "      <td>4.092021e+05</td>\n",
       "      <td>6.410764e+02</td>\n",
       "      <td>4.085610e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16427</th>\n",
       "      <td>3.477115e+05</td>\n",
       "      <td>2.497909e+03</td>\n",
       "      <td>3.452136e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5158</th>\n",
       "      <td>3.343983e+05</td>\n",
       "      <td>9.578064e+02</td>\n",
       "      <td>3.334405e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16291</th>\n",
       "      <td>2.996938e+05</td>\n",
       "      <td>7.685922e+02</td>\n",
       "      <td>2.989252e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9164</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.604687e+05</td>\n",
       "      <td>2.604687e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16562</th>\n",
       "      <td>2.328804e+05</td>\n",
       "      <td>1.391034e+03</td>\n",
       "      <td>2.314894e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16363</th>\n",
       "      <td>2.150030e+05</td>\n",
       "      <td>8.945848e+02</td>\n",
       "      <td>2.141084e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5221</th>\n",
       "      <td>2.118778e+05</td>\n",
       "      <td>4.651796e+02</td>\n",
       "      <td>2.114126e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16585</th>\n",
       "      <td>2.070789e+05</td>\n",
       "      <td>4.967234e+02</td>\n",
       "      <td>2.065821e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21359</th>\n",
       "      <td>1.443310e+05</td>\n",
       "      <td>3.923573e+02</td>\n",
       "      <td>1.439386e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9756</th>\n",
       "      <td>1.319176e+05</td>\n",
       "      <td>5.665644e+02</td>\n",
       "      <td>1.313510e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.245505e+05</td>\n",
       "      <td>1.245505e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9690</th>\n",
       "      <td>1.217493e+05</td>\n",
       "      <td>4.358270e+02</td>\n",
       "      <td>1.213135e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16636</th>\n",
       "      <td>1.212447e+05</td>\n",
       "      <td>3.658985e+02</td>\n",
       "      <td>1.208788e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16659</th>\n",
       "      <td>1.207593e+05</td>\n",
       "      <td>4.903382e+02</td>\n",
       "      <td>1.202690e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16486</th>\n",
       "      <td>1.202048e+05</td>\n",
       "      <td>1.794964e+03</td>\n",
       "      <td>1.184099e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22562</th>\n",
       "      <td>1.032096e+05</td>\n",
       "      <td>8.149553e+02</td>\n",
       "      <td>1.023947e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5111</th>\n",
       "      <td>9.784114e+04</td>\n",
       "      <td>4.247847e+02</td>\n",
       "      <td>9.741635e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>9.470694e+04</td>\n",
       "      <td>5.325701e+02</td>\n",
       "      <td>9.417437e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22555</th>\n",
       "      <td>9.378816e+04</td>\n",
       "      <td>5.421635e+02</td>\n",
       "      <td>9.324600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21381</th>\n",
       "      <td>8.868174e+04</td>\n",
       "      <td>3.741108e+02</td>\n",
       "      <td>8.830763e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16375</th>\n",
       "      <td>8.821697e+04</td>\n",
       "      <td>3.001786e+02</td>\n",
       "      <td>8.791679e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.259011e+04</td>\n",
       "      <td>8.259011e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>8.124026e+04</td>\n",
       "      <td>6.109094e+02</td>\n",
       "      <td>8.062935e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19353</th>\n",
       "      <td>7.629211e+04</td>\n",
       "      <td>3.766246e+02</td>\n",
       "      <td>7.591548e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19370</th>\n",
       "      <td>5.972217e+04</td>\n",
       "      <td>6.467967e+02</td>\n",
       "      <td>5.907537e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16635</th>\n",
       "      <td>5.901691e+04</td>\n",
       "      <td>5.584912e+02</td>\n",
       "      <td>5.845842e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9767</th>\n",
       "      <td>5.833229e+04</td>\n",
       "      <td>4.172818e+02</td>\n",
       "      <td>5.791501e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5299</th>\n",
       "      <td>5.586041e+04</td>\n",
       "      <td>4.375561e+02</td>\n",
       "      <td>5.542285e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16626</th>\n",
       "      <td>5.502392e+04</td>\n",
       "      <td>2.791334e+02</td>\n",
       "      <td>5.474478e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16502</th>\n",
       "      <td>5.363894e+04</td>\n",
       "      <td>2.857180e+02</td>\n",
       "      <td>5.335322e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9841</th>\n",
       "      <td>5.346398e+04</td>\n",
       "      <td>2.961229e+02</td>\n",
       "      <td>5.316786e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9872</th>\n",
       "      <td>5.290713e+04</td>\n",
       "      <td>3.705897e+02</td>\n",
       "      <td>5.253654e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5152</th>\n",
       "      <td>5.269462e+04</td>\n",
       "      <td>1.970427e+02</td>\n",
       "      <td>5.249758e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9781</th>\n",
       "      <td>5.171644e+04</td>\n",
       "      <td>3.567744e+02</td>\n",
       "      <td>5.135967e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>5.160872e+04</td>\n",
       "      <td>2.675424e+02</td>\n",
       "      <td>5.134118e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19383</th>\n",
       "      <td>5.101222e+04</td>\n",
       "      <td>2.274103e+02</td>\n",
       "      <td>5.078481e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9805</th>\n",
       "      <td>5.017593e+04</td>\n",
       "      <td>3.649342e+02</td>\n",
       "      <td>4.981099e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5121</th>\n",
       "      <td>4.964006e+04</td>\n",
       "      <td>2.473295e+02</td>\n",
       "      <td>4.939273e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.900498e+04</td>\n",
       "      <td>4.900498e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19286</th>\n",
       "      <td>4.738947e+04</td>\n",
       "      <td>2.779553e+02</td>\n",
       "      <td>4.711151e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9696</th>\n",
       "      <td>4.748769e+04</td>\n",
       "      <td>3.841718e+02</td>\n",
       "      <td>4.710351e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14741</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.674192e+04</td>\n",
       "      <td>4.674192e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19354</th>\n",
       "      <td>4.675737e+04</td>\n",
       "      <td>2.776729e+02</td>\n",
       "      <td>4.647969e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>4.634726e+04</td>\n",
       "      <td>3.514622e+02</td>\n",
       "      <td>4.599580e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9615</th>\n",
       "      <td>4.483519e+04</td>\n",
       "      <td>4.355789e+02</td>\n",
       "      <td>4.439961e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16406</th>\n",
       "      <td>4.435911e+04</td>\n",
       "      <td>3.083374e+02</td>\n",
       "      <td>4.405078e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16552</th>\n",
       "      <td>4.415491e+04</td>\n",
       "      <td>5.954652e+02</td>\n",
       "      <td>4.355945e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9707</th>\n",
       "      <td>4.378650e+04</td>\n",
       "      <td>4.640557e+02</td>\n",
       "      <td>4.332244e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9821</th>\n",
       "      <td>4.284479e+04</td>\n",
       "      <td>4.119506e+02</td>\n",
       "      <td>4.243284e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824</th>\n",
       "      <td>4.164366e+04</td>\n",
       "      <td>3.194126e+02</td>\n",
       "      <td>4.132425e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9804</th>\n",
       "      <td>4.158377e+04</td>\n",
       "      <td>5.718709e+02</td>\n",
       "      <td>4.101190e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16278</th>\n",
       "      <td>4.107025e+04</td>\n",
       "      <td>2.630951e+02</td>\n",
       "      <td>4.080715e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21395</th>\n",
       "      <td>4.083939e+04</td>\n",
       "      <td>1.781060e+02</td>\n",
       "      <td>4.066128e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5268</th>\n",
       "      <td>3.926413e+04</td>\n",
       "      <td>3.606901e+02</td>\n",
       "      <td>3.890344e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19365</th>\n",
       "      <td>3.849541e+04</td>\n",
       "      <td>4.232177e+02</td>\n",
       "      <td>3.807220e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>3.646177e+04</td>\n",
       "      <td>3.251437e+02</td>\n",
       "      <td>3.613662e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9567</th>\n",
       "      <td>3.468699e+04</td>\n",
       "      <td>3.289109e+02</td>\n",
       "      <td>3.435808e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21392</th>\n",
       "      <td>3.412894e+04</td>\n",
       "      <td>2.213895e+02</td>\n",
       "      <td>3.390755e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>3.401931e+04</td>\n",
       "      <td>2.822863e+02</td>\n",
       "      <td>3.373702e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>3.345643e+04</td>\n",
       "      <td>2.280504e+02</td>\n",
       "      <td>3.322838e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>3.315601e+04</td>\n",
       "      <td>3.138987e+02</td>\n",
       "      <td>3.284211e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>3.243742e+04</td>\n",
       "      <td>2.821051e+02</td>\n",
       "      <td>3.215531e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.141145e+04</td>\n",
       "      <td>3.141145e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16688</th>\n",
       "      <td>3.092304e+04</td>\n",
       "      <td>3.334592e+02</td>\n",
       "      <td>3.058958e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>3.034648e+04</td>\n",
       "      <td>3.643500e+02</td>\n",
       "      <td>2.998213e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16726</th>\n",
       "      <td>3.016427e+04</td>\n",
       "      <td>3.637583e+02</td>\n",
       "      <td>2.980051e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19338</th>\n",
       "      <td>2.983273e+04</td>\n",
       "      <td>2.537527e+02</td>\n",
       "      <td>2.957897e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16469</th>\n",
       "      <td>2.943587e+04</td>\n",
       "      <td>6.584847e+02</td>\n",
       "      <td>2.877738e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16272</th>\n",
       "      <td>2.893916e+04</td>\n",
       "      <td>2.627146e+02</td>\n",
       "      <td>2.867644e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.854915e+04</td>\n",
       "      <td>2.854915e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16521</th>\n",
       "      <td>2.805005e+04</td>\n",
       "      <td>3.136224e+02</td>\n",
       "      <td>2.773642e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9728</th>\n",
       "      <td>2.789629e+04</td>\n",
       "      <td>4.169078e+02</td>\n",
       "      <td>2.747939e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19444</th>\n",
       "      <td>2.719290e+04</td>\n",
       "      <td>2.267497e+02</td>\n",
       "      <td>2.696615e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10834</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.459403e+04</td>\n",
       "      <td>2.459403e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16536</th>\n",
       "      <td>2.487640e+04</td>\n",
       "      <td>3.412702e+02</td>\n",
       "      <td>2.453513e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22564</th>\n",
       "      <td>2.495169e+04</td>\n",
       "      <td>4.170544e+02</td>\n",
       "      <td>2.453463e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5368</th>\n",
       "      <td>2.462976e+04</td>\n",
       "      <td>3.466429e+02</td>\n",
       "      <td>2.428312e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16615</th>\n",
       "      <td>2.429200e+04</td>\n",
       "      <td>2.902473e+02</td>\n",
       "      <td>2.400175e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21379</th>\n",
       "      <td>2.325484e+04</td>\n",
       "      <td>4.798745e+02</td>\n",
       "      <td>2.277496e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16709</th>\n",
       "      <td>2.250643e+04</td>\n",
       "      <td>4.239357e+02</td>\n",
       "      <td>2.208250e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9843</th>\n",
       "      <td>2.215813e+04</td>\n",
       "      <td>2.679266e+02</td>\n",
       "      <td>2.189021e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16378</th>\n",
       "      <td>2.210398e+04</td>\n",
       "      <td>2.278723e+02</td>\n",
       "      <td>2.187611e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21306</th>\n",
       "      <td>2.202200e+04</td>\n",
       "      <td>2.442698e+02</td>\n",
       "      <td>2.177773e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21346</th>\n",
       "      <td>2.216482e+04</td>\n",
       "      <td>5.630493e+02</td>\n",
       "      <td>2.160177e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21340</th>\n",
       "      <td>2.182577e+04</td>\n",
       "      <td>2.323610e+02</td>\n",
       "      <td>2.159341e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>2.136085e+04</td>\n",
       "      <td>4.549519e+02</td>\n",
       "      <td>2.090589e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5271</th>\n",
       "      <td>2.113349e+04</td>\n",
       "      <td>3.279965e+02</td>\n",
       "      <td>2.080550e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16441</th>\n",
       "      <td>1.990620e+04</td>\n",
       "      <td>4.122445e+02</td>\n",
       "      <td>1.949396e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  y          pred         resid\n",
       "450    0.000000e+00  5.162888e+09  5.162888e+09\n",
       "18746  0.000000e+00  2.055908e+08  2.055908e+08\n",
       "7194   0.000000e+00  2.061978e+07  2.061978e+07\n",
       "335    0.000000e+00  1.016582e+07  1.016582e+07\n",
       "14642  0.000000e+00  6.226152e+06  6.226152e+06\n",
       "19348  2.537745e+06  3.786993e+05  2.159045e+06\n",
       "19333  1.678722e+06  4.351080e+03  1.674370e+06\n",
       "22116  0.000000e+00  8.009596e+05  8.009596e+05\n",
       "9748   5.204061e+05  8.064435e+02  5.195997e+05\n",
       "1663   0.000000e+00  4.949122e+05  4.949122e+05\n",
       "3335   0.000000e+00  4.720589e+05  4.720589e+05\n",
       "22538  4.092021e+05  6.410764e+02  4.085610e+05\n",
       "16427  3.477115e+05  2.497909e+03  3.452136e+05\n",
       "5158   3.343983e+05  9.578064e+02  3.334405e+05\n",
       "16291  2.996938e+05  7.685922e+02  2.989252e+05\n",
       "9164   0.000000e+00  2.604687e+05  2.604687e+05\n",
       "16562  2.328804e+05  1.391034e+03  2.314894e+05\n",
       "16363  2.150030e+05  8.945848e+02  2.141084e+05\n",
       "5221   2.118778e+05  4.651796e+02  2.114126e+05\n",
       "16585  2.070789e+05  4.967234e+02  2.065821e+05\n",
       "21359  1.443310e+05  3.923573e+02  1.439386e+05\n",
       "9756   1.319176e+05  5.665644e+02  1.313510e+05\n",
       "2509   0.000000e+00  1.245505e+05  1.245505e+05\n",
       "9690   1.217493e+05  4.358270e+02  1.213135e+05\n",
       "16636  1.212447e+05  3.658985e+02  1.208788e+05\n",
       "16659  1.207593e+05  4.903382e+02  1.202690e+05\n",
       "16486  1.202048e+05  1.794964e+03  1.184099e+05\n",
       "22562  1.032096e+05  8.149553e+02  1.023947e+05\n",
       "5111   9.784114e+04  4.247847e+02  9.741635e+04\n",
       "5108   9.470694e+04  5.325701e+02  9.417437e+04\n",
       "22555  9.378816e+04  5.421635e+02  9.324600e+04\n",
       "21381  8.868174e+04  3.741108e+02  8.830763e+04\n",
       "16375  8.821697e+04  3.001786e+02  8.791679e+04\n",
       "11310  0.000000e+00  8.259011e+04  8.259011e+04\n",
       "22519  8.124026e+04  6.109094e+02  8.062935e+04\n",
       "19353  7.629211e+04  3.766246e+02  7.591548e+04\n",
       "19370  5.972217e+04  6.467967e+02  5.907537e+04\n",
       "16635  5.901691e+04  5.584912e+02  5.845842e+04\n",
       "9767   5.833229e+04  4.172818e+02  5.791501e+04\n",
       "5299   5.586041e+04  4.375561e+02  5.542285e+04\n",
       "16626  5.502392e+04  2.791334e+02  5.474478e+04\n",
       "16502  5.363894e+04  2.857180e+02  5.335322e+04\n",
       "9841   5.346398e+04  2.961229e+02  5.316786e+04\n",
       "9872   5.290713e+04  3.705897e+02  5.253654e+04\n",
       "5152   5.269462e+04  1.970427e+02  5.249758e+04\n",
       "9781   5.171644e+04  3.567744e+02  5.135967e+04\n",
       "5097   5.160872e+04  2.675424e+02  5.134118e+04\n",
       "19383  5.101222e+04  2.274103e+02  5.078481e+04\n",
       "9805   5.017593e+04  3.649342e+02  4.981099e+04\n",
       "5121   4.964006e+04  2.473295e+02  4.939273e+04\n",
       "3351   0.000000e+00  4.900498e+04  4.900498e+04\n",
       "19286  4.738947e+04  2.779553e+02  4.711151e+04\n",
       "9696   4.748769e+04  3.841718e+02  4.710351e+04\n",
       "14741  0.000000e+00  4.674192e+04  4.674192e+04\n",
       "19354  4.675737e+04  2.776729e+02  4.647969e+04\n",
       "5086   4.634726e+04  3.514622e+02  4.599580e+04\n",
       "9615   4.483519e+04  4.355789e+02  4.439961e+04\n",
       "16406  4.435911e+04  3.083374e+02  4.405078e+04\n",
       "16552  4.415491e+04  5.954652e+02  4.355945e+04\n",
       "9707   4.378650e+04  4.640557e+02  4.332244e+04\n",
       "9821   4.284479e+04  4.119506e+02  4.243284e+04\n",
       "9824   4.164366e+04  3.194126e+02  4.132425e+04\n",
       "9804   4.158377e+04  5.718709e+02  4.101190e+04\n",
       "16278  4.107025e+04  2.630951e+02  4.080715e+04\n",
       "21395  4.083939e+04  1.781060e+02  4.066128e+04\n",
       "5268   3.926413e+04  3.606901e+02  3.890344e+04\n",
       "19365  3.849541e+04  4.232177e+02  3.807220e+04\n",
       "5164   3.646177e+04  3.251437e+02  3.613662e+04\n",
       "9567   3.468699e+04  3.289109e+02  3.435808e+04\n",
       "21392  3.412894e+04  2.213895e+02  3.390755e+04\n",
       "5100   3.401931e+04  2.822863e+02  3.373702e+04\n",
       "5267   3.345643e+04  2.280504e+02  3.322838e+04\n",
       "5166   3.315601e+04  3.138987e+02  3.284211e+04\n",
       "22522  3.243742e+04  2.821051e+02  3.215531e+04\n",
       "2776   0.000000e+00  3.141145e+04  3.141145e+04\n",
       "16688  3.092304e+04  3.334592e+02  3.058958e+04\n",
       "5092   3.034648e+04  3.643500e+02  2.998213e+04\n",
       "16726  3.016427e+04  3.637583e+02  2.980051e+04\n",
       "19338  2.983273e+04  2.537527e+02  2.957897e+04\n",
       "16469  2.943587e+04  6.584847e+02  2.877738e+04\n",
       "16272  2.893916e+04  2.627146e+02  2.867644e+04\n",
       "3291   0.000000e+00  2.854915e+04  2.854915e+04\n",
       "16521  2.805005e+04  3.136224e+02  2.773642e+04\n",
       "9728   2.789629e+04  4.169078e+02  2.747939e+04\n",
       "19444  2.719290e+04  2.267497e+02  2.696615e+04\n",
       "10834  0.000000e+00  2.459403e+04  2.459403e+04\n",
       "16536  2.487640e+04  3.412702e+02  2.453513e+04\n",
       "22564  2.495169e+04  4.170544e+02  2.453463e+04\n",
       "5368   2.462976e+04  3.466429e+02  2.428312e+04\n",
       "16615  2.429200e+04  2.902473e+02  2.400175e+04\n",
       "21379  2.325484e+04  4.798745e+02  2.277496e+04\n",
       "16709  2.250643e+04  4.239357e+02  2.208250e+04\n",
       "9843   2.215813e+04  2.679266e+02  2.189021e+04\n",
       "16378  2.210398e+04  2.278723e+02  2.187611e+04\n",
       "21306  2.202200e+04  2.442698e+02  2.177773e+04\n",
       "21346  2.216482e+04  5.630493e+02  2.160177e+04\n",
       "21340  2.182577e+04  2.323610e+02  2.159341e+04\n",
       "9735   2.136085e+04  4.549519e+02  2.090589e+04\n",
       "5271   2.113349e+04  3.279965e+02  2.080550e+04\n",
       "16441  1.990620e+04  4.122445e+02  1.949396e+04"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'y': y['pure_premium'], 'pred': stack_cv_preds_tweedie, 'resid': np.abs(y['pure_premium'] - stack_cv_preds_tweedie)}).sort_values('resid', ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1601496688125165\n",
      "0.29018928313038395\n",
      "0.2448004726936185\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "      <th>resid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.581451e+09</td>\n",
       "      <td>2.581451e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18746</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.027997e+08</td>\n",
       "      <td>1.027997e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.031509e+07</td>\n",
       "      <td>1.031509e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.086605e+06</td>\n",
       "      <td>5.086605e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14642</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.117376e+06</td>\n",
       "      <td>3.117376e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19348</th>\n",
       "      <td>2.537745e+06</td>\n",
       "      <td>1.921766e+05</td>\n",
       "      <td>2.345568e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19333</th>\n",
       "      <td>1.678722e+06</td>\n",
       "      <td>3.133894e+03</td>\n",
       "      <td>1.675588e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9748</th>\n",
       "      <td>5.204061e+05</td>\n",
       "      <td>7.976876e+02</td>\n",
       "      <td>5.196084e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22538</th>\n",
       "      <td>4.092021e+05</td>\n",
       "      <td>5.406206e+02</td>\n",
       "      <td>4.086615e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22116</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.057753e+05</td>\n",
       "      <td>4.057753e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16427</th>\n",
       "      <td>3.477115e+05</td>\n",
       "      <td>1.890238e+03</td>\n",
       "      <td>3.458212e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5158</th>\n",
       "      <td>3.343983e+05</td>\n",
       "      <td>6.243451e+02</td>\n",
       "      <td>3.337740e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16291</th>\n",
       "      <td>2.996938e+05</td>\n",
       "      <td>1.007566e+03</td>\n",
       "      <td>2.986862e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.539296e+05</td>\n",
       "      <td>2.539296e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.390683e+05</td>\n",
       "      <td>2.390683e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16562</th>\n",
       "      <td>2.328804e+05</td>\n",
       "      <td>1.156738e+03</td>\n",
       "      <td>2.317237e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16363</th>\n",
       "      <td>2.150030e+05</td>\n",
       "      <td>6.764791e+02</td>\n",
       "      <td>2.143265e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5221</th>\n",
       "      <td>2.118778e+05</td>\n",
       "      <td>7.618874e+02</td>\n",
       "      <td>2.111159e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16585</th>\n",
       "      <td>2.070789e+05</td>\n",
       "      <td>4.290179e+02</td>\n",
       "      <td>2.066498e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21359</th>\n",
       "      <td>1.443310e+05</td>\n",
       "      <td>4.093396e+02</td>\n",
       "      <td>1.439217e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9164</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.335184e+05</td>\n",
       "      <td>1.335184e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9756</th>\n",
       "      <td>1.319176e+05</td>\n",
       "      <td>5.366911e+02</td>\n",
       "      <td>1.313809e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9690</th>\n",
       "      <td>1.217493e+05</td>\n",
       "      <td>8.708525e+02</td>\n",
       "      <td>1.208785e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16636</th>\n",
       "      <td>1.212447e+05</td>\n",
       "      <td>4.326513e+02</td>\n",
       "      <td>1.208120e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16659</th>\n",
       "      <td>1.207593e+05</td>\n",
       "      <td>1.670522e+03</td>\n",
       "      <td>1.190888e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16486</th>\n",
       "      <td>1.202048e+05</td>\n",
       "      <td>2.086358e+03</td>\n",
       "      <td>1.181185e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22562</th>\n",
       "      <td>1.032096e+05</td>\n",
       "      <td>5.997833e+02</td>\n",
       "      <td>1.026099e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5111</th>\n",
       "      <td>9.784114e+04</td>\n",
       "      <td>5.376042e+02</td>\n",
       "      <td>9.730353e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>9.470694e+04</td>\n",
       "      <td>4.955798e+02</td>\n",
       "      <td>9.421136e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22555</th>\n",
       "      <td>9.378816e+04</td>\n",
       "      <td>4.990574e+02</td>\n",
       "      <td>9.328910e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21381</th>\n",
       "      <td>8.868174e+04</td>\n",
       "      <td>3.619278e+02</td>\n",
       "      <td>8.831981e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16375</th>\n",
       "      <td>8.821697e+04</td>\n",
       "      <td>3.304803e+02</td>\n",
       "      <td>8.788649e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>8.124026e+04</td>\n",
       "      <td>4.339497e+02</td>\n",
       "      <td>8.080631e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19353</th>\n",
       "      <td>7.629211e+04</td>\n",
       "      <td>3.311908e+02</td>\n",
       "      <td>7.596092e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.454737e+04</td>\n",
       "      <td>6.454737e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19370</th>\n",
       "      <td>5.972217e+04</td>\n",
       "      <td>4.624482e+02</td>\n",
       "      <td>5.925972e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16635</th>\n",
       "      <td>5.901691e+04</td>\n",
       "      <td>4.630503e+02</td>\n",
       "      <td>5.855386e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9767</th>\n",
       "      <td>5.833229e+04</td>\n",
       "      <td>3.941277e+02</td>\n",
       "      <td>5.793817e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5299</th>\n",
       "      <td>5.586041e+04</td>\n",
       "      <td>3.722199e+02</td>\n",
       "      <td>5.548819e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16626</th>\n",
       "      <td>5.502392e+04</td>\n",
       "      <td>2.853648e+02</td>\n",
       "      <td>5.473855e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16502</th>\n",
       "      <td>5.363894e+04</td>\n",
       "      <td>2.961391e+02</td>\n",
       "      <td>5.334280e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9841</th>\n",
       "      <td>5.346398e+04</td>\n",
       "      <td>3.010578e+02</td>\n",
       "      <td>5.316292e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9872</th>\n",
       "      <td>5.290713e+04</td>\n",
       "      <td>3.694406e+02</td>\n",
       "      <td>5.253769e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5152</th>\n",
       "      <td>5.269462e+04</td>\n",
       "      <td>2.544131e+02</td>\n",
       "      <td>5.244021e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9781</th>\n",
       "      <td>5.171644e+04</td>\n",
       "      <td>3.533952e+02</td>\n",
       "      <td>5.136304e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>5.160872e+04</td>\n",
       "      <td>3.079544e+02</td>\n",
       "      <td>5.130077e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19383</th>\n",
       "      <td>5.101222e+04</td>\n",
       "      <td>2.423206e+02</td>\n",
       "      <td>5.076990e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9805</th>\n",
       "      <td>5.017593e+04</td>\n",
       "      <td>4.075948e+02</td>\n",
       "      <td>4.976833e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5121</th>\n",
       "      <td>4.964006e+04</td>\n",
       "      <td>2.729275e+02</td>\n",
       "      <td>4.936713e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9696</th>\n",
       "      <td>4.748769e+04</td>\n",
       "      <td>3.692295e+02</td>\n",
       "      <td>4.711846e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19286</th>\n",
       "      <td>4.738947e+04</td>\n",
       "      <td>2.953433e+02</td>\n",
       "      <td>4.709413e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19354</th>\n",
       "      <td>4.675737e+04</td>\n",
       "      <td>3.012846e+02</td>\n",
       "      <td>4.645608e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>4.634726e+04</td>\n",
       "      <td>3.393407e+02</td>\n",
       "      <td>4.600792e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.532388e+04</td>\n",
       "      <td>4.532388e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9615</th>\n",
       "      <td>4.483519e+04</td>\n",
       "      <td>3.470169e+02</td>\n",
       "      <td>4.448817e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16406</th>\n",
       "      <td>4.435911e+04</td>\n",
       "      <td>3.320747e+02</td>\n",
       "      <td>4.402704e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16552</th>\n",
       "      <td>4.415491e+04</td>\n",
       "      <td>4.650880e+02</td>\n",
       "      <td>4.368982e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9707</th>\n",
       "      <td>4.378650e+04</td>\n",
       "      <td>4.129426e+02</td>\n",
       "      <td>4.337356e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9821</th>\n",
       "      <td>4.284479e+04</td>\n",
       "      <td>3.780544e+02</td>\n",
       "      <td>4.246674e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9824</th>\n",
       "      <td>4.164366e+04</td>\n",
       "      <td>3.245145e+02</td>\n",
       "      <td>4.131914e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9804</th>\n",
       "      <td>4.158377e+04</td>\n",
       "      <td>4.769313e+02</td>\n",
       "      <td>4.110684e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16278</th>\n",
       "      <td>4.107025e+04</td>\n",
       "      <td>2.660251e+02</td>\n",
       "      <td>4.080422e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21395</th>\n",
       "      <td>4.083939e+04</td>\n",
       "      <td>2.219344e+02</td>\n",
       "      <td>4.061745e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5268</th>\n",
       "      <td>3.926413e+04</td>\n",
       "      <td>4.195583e+02</td>\n",
       "      <td>3.884457e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19365</th>\n",
       "      <td>3.849541e+04</td>\n",
       "      <td>3.884116e+02</td>\n",
       "      <td>3.810700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>3.646177e+04</td>\n",
       "      <td>3.945715e+02</td>\n",
       "      <td>3.606720e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9567</th>\n",
       "      <td>3.468699e+04</td>\n",
       "      <td>3.029236e+02</td>\n",
       "      <td>3.438406e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21392</th>\n",
       "      <td>3.412894e+04</td>\n",
       "      <td>2.681289e+02</td>\n",
       "      <td>3.386081e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>3.401931e+04</td>\n",
       "      <td>2.915350e+02</td>\n",
       "      <td>3.372777e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>3.345643e+04</td>\n",
       "      <td>3.163991e+02</td>\n",
       "      <td>3.314003e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>3.315601e+04</td>\n",
       "      <td>2.990259e+02</td>\n",
       "      <td>3.285698e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>3.243742e+04</td>\n",
       "      <td>3.182579e+02</td>\n",
       "      <td>3.211916e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16688</th>\n",
       "      <td>3.092304e+04</td>\n",
       "      <td>3.426537e+02</td>\n",
       "      <td>3.058039e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5092</th>\n",
       "      <td>3.034648e+04</td>\n",
       "      <td>3.119425e+02</td>\n",
       "      <td>3.003454e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16726</th>\n",
       "      <td>3.016427e+04</td>\n",
       "      <td>3.182306e+02</td>\n",
       "      <td>2.984604e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19338</th>\n",
       "      <td>2.983273e+04</td>\n",
       "      <td>2.659075e+02</td>\n",
       "      <td>2.956682e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16469</th>\n",
       "      <td>2.943587e+04</td>\n",
       "      <td>5.252277e+02</td>\n",
       "      <td>2.891064e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16272</th>\n",
       "      <td>2.893916e+04</td>\n",
       "      <td>3.246867e+02</td>\n",
       "      <td>2.861447e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16521</th>\n",
       "      <td>2.805005e+04</td>\n",
       "      <td>2.923991e+02</td>\n",
       "      <td>2.775765e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9728</th>\n",
       "      <td>2.789629e+04</td>\n",
       "      <td>3.804388e+02</td>\n",
       "      <td>2.751585e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19444</th>\n",
       "      <td>2.719290e+04</td>\n",
       "      <td>2.505959e+02</td>\n",
       "      <td>2.694230e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14741</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.606557e+04</td>\n",
       "      <td>2.606557e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.573122e+04</td>\n",
       "      <td>2.573122e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22564</th>\n",
       "      <td>2.495169e+04</td>\n",
       "      <td>3.649566e+02</td>\n",
       "      <td>2.458673e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16536</th>\n",
       "      <td>2.487640e+04</td>\n",
       "      <td>3.301213e+02</td>\n",
       "      <td>2.454628e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5368</th>\n",
       "      <td>2.462976e+04</td>\n",
       "      <td>3.590817e+02</td>\n",
       "      <td>2.427068e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16615</th>\n",
       "      <td>2.429200e+04</td>\n",
       "      <td>2.957201e+02</td>\n",
       "      <td>2.399628e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21379</th>\n",
       "      <td>2.325484e+04</td>\n",
       "      <td>4.761102e+02</td>\n",
       "      <td>2.277873e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16709</th>\n",
       "      <td>2.250643e+04</td>\n",
       "      <td>3.515004e+02</td>\n",
       "      <td>2.215493e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16378</th>\n",
       "      <td>2.210398e+04</td>\n",
       "      <td>2.524498e+02</td>\n",
       "      <td>2.185153e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21306</th>\n",
       "      <td>2.202200e+04</td>\n",
       "      <td>2.557625e+02</td>\n",
       "      <td>2.176623e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9843</th>\n",
       "      <td>2.215813e+04</td>\n",
       "      <td>4.134999e+02</td>\n",
       "      <td>2.174463e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21346</th>\n",
       "      <td>2.216482e+04</td>\n",
       "      <td>4.304325e+02</td>\n",
       "      <td>2.173439e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21340</th>\n",
       "      <td>2.182577e+04</td>\n",
       "      <td>2.461300e+02</td>\n",
       "      <td>2.157964e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>2.136085e+04</td>\n",
       "      <td>3.901085e+02</td>\n",
       "      <td>2.097074e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5271</th>\n",
       "      <td>2.113349e+04</td>\n",
       "      <td>3.277478e+02</td>\n",
       "      <td>2.080574e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.030699e+04</td>\n",
       "      <td>2.030699e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.983385e+04</td>\n",
       "      <td>1.983385e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16441</th>\n",
       "      <td>1.990620e+04</td>\n",
       "      <td>3.843211e+02</td>\n",
       "      <td>1.952188e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16275</th>\n",
       "      <td>1.985631e+04</td>\n",
       "      <td>3.364388e+02</td>\n",
       "      <td>1.951987e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  y          pred         resid\n",
       "450    0.000000e+00  2.581451e+09  2.581451e+09\n",
       "18746  0.000000e+00  1.027997e+08  1.027997e+08\n",
       "7194   0.000000e+00  1.031509e+07  1.031509e+07\n",
       "335    0.000000e+00  5.086605e+06  5.086605e+06\n",
       "14642  0.000000e+00  3.117376e+06  3.117376e+06\n",
       "19348  2.537745e+06  1.921766e+05  2.345568e+06\n",
       "19333  1.678722e+06  3.133894e+03  1.675588e+06\n",
       "9748   5.204061e+05  7.976876e+02  5.196084e+05\n",
       "22538  4.092021e+05  5.406206e+02  4.086615e+05\n",
       "22116  0.000000e+00  4.057753e+05  4.057753e+05\n",
       "16427  3.477115e+05  1.890238e+03  3.458212e+05\n",
       "5158   3.343983e+05  6.243451e+02  3.337740e+05\n",
       "16291  2.996938e+05  1.007566e+03  2.986862e+05\n",
       "1663   0.000000e+00  2.539296e+05  2.539296e+05\n",
       "3335   0.000000e+00  2.390683e+05  2.390683e+05\n",
       "16562  2.328804e+05  1.156738e+03  2.317237e+05\n",
       "16363  2.150030e+05  6.764791e+02  2.143265e+05\n",
       "5221   2.118778e+05  7.618874e+02  2.111159e+05\n",
       "16585  2.070789e+05  4.290179e+02  2.066498e+05\n",
       "21359  1.443310e+05  4.093396e+02  1.439217e+05\n",
       "9164   0.000000e+00  1.335184e+05  1.335184e+05\n",
       "9756   1.319176e+05  5.366911e+02  1.313809e+05\n",
       "9690   1.217493e+05  8.708525e+02  1.208785e+05\n",
       "16636  1.212447e+05  4.326513e+02  1.208120e+05\n",
       "16659  1.207593e+05  1.670522e+03  1.190888e+05\n",
       "16486  1.202048e+05  2.086358e+03  1.181185e+05\n",
       "22562  1.032096e+05  5.997833e+02  1.026099e+05\n",
       "5111   9.784114e+04  5.376042e+02  9.730353e+04\n",
       "5108   9.470694e+04  4.955798e+02  9.421136e+04\n",
       "22555  9.378816e+04  4.990574e+02  9.328910e+04\n",
       "21381  8.868174e+04  3.619278e+02  8.831981e+04\n",
       "16375  8.821697e+04  3.304803e+02  8.788649e+04\n",
       "22519  8.124026e+04  4.339497e+02  8.080631e+04\n",
       "19353  7.629211e+04  3.311908e+02  7.596092e+04\n",
       "2509   0.000000e+00  6.454737e+04  6.454737e+04\n",
       "19370  5.972217e+04  4.624482e+02  5.925972e+04\n",
       "16635  5.901691e+04  4.630503e+02  5.855386e+04\n",
       "9767   5.833229e+04  3.941277e+02  5.793817e+04\n",
       "5299   5.586041e+04  3.722199e+02  5.548819e+04\n",
       "16626  5.502392e+04  2.853648e+02  5.473855e+04\n",
       "16502  5.363894e+04  2.961391e+02  5.334280e+04\n",
       "9841   5.346398e+04  3.010578e+02  5.316292e+04\n",
       "9872   5.290713e+04  3.694406e+02  5.253769e+04\n",
       "5152   5.269462e+04  2.544131e+02  5.244021e+04\n",
       "9781   5.171644e+04  3.533952e+02  5.136304e+04\n",
       "5097   5.160872e+04  3.079544e+02  5.130077e+04\n",
       "19383  5.101222e+04  2.423206e+02  5.076990e+04\n",
       "9805   5.017593e+04  4.075948e+02  4.976833e+04\n",
       "5121   4.964006e+04  2.729275e+02  4.936713e+04\n",
       "9696   4.748769e+04  3.692295e+02  4.711846e+04\n",
       "19286  4.738947e+04  2.953433e+02  4.709413e+04\n",
       "19354  4.675737e+04  3.012846e+02  4.645608e+04\n",
       "5086   4.634726e+04  3.393407e+02  4.600792e+04\n",
       "11310  0.000000e+00  4.532388e+04  4.532388e+04\n",
       "9615   4.483519e+04  3.470169e+02  4.448817e+04\n",
       "16406  4.435911e+04  3.320747e+02  4.402704e+04\n",
       "16552  4.415491e+04  4.650880e+02  4.368982e+04\n",
       "9707   4.378650e+04  4.129426e+02  4.337356e+04\n",
       "9821   4.284479e+04  3.780544e+02  4.246674e+04\n",
       "9824   4.164366e+04  3.245145e+02  4.131914e+04\n",
       "9804   4.158377e+04  4.769313e+02  4.110684e+04\n",
       "16278  4.107025e+04  2.660251e+02  4.080422e+04\n",
       "21395  4.083939e+04  2.219344e+02  4.061745e+04\n",
       "5268   3.926413e+04  4.195583e+02  3.884457e+04\n",
       "19365  3.849541e+04  3.884116e+02  3.810700e+04\n",
       "5164   3.646177e+04  3.945715e+02  3.606720e+04\n",
       "9567   3.468699e+04  3.029236e+02  3.438406e+04\n",
       "21392  3.412894e+04  2.681289e+02  3.386081e+04\n",
       "5100   3.401931e+04  2.915350e+02  3.372777e+04\n",
       "5267   3.345643e+04  3.163991e+02  3.314003e+04\n",
       "5166   3.315601e+04  2.990259e+02  3.285698e+04\n",
       "22522  3.243742e+04  3.182579e+02  3.211916e+04\n",
       "16688  3.092304e+04  3.426537e+02  3.058039e+04\n",
       "5092   3.034648e+04  3.119425e+02  3.003454e+04\n",
       "16726  3.016427e+04  3.182306e+02  2.984604e+04\n",
       "19338  2.983273e+04  2.659075e+02  2.956682e+04\n",
       "16469  2.943587e+04  5.252277e+02  2.891064e+04\n",
       "16272  2.893916e+04  3.246867e+02  2.861447e+04\n",
       "16521  2.805005e+04  2.923991e+02  2.775765e+04\n",
       "9728   2.789629e+04  3.804388e+02  2.751585e+04\n",
       "19444  2.719290e+04  2.505959e+02  2.694230e+04\n",
       "14741  0.000000e+00  2.606557e+04  2.606557e+04\n",
       "3351   0.000000e+00  2.573122e+04  2.573122e+04\n",
       "22564  2.495169e+04  3.649566e+02  2.458673e+04\n",
       "16536  2.487640e+04  3.301213e+02  2.454628e+04\n",
       "5368   2.462976e+04  3.590817e+02  2.427068e+04\n",
       "16615  2.429200e+04  2.957201e+02  2.399628e+04\n",
       "21379  2.325484e+04  4.761102e+02  2.277873e+04\n",
       "16709  2.250643e+04  3.515004e+02  2.215493e+04\n",
       "16378  2.210398e+04  2.524498e+02  2.185153e+04\n",
       "21306  2.202200e+04  2.557625e+02  2.176623e+04\n",
       "9843   2.215813e+04  4.134999e+02  2.174463e+04\n",
       "21346  2.216482e+04  4.304325e+02  2.173439e+04\n",
       "21340  2.182577e+04  2.461300e+02  2.157964e+04\n",
       "9735   2.136085e+04  3.901085e+02  2.097074e+04\n",
       "5271   2.113349e+04  3.277478e+02  2.080574e+04\n",
       "3291   0.000000e+00  2.030699e+04  2.030699e+04\n",
       "2776   0.000000e+00  1.983385e+04  1.983385e+04\n",
       "16441  1.990620e+04  3.843211e+02  1.952188e+04\n",
       "16275  1.985631e+04  3.364388e+02  1.951987e+04"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Averaging stacked preds\n",
    "avg_preds = (stack_cv_preds_tweedie + stack_cv_preds) / 2\n",
    "print(gini(y['claim_cost'], stack_cv_preds * y['exposure']))\n",
    "print(gini(y['claim_cost'], stack_cv_preds_tweedie * y['exposure']))\n",
    "print(gini(y['claim_cost'], avg_preds * y['exposure']))\n",
    "pd.DataFrame({'y': y['pure_premium'], 'pred': avg_preds, 'resid': np.abs(y['pure_premium'] - avg_preds)}).sort_values('resid', ascending=False).head(100)\n",
    "# Everything here is saying our tweedie model should be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cl, c1, ct = gini_curves(y['claim_cost'], stack_cv_preds * y['exposure'])\n",
    "_, c2, _ = gini_curves(y['claim_cost'], stack_cv_preds_tweedie * y['exposure'])\n",
    "_, c3, _ = gini_curves(y['claim_cost'], avg_preds * y['exposure'])\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.plot(cl, color='black')\n",
    "plt.plot(ct, label='True')\n",
    "plt.plot(c1, label='LGBM Stack')\n",
    "plt.plot(c2, label='Tweedie Stack')\n",
    "plt.plot(c3, label='Avg Stack')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our submission\n",
    "df_test['claim_cost'] = df_test['exposure'] * stacker.predict(pd.concat([df_test[boost_cols].copy(), pd.DataFrame(all_test_preds)], axis=1))\n",
    "df_test['id'] = np.arange(df_test.shape[0])\n",
    "df_test['id'] = df_test['id'].astype(int)\n",
    "df_test['id'] += 1\n",
    "df_test[['id', 'claim_cost']].to_csv('revised_stack_predictions_5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
